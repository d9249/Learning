{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "vgg13 Train+Inference (ImageNet statistics, pretrained, full).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6d8ac7622ea342b28c1ec135a9d6bf0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d2fbcb34ab2c4522985771b236665e95",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fb2633ceaa714f29b8b8b9e7c1548fcc",
              "IPY_MODEL_81d2a4b3f0aa456cbf78b856cada3c18",
              "IPY_MODEL_e8572b9098af4cbb8c738476506a2433"
            ]
          }
        },
        "d2fbcb34ab2c4522985771b236665e95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fb2633ceaa714f29b8b8b9e7c1548fcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e332e7272c04414ba8ddbcd4cb5059ae",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6435653a26ca4490929ec775e96e82a0"
          }
        },
        "81d2a4b3f0aa456cbf78b856cada3c18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_05579c6806c84a34805b3ee0830d6677",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 532199577,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 532199577,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a1ef5743b83a48da8a14fb65ae3b2c9c"
          }
        },
        "e8572b9098af4cbb8c738476506a2433": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_69821d23ccf74b89a6b50572f0b7b9a6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 508M/508M [00:13&lt;00:00, 53.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_60e0c18f1bdb4833a9e41f6b8b1b4006"
          }
        },
        "e332e7272c04414ba8ddbcd4cb5059ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6435653a26ca4490929ec775e96e82a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "05579c6806c84a34805b3ee0830d6677": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a1ef5743b83a48da8a14fb65ae3b2c9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "69821d23ccf74b89a6b50572f0b7b9a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "60e0c18f1bdb4833a9e41f6b8b1b4006": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/d9249/DACON/blob/main/vgg13%20Train%2BInference%20(ImageNet%20statistics%2C%20pretrained%2C%20full).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRmwJ7v-s4NO",
        "outputId": "2377ce2b-5499-41ab-d92e-bfcbbedf2e5e"
      },
      "source": [
        "!nvidia-smi"
      ],
      "id": "MRmwJ7v-s4NO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Oct 10 03:09:26 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.74       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vl7w9Wass7ah",
        "outputId": "d3c749ef-dc01-403c-d28f-b404a9aa1d1f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "vl7w9Wass7ah",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbOlLdrZtUhy"
      },
      "source": [
        "!unzip -uq \"/content/drive/MyDrive/DACON_2021_recognize_traffic_signal_motion/new_open.zip\" -d \"/content/\""
      ],
      "id": "dbOlLdrZtUhy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVh2zgiw8efC"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import torch, torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import os, json, PIL\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "import os, json, PIL\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "id": "HVh2zgiw8efC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOsYBUvQ6I8K"
      },
      "source": [
        "def count(output, target):\n",
        "    with torch.no_grad():\n",
        "        predict = torch.argmax(output, 1)\n",
        "        correct = (predict == target).sum().item()\n",
        "        return correct\n",
        "\n",
        "def select_model(model, num_classes):\n",
        "    if model == 'resnet18':\n",
        "        model_ = models.resnet18(pretrained=True)\n",
        "        model_.fc = nn.Linear(512, num_classes)\n",
        "    elif model == 'resnet34':\n",
        "        model_ = models.cc(pretrained=True)\n",
        "        model_.fc = nn.Linear(512, num_classes)\n",
        "    elif model == 'resnet50':\n",
        "        model_ = models.resnet50(pretrained=True)\n",
        "        model_.fc = nn.Linear(2048, num_classes)\n",
        "    elif model == 'resnet101':\n",
        "        model_ = models.resnet101(pretrained=True)\n",
        "        model_.fc = nn.Linear(2048, num_classes)\n",
        "    elif model == 'resnet152':\n",
        "        model_ = models.resnet152(pretrained=True)\n",
        "        model_.fc = nn.Linear(2048, num_classes)\n",
        "    elif model == 'densenet121':\n",
        "        model_ = models.densenet121(pretrained=True)\n",
        "        model_.classifier = nn.Linear(1024, num_classes)\n",
        "    elif model == 'densenet169':\n",
        "        model_ = models.densenet169(pretrained=True)\n",
        "        model_.classifier = nn.Linear(1664, num_classes)\n",
        "    elif model == 'densenet201':\n",
        "        model_ = models.densenet201(pretrained=True)\n",
        "        model_.classifier = nn.Linear(1920, num_classes)\n",
        "    elif model == 'densenet161':\n",
        "        model_ = models.densenet161(pretrained=True)\n",
        "        model_.classifier = nn.Linear(2208, num_classes)\n",
        "    elif model == 'inception_v3':\n",
        "        model_ = models.inception_v3(pretrained=True)\n",
        "        model_.fc = nn.Linear(512, num_classes)\n",
        "    elif model == 'googlenet':\n",
        "        model_ = models.googlenet(pretrained=True)\n",
        "        model_.fc = nn.Linear(1024, num_classes)\n",
        "    elif model == 'shufflenet_v2_x1_0':\n",
        "        model_ = models.shufflenet_v2_x1_0(pretrained=True)\n",
        "        model_.fc = nn.Linear(1024, num_classes)\n",
        "    elif model == 'alexnet':\n",
        "        model_ = models.alexnet(pretrained=True)\n",
        "        model_.fc = nn.Linear(512, num_classes)\n",
        "    elif model == 'squeezenet1_0':\n",
        "        model_ = models.squeezenet1_0(pretrained=True)\n",
        "        model_.fc = nn.Linear(512, num_classes)\n",
        "    elif model == 'squeezenet1_1':\n",
        "        model_ = models.squeezenet1_1(pretrained=True)\n",
        "        model_.fc = nn.Linear(512, num_classes)\n",
        "    elif model == 'vgg11':\n",
        "        model_ = models.vgg11(pretrained=True)\n",
        "        model_.classifier = nn.Sequential(\n",
        "            nn.Linear(in_features=25088, out_features=4096, bias=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5, inplace=False),\n",
        "            nn.Linear(in_features=4096, out_features=4096, bias=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5, inplace=False),\n",
        "            nn.Linear(4096, num_classes, bias=True),\n",
        "        )\n",
        "    elif model == 'vgg13':\n",
        "        model_ = models.vgg13(pretrained=True)\n",
        "        model_.classifier = nn.Linear(25088, num_classes)\n",
        "    elif model == 'vgg16':\n",
        "        model_ = models.vgg16(pretrained=True)\n",
        "        model_.classifier = nn.Linear(25088, num_classes)\n",
        "    elif model == 'vgg19':\n",
        "        model_ = models.vgg19(pretrained=True)\n",
        "        model_.classifier = nn.Linear(25088, num_classes)\n",
        "    return model_\n",
        "\n",
        "class Baseline():\n",
        "    def __init__(self, model, num_classes, gpu_id=0, epoch_print=1, print_freq=10, save=False):\n",
        "        self.gpu = gpu_id\n",
        "        self.epoch_print = epoch_print\n",
        "        self.print_freq = print_freq\n",
        "        self.save = save\n",
        "\n",
        "        torch.cuda.set_device(self.gpu)\n",
        "\n",
        "        self.loss_function = nn.CrossEntropyLoss().cuda(self.gpu)\n",
        "\n",
        "        model = select_model(model, num_classes)\n",
        "        self.model = model.cuda(self.gpu)\n",
        "\n",
        "        self.train_losses, self.test_losses = [], []\n",
        "        self.train_acc, self.test_acc = [], []\n",
        "        self.best_acc = None\n",
        "        self.best_loss = None\n",
        "\n",
        "    def train(self, train_data, test_data, epochs=100, lr=0.1, weight_decay=0.0001):\n",
        "        self.model.train()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr, weight_decay=weight_decay)\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            if epoch % self.epoch_print == 0: print('Epoch {} Started...'.format(epoch+1))\n",
        "            for i, (X, y) in enumerate(train_data):\n",
        "                X, y = X.cuda(self.gpu), y.cuda(self.gpu)\n",
        "                output = self.model(X)\n",
        "                loss = self.loss_function(output, y)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                if (epoch % self.epoch_print == 0) and (i % self.print_freq == 0):\n",
        "                    train_acc = 100 * count(output, y) / y.size(0)\n",
        "                    test_acc, test_loss = self.test(test_data)\n",
        "                    \n",
        "                    if self.save and ((self.best_acc == None) or (self.best_acc < test_acc) or (test_loss < self.best_loss)):\n",
        "                        torch.save(self.model.state_dict(), '{}_{}.pt'.format(epoch, i))\n",
        "                        self.best_acc = test_acc\n",
        "                        self.best_loss = test_loss\n",
        "                        print('Best Model Saved')\n",
        "\n",
        "                    self.train_losses.append(loss.item())\n",
        "                    self.train_acc.append(train_acc)\n",
        "                    self.test_losses.append(test_loss)\n",
        "                    self.test_acc.append(test_acc)\n",
        "\n",
        "                    print('Iteration : {} - Train Loss : {:.6f}, Test Loss : {:.6f}, '\n",
        "                          'Train Acc : {:.6f}, Test Acc : {:.6f}'.format(i+1, loss.item(), test_loss, train_acc, test_acc))\n",
        "            print()\n",
        "\n",
        "    def test(self, test_data):\n",
        "        correct, total = 0, 0\n",
        "        losses = []\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, (X, y) in enumerate(test_data):\n",
        "                X, y = X.cuda(self.gpu), y.cuda(self.gpu)\n",
        "                output = self.model(X)\n",
        "\n",
        "                loss = self.loss_function(output, y)\n",
        "                losses.append(loss.item())\n",
        "                \n",
        "                correct += count(output, y)\n",
        "                total += y.size(0)\n",
        "        self.model.train()\n",
        "        return (100*correct/total, sum(losses)/len(losses))"
      ],
      "id": "AOsYBUvQ6I8K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71wHVNZF6kXY"
      },
      "source": [
        "data_dir = './train/'\n",
        "imgs, labels = [], []\n",
        "for num in sorted(os.listdir(data_dir)):\n",
        "    with open(data_dir + '{}/{}.json'.format(num, num), 'r') as j:\n",
        "        temp = json.load(j)\n",
        "        for info in temp['annotations']:\n",
        "            imgs.append(data_dir + '{}/{}.png'.format(num, info['image_id']))\n",
        "            labels.append(temp['action'][0])\n",
        "\n",
        "label_info = {label:i for i, label in enumerate(sorted(set(labels)))}\n",
        "train_imgs, val_imgs, train_labels, val_labels = train_test_split(imgs, labels, random_state=0, stratify=labels)\n",
        "\n",
        "class TrainDataset(Dataset):\n",
        "    def __init__(self, transform=None):\n",
        "        self.imgs = train_imgs\n",
        "        self.labels = train_labels\n",
        "        self.label_info = label_info\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = PIL.Image.open(self.imgs[idx]).convert('RGB')\n",
        "        if self.transform: img = self.transform(img)\n",
        "\n",
        "        label = self.label_info[self.labels[idx]]\n",
        "        return img, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "    \n",
        "class ValDataset(Dataset):\n",
        "    def __init__(self, transform=None):\n",
        "        self.imgs = val_imgs\n",
        "        self.labels = val_labels\n",
        "        self.label_info = label_info\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = PIL.Image.open(self.imgs[idx]).convert('RGB')\n",
        "        if self.transform: img = self.transform(img)\n",
        "\n",
        "        label = self.label_info[self.labels[idx]]\n",
        "        return img, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "id": "71wHVNZF6kXY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7f97ab6"
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(256), transforms.RandomCrop((224, 400)),\n",
        "    transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize(256), transforms.CenterCrop((224, 400)),\n",
        "    transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])"
      ],
      "id": "d7f97ab6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wly9AjM347Nk"
      },
      "source": [
        "train_dataset = TrainDataset(transform=train_transform)\n",
        "val_dataset = ValDataset(transform=val_transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "id": "Wly9AjM347Nk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff8fd340",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "6d8ac7622ea342b28c1ec135a9d6bf0f",
            "d2fbcb34ab2c4522985771b236665e95",
            "fb2633ceaa714f29b8b8b9e7c1548fcc",
            "81d2a4b3f0aa456cbf78b856cada3c18",
            "e8572b9098af4cbb8c738476506a2433",
            "e332e7272c04414ba8ddbcd4cb5059ae",
            "6435653a26ca4490929ec775e96e82a0",
            "05579c6806c84a34805b3ee0830d6677",
            "a1ef5743b83a48da8a14fb65ae3b2c9c",
            "69821d23ccf74b89a6b50572f0b7b9a6",
            "60e0c18f1bdb4833a9e41f6b8b1b4006"
          ]
        },
        "outputId": "b25c3dbd-892b-46e2-dc6b-8a0dcc8bc4ed"
      },
      "source": [
        "model = Baseline(model='vgg13', num_classes=len(train_dataset.label_info), print_freq=5, save=True)"
      ],
      "id": "ff8fd340",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg13-19584684.pth\" to /root/.cache/torch/hub/checkpoints/vgg13-19584684.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d8ac7622ea342b28c1ec135a9d6bf0f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/508M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peiTg0WkZpJM",
        "outputId": "ac1b8a74-0383-4e9c-8564-ac1fcc350dd4"
      },
      "source": [
        "len(train_dataset.label_info)"
      ],
      "id": "peiTg0WkZpJM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "157"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb07767c"
      },
      "source": [
        "epochs = 5\n",
        "lr = 0.0005\n",
        "weight_decay = 0.00001"
      ],
      "id": "cb07767c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ae6f71d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8902a78-8176-470c-9968-5d2086198efd"
      },
      "source": [
        "model.train(train_loader, val_loader, epochs=epochs, lr=lr, weight_decay=weight_decay)"
      ],
      "id": "3ae6f71d",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Started...\n",
            "Best Model Saved\n",
            "Iteration : 1 - Train Loss : 5.012030, Test Loss : 5.541494, Train Acc : 6.250000, Test Acc : 6.861413\n",
            "Best Model Saved\n",
            "Iteration : 6 - Train Loss : 5.063785, Test Loss : 4.845655, Train Acc : 3.125000, Test Acc : 3.396739\n",
            "Best Model Saved\n",
            "Iteration : 11 - Train Loss : 4.610245, Test Loss : 4.512028, Train Acc : 3.125000, Test Acc : 5.095109\n",
            "Best Model Saved\n",
            "Iteration : 16 - Train Loss : 4.508835, Test Loss : 4.175808, Train Acc : 6.250000, Test Acc : 8.220109\n",
            "Best Model Saved\n",
            "Iteration : 21 - Train Loss : 4.040812, Test Loss : 3.953782, Train Acc : 6.250000, Test Acc : 11.345109\n",
            "Best Model Saved\n",
            "Iteration : 26 - Train Loss : 3.684396, Test Loss : 3.505341, Train Acc : 12.500000, Test Acc : 16.440217\n",
            "Best Model Saved\n",
            "Iteration : 31 - Train Loss : 3.272983, Test Loss : 3.244761, Train Acc : 15.625000, Test Acc : 22.010870\n",
            "Best Model Saved\n",
            "Iteration : 36 - Train Loss : 3.121511, Test Loss : 2.943671, Train Acc : 21.875000, Test Acc : 24.524457\n",
            "Best Model Saved\n",
            "Iteration : 41 - Train Loss : 3.174985, Test Loss : 2.674334, Train Acc : 31.250000, Test Acc : 28.057065\n",
            "Best Model Saved\n",
            "Iteration : 46 - Train Loss : 2.825627, Test Loss : 2.575826, Train Acc : 12.500000, Test Acc : 28.804348\n",
            "Best Model Saved\n",
            "Iteration : 51 - Train Loss : 2.302351, Test Loss : 2.343946, Train Acc : 31.250000, Test Acc : 33.491848\n",
            "Best Model Saved\n",
            "Iteration : 56 - Train Loss : 2.621500, Test Loss : 2.146665, Train Acc : 21.875000, Test Acc : 37.500000\n",
            "Best Model Saved\n",
            "Iteration : 61 - Train Loss : 2.510528, Test Loss : 2.014225, Train Acc : 34.375000, Test Acc : 39.945652\n",
            "Best Model Saved\n",
            "Iteration : 66 - Train Loss : 2.138003, Test Loss : 1.827531, Train Acc : 37.500000, Test Acc : 42.730978\n",
            "Best Model Saved\n",
            "Iteration : 71 - Train Loss : 1.798245, Test Loss : 1.640326, Train Acc : 43.750000, Test Acc : 48.980978\n",
            "Best Model Saved\n",
            "Iteration : 76 - Train Loss : 2.324023, Test Loss : 1.561787, Train Acc : 28.125000, Test Acc : 52.309783\n",
            "Best Model Saved\n",
            "Iteration : 81 - Train Loss : 1.442366, Test Loss : 1.523790, Train Acc : 50.000000, Test Acc : 51.766304\n",
            "Best Model Saved\n",
            "Iteration : 86 - Train Loss : 1.958162, Test Loss : 1.363274, Train Acc : 56.250000, Test Acc : 58.899457\n",
            "Best Model Saved\n",
            "Iteration : 91 - Train Loss : 1.315994, Test Loss : 1.231228, Train Acc : 53.125000, Test Acc : 62.296196\n",
            "Iteration : 96 - Train Loss : 1.959122, Test Loss : 1.247744, Train Acc : 43.750000, Test Acc : 59.442935\n",
            "Best Model Saved\n",
            "Iteration : 101 - Train Loss : 1.265081, Test Loss : 1.122869, Train Acc : 56.250000, Test Acc : 63.858696\n",
            "Best Model Saved\n",
            "Iteration : 106 - Train Loss : 1.001671, Test Loss : 1.079765, Train Acc : 59.375000, Test Acc : 65.964674\n",
            "Iteration : 111 - Train Loss : 1.376191, Test Loss : 1.120347, Train Acc : 56.250000, Test Acc : 65.421196\n",
            "Best Model Saved\n",
            "Iteration : 116 - Train Loss : 0.770806, Test Loss : 0.928797, Train Acc : 84.375000, Test Acc : 69.769022\n",
            "Best Model Saved\n",
            "Iteration : 121 - Train Loss : 1.145274, Test Loss : 0.853878, Train Acc : 62.500000, Test Acc : 72.961957\n",
            "Best Model Saved\n",
            "Iteration : 126 - Train Loss : 1.331427, Test Loss : 0.787306, Train Acc : 50.000000, Test Acc : 73.845109\n",
            "Iteration : 131 - Train Loss : 1.350946, Test Loss : 0.846733, Train Acc : 50.000000, Test Acc : 72.418478\n",
            "Best Model Saved\n",
            "Iteration : 136 - Train Loss : 1.062992, Test Loss : 0.754261, Train Acc : 62.500000, Test Acc : 76.222826\n",
            "\n",
            "Epoch 2 Started...\n",
            "Iteration : 1 - Train Loss : 1.107394, Test Loss : 0.769461, Train Acc : 71.875000, Test Acc : 75.000000\n",
            "Best Model Saved\n",
            "Iteration : 6 - Train Loss : 0.823082, Test Loss : 0.715746, Train Acc : 71.875000, Test Acc : 76.426630\n",
            "Iteration : 11 - Train Loss : 0.646382, Test Loss : 0.718238, Train Acc : 84.375000, Test Acc : 75.271739\n",
            "Best Model Saved\n",
            "Iteration : 16 - Train Loss : 0.797851, Test Loss : 0.625407, Train Acc : 68.750000, Test Acc : 81.385870\n",
            "Iteration : 21 - Train Loss : 0.856214, Test Loss : 0.691712, Train Acc : 75.000000, Test Acc : 77.445652\n",
            "Best Model Saved\n",
            "Iteration : 26 - Train Loss : 0.874449, Test Loss : 0.536307, Train Acc : 65.625000, Test Acc : 82.201087\n",
            "Best Model Saved\n",
            "Iteration : 31 - Train Loss : 0.645898, Test Loss : 0.485518, Train Acc : 81.250000, Test Acc : 84.035326\n",
            "Iteration : 36 - Train Loss : 0.838647, Test Loss : 0.543725, Train Acc : 81.250000, Test Acc : 82.540761\n",
            "Iteration : 41 - Train Loss : 0.740830, Test Loss : 0.511095, Train Acc : 75.000000, Test Acc : 83.967391\n",
            "Iteration : 46 - Train Loss : 0.888341, Test Loss : 0.577744, Train Acc : 65.625000, Test Acc : 81.250000\n",
            "Iteration : 51 - Train Loss : 0.596630, Test Loss : 0.530200, Train Acc : 78.125000, Test Acc : 83.899457\n",
            "Iteration : 56 - Train Loss : 0.734275, Test Loss : 0.573102, Train Acc : 75.000000, Test Acc : 83.288043\n",
            "Best Model Saved\n",
            "Iteration : 61 - Train Loss : 0.799812, Test Loss : 0.469579, Train Acc : 75.000000, Test Acc : 85.801630\n",
            "Best Model Saved\n",
            "Iteration : 66 - Train Loss : 0.709291, Test Loss : 0.389910, Train Acc : 81.250000, Test Acc : 87.907609\n",
            "Best Model Saved\n",
            "Iteration : 71 - Train Loss : 0.620837, Test Loss : 0.329539, Train Acc : 75.000000, Test Acc : 90.285326\n",
            "Best Model Saved\n",
            "Iteration : 76 - Train Loss : 0.625349, Test Loss : 0.315789, Train Acc : 81.250000, Test Acc : 90.964674\n",
            "Best Model Saved\n",
            "Iteration : 81 - Train Loss : 0.428397, Test Loss : 0.294177, Train Acc : 84.375000, Test Acc : 91.100543\n",
            "Iteration : 86 - Train Loss : 0.184852, Test Loss : 0.343530, Train Acc : 93.750000, Test Acc : 89.538043\n",
            "Iteration : 91 - Train Loss : 0.562266, Test Loss : 0.336550, Train Acc : 78.125000, Test Acc : 89.470109\n",
            "Iteration : 96 - Train Loss : 0.762820, Test Loss : 0.329735, Train Acc : 87.500000, Test Acc : 88.994565\n",
            "Best Model Saved\n",
            "Iteration : 101 - Train Loss : 0.436769, Test Loss : 0.260530, Train Acc : 84.375000, Test Acc : 92.187500\n",
            "Best Model Saved\n",
            "Iteration : 106 - Train Loss : 0.345101, Test Loss : 0.255090, Train Acc : 87.500000, Test Acc : 92.323370\n",
            "Iteration : 111 - Train Loss : 0.421031, Test Loss : 0.264994, Train Acc : 87.500000, Test Acc : 91.236413\n",
            "Iteration : 116 - Train Loss : 0.268097, Test Loss : 0.308870, Train Acc : 93.750000, Test Acc : 90.353261\n",
            "Iteration : 121 - Train Loss : 0.383132, Test Loss : 0.297118, Train Acc : 84.375000, Test Acc : 89.605978\n",
            "Best Model Saved\n",
            "Iteration : 126 - Train Loss : 0.648686, Test Loss : 0.229882, Train Acc : 81.250000, Test Acc : 92.663043\n",
            "Best Model Saved\n",
            "Iteration : 131 - Train Loss : 0.260725, Test Loss : 0.251997, Train Acc : 90.625000, Test Acc : 92.730978\n",
            "Best Model Saved\n",
            "Iteration : 136 - Train Loss : 0.582373, Test Loss : 0.239707, Train Acc : 81.250000, Test Acc : 93.002717\n",
            "\n",
            "Epoch 3 Started...\n",
            "Iteration : 1 - Train Loss : 0.235637, Test Loss : 0.251756, Train Acc : 93.750000, Test Acc : 91.508152\n",
            "Best Model Saved\n",
            "Iteration : 6 - Train Loss : 0.529059, Test Loss : 0.204974, Train Acc : 87.500000, Test Acc : 93.682065\n",
            "Iteration : 11 - Train Loss : 0.439639, Test Loss : 0.227571, Train Acc : 84.375000, Test Acc : 93.546196\n",
            "Iteration : 16 - Train Loss : 0.073207, Test Loss : 0.270174, Train Acc : 96.875000, Test Acc : 91.100543\n",
            "Best Model Saved\n",
            "Iteration : 21 - Train Loss : 0.137195, Test Loss : 0.215762, Train Acc : 96.875000, Test Acc : 93.817935\n",
            "Best Model Saved\n",
            "Iteration : 26 - Train Loss : 0.220100, Test Loss : 0.203912, Train Acc : 90.625000, Test Acc : 94.089674\n",
            "Best Model Saved\n",
            "Iteration : 31 - Train Loss : 0.345389, Test Loss : 0.177945, Train Acc : 87.500000, Test Acc : 94.701087\n",
            "Iteration : 36 - Train Loss : 0.193137, Test Loss : 0.184443, Train Acc : 90.625000, Test Acc : 94.089674\n",
            "Best Model Saved\n",
            "Iteration : 41 - Train Loss : 0.076479, Test Loss : 0.170219, Train Acc : 96.875000, Test Acc : 93.682065\n",
            "Best Model Saved\n",
            "Iteration : 46 - Train Loss : 0.130574, Test Loss : 0.118394, Train Acc : 90.625000, Test Acc : 96.263587\n",
            "Iteration : 51 - Train Loss : 0.095333, Test Loss : 0.138337, Train Acc : 96.875000, Test Acc : 95.312500\n",
            "Iteration : 56 - Train Loss : 0.105128, Test Loss : 0.147746, Train Acc : 96.875000, Test Acc : 95.244565\n",
            "Iteration : 61 - Train Loss : 0.478713, Test Loss : 0.122671, Train Acc : 87.500000, Test Acc : 96.263587\n",
            "Iteration : 66 - Train Loss : 0.262240, Test Loss : 0.126947, Train Acc : 90.625000, Test Acc : 96.263587\n",
            "Best Model Saved\n",
            "Iteration : 71 - Train Loss : 0.266304, Test Loss : 0.105354, Train Acc : 90.625000, Test Acc : 97.078804\n",
            "Iteration : 76 - Train Loss : 0.154412, Test Loss : 0.191452, Train Acc : 96.875000, Test Acc : 93.546196\n",
            "Iteration : 81 - Train Loss : 0.176046, Test Loss : 0.144337, Train Acc : 93.750000, Test Acc : 95.584239\n",
            "Iteration : 86 - Train Loss : 0.277620, Test Loss : 0.153056, Train Acc : 90.625000, Test Acc : 95.720109\n",
            "Iteration : 91 - Train Loss : 0.148745, Test Loss : 0.169300, Train Acc : 93.750000, Test Acc : 94.565217\n",
            "Iteration : 96 - Train Loss : 0.445530, Test Loss : 0.148437, Train Acc : 81.250000, Test Acc : 94.633152\n",
            "Iteration : 101 - Train Loss : 0.373707, Test Loss : 0.181919, Train Acc : 90.625000, Test Acc : 93.817935\n",
            "Iteration : 106 - Train Loss : 0.382615, Test Loss : 0.175809, Train Acc : 84.375000, Test Acc : 93.750000\n",
            "Iteration : 111 - Train Loss : 0.411676, Test Loss : 0.146629, Train Acc : 84.375000, Test Acc : 94.904891\n",
            "Iteration : 116 - Train Loss : 0.336335, Test Loss : 0.180046, Train Acc : 87.500000, Test Acc : 94.701087\n",
            "Iteration : 121 - Train Loss : 0.289791, Test Loss : 0.121700, Train Acc : 84.375000, Test Acc : 95.516304\n",
            "Iteration : 126 - Train Loss : 0.084565, Test Loss : 0.146168, Train Acc : 96.875000, Test Acc : 95.040761\n",
            "Iteration : 131 - Train Loss : 0.736407, Test Loss : 0.231416, Train Acc : 81.250000, Test Acc : 93.070652\n",
            "Iteration : 136 - Train Loss : 0.189329, Test Loss : 0.149127, Train Acc : 90.625000, Test Acc : 95.108696\n",
            "\n",
            "Epoch 4 Started...\n",
            "Iteration : 1 - Train Loss : 0.208509, Test Loss : 0.201907, Train Acc : 93.750000, Test Acc : 92.663043\n",
            "Iteration : 6 - Train Loss : 0.139167, Test Loss : 0.250900, Train Acc : 96.875000, Test Acc : 93.002717\n",
            "Iteration : 11 - Train Loss : 0.396782, Test Loss : 0.144470, Train Acc : 81.250000, Test Acc : 94.633152\n",
            "Iteration : 16 - Train Loss : 0.251258, Test Loss : 0.142012, Train Acc : 87.500000, Test Acc : 95.652174\n",
            "Iteration : 21 - Train Loss : 0.363197, Test Loss : 0.165234, Train Acc : 90.625000, Test Acc : 94.429348\n",
            "Iteration : 26 - Train Loss : 0.084281, Test Loss : 0.114768, Train Acc : 96.875000, Test Acc : 95.720109\n",
            "Iteration : 31 - Train Loss : 0.042775, Test Loss : 0.113386, Train Acc : 96.875000, Test Acc : 96.263587\n",
            "Iteration : 36 - Train Loss : 0.173359, Test Loss : 0.119679, Train Acc : 90.625000, Test Acc : 96.127717\n",
            "Iteration : 41 - Train Loss : 0.211686, Test Loss : 0.132016, Train Acc : 93.750000, Test Acc : 95.652174\n",
            "Best Model Saved\n",
            "Iteration : 46 - Train Loss : 0.088925, Test Loss : 0.101617, Train Acc : 96.875000, Test Acc : 96.195652\n",
            "Best Model Saved\n",
            "Iteration : 51 - Train Loss : 0.131089, Test Loss : 0.103347, Train Acc : 96.875000, Test Acc : 96.875000\n",
            "Best Model Saved\n",
            "Iteration : 56 - Train Loss : 0.242984, Test Loss : 0.087005, Train Acc : 93.750000, Test Acc : 97.622283\n",
            "Iteration : 61 - Train Loss : 0.058866, Test Loss : 0.098063, Train Acc : 96.875000, Test Acc : 96.195652\n",
            "Iteration : 66 - Train Loss : 0.046791, Test Loss : 0.125891, Train Acc : 100.000000, Test Acc : 96.263587\n",
            "Iteration : 71 - Train Loss : 0.089145, Test Loss : 0.158662, Train Acc : 96.875000, Test Acc : 94.701087\n",
            "Iteration : 76 - Train Loss : 0.159055, Test Loss : 0.110130, Train Acc : 90.625000, Test Acc : 96.263587\n",
            "Iteration : 81 - Train Loss : 0.166982, Test Loss : 0.103436, Train Acc : 93.750000, Test Acc : 96.739130\n",
            "Best Model Saved\n",
            "Iteration : 86 - Train Loss : 0.187231, Test Loss : 0.081305, Train Acc : 93.750000, Test Acc : 97.961957\n",
            "Iteration : 91 - Train Loss : 0.176401, Test Loss : 0.117126, Train Acc : 90.625000, Test Acc : 96.127717\n",
            "Iteration : 96 - Train Loss : 0.098540, Test Loss : 0.116924, Train Acc : 93.750000, Test Acc : 96.127717\n",
            "Iteration : 101 - Train Loss : 0.056061, Test Loss : 0.116249, Train Acc : 100.000000, Test Acc : 96.535326\n",
            "Iteration : 106 - Train Loss : 0.021200, Test Loss : 0.125143, Train Acc : 100.000000, Test Acc : 95.788043\n",
            "Best Model Saved\n",
            "Iteration : 111 - Train Loss : 0.221802, Test Loss : 0.060092, Train Acc : 93.750000, Test Acc : 98.437500\n",
            "Iteration : 116 - Train Loss : 0.018481, Test Loss : 0.121958, Train Acc : 100.000000, Test Acc : 96.059783\n",
            "Iteration : 121 - Train Loss : 0.312205, Test Loss : 0.074095, Train Acc : 90.625000, Test Acc : 98.301630\n",
            "Iteration : 126 - Train Loss : 0.354429, Test Loss : 0.141538, Train Acc : 93.750000, Test Acc : 94.361413\n",
            "Iteration : 131 - Train Loss : 0.223729, Test Loss : 0.099638, Train Acc : 93.750000, Test Acc : 96.671196\n",
            "Iteration : 136 - Train Loss : 0.419953, Test Loss : 0.083038, Train Acc : 84.375000, Test Acc : 97.690217\n",
            "\n",
            "Epoch 5 Started...\n",
            "Iteration : 1 - Train Loss : 0.088196, Test Loss : 0.072521, Train Acc : 93.750000, Test Acc : 98.165761\n",
            "Iteration : 6 - Train Loss : 0.161158, Test Loss : 0.151456, Train Acc : 96.875000, Test Acc : 95.516304\n",
            "Iteration : 11 - Train Loss : 0.050747, Test Loss : 0.084169, Train Acc : 96.875000, Test Acc : 97.418478\n",
            "Iteration : 16 - Train Loss : 0.165437, Test Loss : 0.069183, Train Acc : 93.750000, Test Acc : 97.758152\n",
            "Iteration : 21 - Train Loss : 0.118978, Test Loss : 0.071064, Train Acc : 93.750000, Test Acc : 97.690217\n",
            "Iteration : 26 - Train Loss : 0.164526, Test Loss : 0.076785, Train Acc : 93.750000, Test Acc : 98.029891\n",
            "Best Model Saved\n",
            "Iteration : 31 - Train Loss : 0.348624, Test Loss : 0.036462, Train Acc : 96.875000, Test Acc : 99.116848\n",
            "Iteration : 36 - Train Loss : 0.139527, Test Loss : 0.073087, Train Acc : 96.875000, Test Acc : 98.097826\n",
            "Iteration : 41 - Train Loss : 0.024343, Test Loss : 0.077576, Train Acc : 100.000000, Test Acc : 97.826087\n",
            "Iteration : 46 - Train Loss : 0.598365, Test Loss : 0.129041, Train Acc : 84.375000, Test Acc : 95.855978\n",
            "Iteration : 51 - Train Loss : 0.057379, Test Loss : 0.083157, Train Acc : 96.875000, Test Acc : 97.418478\n",
            "Iteration : 56 - Train Loss : 0.080355, Test Loss : 0.069857, Train Acc : 96.875000, Test Acc : 97.826087\n",
            "Iteration : 61 - Train Loss : 0.035622, Test Loss : 0.046187, Train Acc : 100.000000, Test Acc : 98.913043\n",
            "Iteration : 66 - Train Loss : 0.148446, Test Loss : 0.052244, Train Acc : 96.875000, Test Acc : 98.437500\n",
            "Iteration : 71 - Train Loss : 0.417883, Test Loss : 0.065510, Train Acc : 96.875000, Test Acc : 97.894022\n",
            "Iteration : 76 - Train Loss : 0.019118, Test Loss : 0.107528, Train Acc : 100.000000, Test Acc : 96.942935\n",
            "Iteration : 81 - Train Loss : 0.138593, Test Loss : 0.101464, Train Acc : 96.875000, Test Acc : 97.010870\n",
            "Iteration : 86 - Train Loss : 0.021252, Test Loss : 0.053770, Train Acc : 100.000000, Test Acc : 98.165761\n",
            "Iteration : 91 - Train Loss : 0.090323, Test Loss : 0.061769, Train Acc : 93.750000, Test Acc : 98.233696\n",
            "Iteration : 96 - Train Loss : 0.111539, Test Loss : 0.079554, Train Acc : 93.750000, Test Acc : 97.961957\n",
            "Iteration : 101 - Train Loss : 0.048059, Test Loss : 0.073943, Train Acc : 96.875000, Test Acc : 97.961957\n",
            "Iteration : 106 - Train Loss : 0.067526, Test Loss : 0.061108, Train Acc : 96.875000, Test Acc : 98.029891\n",
            "Iteration : 111 - Train Loss : 0.137268, Test Loss : 0.051165, Train Acc : 93.750000, Test Acc : 98.097826\n",
            "Iteration : 116 - Train Loss : 0.012161, Test Loss : 0.038101, Train Acc : 100.000000, Test Acc : 98.845109\n",
            "Iteration : 121 - Train Loss : 0.035630, Test Loss : 0.047077, Train Acc : 100.000000, Test Acc : 98.505435\n",
            "Iteration : 126 - Train Loss : 0.160594, Test Loss : 0.047580, Train Acc : 93.750000, Test Acc : 98.573370\n",
            "Iteration : 131 - Train Loss : 0.060514, Test Loss : 0.044522, Train Acc : 96.875000, Test Acc : 98.641304\n",
            "Iteration : 136 - Train Loss : 0.022970, Test Loss : 0.062958, Train Acc : 100.000000, Test Acc : 98.301630\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e1f37fc"
      },
      "source": [
        "label_fontsize = 25\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "train_lossline, = plt.plot(model.train_losses, label='Train')\n",
        "test_lossline, = plt.plot(model.test_losses, color='red', label='Test')\n",
        "plt.legend(handles=[train_lossline, test_lossline], fontsize=20)\n",
        "plt.xlabel('Step', fontsize=label_fontsize)\n",
        "plt.ylabel('Loss', fontsize=label_fontsize)\n",
        "plt.show()"
      ],
      "id": "6e1f37fc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c76d7e20"
      },
      "source": [
        "plt.figure(figsize=(20, 10))\n",
        "train_accline, = plt.plot(model.train_acc, label='Train')\n",
        "test_accline, = plt.plot(model.test_acc, color='red', label='Test')\n",
        "plt.legend(handles=[train_accline, test_accline], fontsize=20)\n",
        "plt.xlabel('Step', fontsize=label_fontsize)\n",
        "plt.ylabel('Acc', fontsize=label_fontsize)\n",
        "plt.show()"
      ],
      "id": "c76d7e20",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "694ffa6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4c02405-e244-46de-de6f-fe65a98ba221"
      },
      "source": [
        "model = Baseline(model='vgg13', num_classes=157)\n",
        "model.model.load_state_dict(torch.load('./4_30.pt'))\n",
        "model.model.eval()"
      ],
      "id": "694ffa6a",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (16): ReLU(inplace=True)\n",
              "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (20): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (21): ReLU(inplace=True)\n",
              "    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (23): ReLU(inplace=True)\n",
              "    (24): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Linear(in_features=25088, out_features=157, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "59b1822e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63745397-17e4-41ce-cc8f-4d876553474f"
      },
      "source": [
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(256), transforms.CenterCrop((224, 400)),\n",
        "    transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "data_dir = './test/'\n",
        "\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for num in sorted(os.listdir(data_dir)):\n",
        "        with open(data_dir + '{}/{}.json'.format(num, num), 'r') as j:\n",
        "            temp = json.load(j)\n",
        "            imgs = []\n",
        "            for info in temp['annotations']:\n",
        "                img_dir = data_dir + '{}/{}.png'.format(num, info['image_id'])\n",
        "                img = PIL.Image.open(img_dir).convert('RGB')\n",
        "                img = test_transform(img)\n",
        "                imgs.append(img)\n",
        "            imgs = torch.stack(imgs).cuda()\n",
        "            prediction = torch.nn.Softmax(dim=1)(model.model(imgs))\n",
        "            prediction = torch.mean(prediction, dim=0)\n",
        "            \n",
        "            if torch.sum(prediction) != 1: print(torch.sum(prediction))\n",
        "            predictions.append(prediction.cpu().numpy())\n",
        "\n",
        "print(len(predictions))\n",
        "print(len(predictions[0]))"
      ],
      "id": "59b1822e",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "tensor(1.0000, device='cuda:0')\n",
            "217\n",
            "157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f26e0b88"
      },
      "source": [
        "sample_submission = pd.read_csv('./sample_submission.csv')\n",
        "sample_submission.iloc[:,1:] = predictions\n",
        "sample_submission.to_csv('./vgg13_ImageNetStat_4_30_threshno.csv', index=False)"
      ],
      "id": "f26e0b88",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUWR8SAcJD8w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "outputId": "b10315cd-b9e3-4acd-f8f7-cd93108cdff3"
      },
      "source": [
        "sample_submission.iloc[:,1:]"
      ],
      "id": "IUWR8SAcJD8w",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label_0</th>\n",
              "      <th>Label_1</th>\n",
              "      <th>Label_2</th>\n",
              "      <th>Label_3</th>\n",
              "      <th>Label_4</th>\n",
              "      <th>Label_5</th>\n",
              "      <th>Label_6</th>\n",
              "      <th>Label_7</th>\n",
              "      <th>Label_8</th>\n",
              "      <th>Label_9</th>\n",
              "      <th>Label_10</th>\n",
              "      <th>Label_11</th>\n",
              "      <th>Label_12</th>\n",
              "      <th>Label_13</th>\n",
              "      <th>Label_14</th>\n",
              "      <th>Label_15</th>\n",
              "      <th>Label_16</th>\n",
              "      <th>Label_17</th>\n",
              "      <th>Label_18</th>\n",
              "      <th>Label_19</th>\n",
              "      <th>Label_22</th>\n",
              "      <th>Label_23</th>\n",
              "      <th>Label_25</th>\n",
              "      <th>Label_26</th>\n",
              "      <th>Label_27</th>\n",
              "      <th>Label_28</th>\n",
              "      <th>Label_29</th>\n",
              "      <th>Label_31</th>\n",
              "      <th>Label_32</th>\n",
              "      <th>Label_34</th>\n",
              "      <th>Label_35</th>\n",
              "      <th>Label_36</th>\n",
              "      <th>Label_37</th>\n",
              "      <th>Label_39</th>\n",
              "      <th>Label_40</th>\n",
              "      <th>Label_41</th>\n",
              "      <th>Label_42</th>\n",
              "      <th>Label_43</th>\n",
              "      <th>Label_44</th>\n",
              "      <th>Label_47</th>\n",
              "      <th>...</th>\n",
              "      <th>Label_142</th>\n",
              "      <th>Label_143</th>\n",
              "      <th>Label_144</th>\n",
              "      <th>Label_145</th>\n",
              "      <th>Label_146</th>\n",
              "      <th>Label_147</th>\n",
              "      <th>Label_148</th>\n",
              "      <th>Label_149</th>\n",
              "      <th>Label_150</th>\n",
              "      <th>Label_151</th>\n",
              "      <th>Label_153</th>\n",
              "      <th>Label_154</th>\n",
              "      <th>Label_155</th>\n",
              "      <th>Label_156</th>\n",
              "      <th>Label_157</th>\n",
              "      <th>Label_158</th>\n",
              "      <th>Label_159</th>\n",
              "      <th>Label_160</th>\n",
              "      <th>Label_161</th>\n",
              "      <th>Label_162</th>\n",
              "      <th>Label_163</th>\n",
              "      <th>Label_165</th>\n",
              "      <th>Label_167</th>\n",
              "      <th>Label_168</th>\n",
              "      <th>Label_169</th>\n",
              "      <th>Label_171</th>\n",
              "      <th>Label_172</th>\n",
              "      <th>Label_173</th>\n",
              "      <th>Label_174</th>\n",
              "      <th>Label_175</th>\n",
              "      <th>Label_177</th>\n",
              "      <th>Label_186</th>\n",
              "      <th>Label_188</th>\n",
              "      <th>Label_189</th>\n",
              "      <th>Label_190</th>\n",
              "      <th>Label_191</th>\n",
              "      <th>Label_192</th>\n",
              "      <th>Label_193</th>\n",
              "      <th>Label_194</th>\n",
              "      <th>Label_195</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.884743e-01</td>\n",
              "      <td>5.183269e-01</td>\n",
              "      <td>5.015989e-09</td>\n",
              "      <td>1.775953e-12</td>\n",
              "      <td>3.819513e-15</td>\n",
              "      <td>9.200792e-06</td>\n",
              "      <td>2.965444e-03</td>\n",
              "      <td>6.300730e-05</td>\n",
              "      <td>7.638244e-12</td>\n",
              "      <td>1.827128e-11</td>\n",
              "      <td>2.814850e-04</td>\n",
              "      <td>2.675136e-05</td>\n",
              "      <td>3.264304e-10</td>\n",
              "      <td>3.513713e-11</td>\n",
              "      <td>6.913221e-15</td>\n",
              "      <td>2.361769e-07</td>\n",
              "      <td>2.254494e-08</td>\n",
              "      <td>1.203507e-06</td>\n",
              "      <td>2.320463e-11</td>\n",
              "      <td>1.828517e-10</td>\n",
              "      <td>1.496138e-07</td>\n",
              "      <td>5.950398e-03</td>\n",
              "      <td>6.405854e-07</td>\n",
              "      <td>7.054931e-05</td>\n",
              "      <td>4.024530e-02</td>\n",
              "      <td>9.676357e-03</td>\n",
              "      <td>2.315061e-08</td>\n",
              "      <td>5.640425e-14</td>\n",
              "      <td>3.063240e-13</td>\n",
              "      <td>7.882476e-16</td>\n",
              "      <td>3.068610e-02</td>\n",
              "      <td>8.473305e-10</td>\n",
              "      <td>3.569839e-09</td>\n",
              "      <td>3.941489e-13</td>\n",
              "      <td>4.784448e-11</td>\n",
              "      <td>6.636609e-08</td>\n",
              "      <td>1.035294e-09</td>\n",
              "      <td>1.158940e-07</td>\n",
              "      <td>6.071340e-12</td>\n",
              "      <td>2.776976e-11</td>\n",
              "      <td>...</td>\n",
              "      <td>2.785248e-04</td>\n",
              "      <td>6.709577e-12</td>\n",
              "      <td>3.003942e-07</td>\n",
              "      <td>6.742783e-08</td>\n",
              "      <td>3.295213e-10</td>\n",
              "      <td>1.251319e-11</td>\n",
              "      <td>3.440664e-11</td>\n",
              "      <td>1.088457e-06</td>\n",
              "      <td>4.227576e-14</td>\n",
              "      <td>6.501779e-10</td>\n",
              "      <td>1.670530e-13</td>\n",
              "      <td>4.300069e-08</td>\n",
              "      <td>1.239692e-07</td>\n",
              "      <td>8.075891e-13</td>\n",
              "      <td>1.789620e-15</td>\n",
              "      <td>1.509011e-13</td>\n",
              "      <td>1.404298e-14</td>\n",
              "      <td>2.141944e-04</td>\n",
              "      <td>1.589368e-15</td>\n",
              "      <td>8.284636e-17</td>\n",
              "      <td>3.790817e-11</td>\n",
              "      <td>5.373544e-10</td>\n",
              "      <td>2.718707e-06</td>\n",
              "      <td>3.210413e-13</td>\n",
              "      <td>4.328491e-12</td>\n",
              "      <td>5.293727e-15</td>\n",
              "      <td>1.907834e-11</td>\n",
              "      <td>1.021603e-12</td>\n",
              "      <td>4.416137e-12</td>\n",
              "      <td>4.522575e-11</td>\n",
              "      <td>2.069197e-13</td>\n",
              "      <td>1.224558e-09</td>\n",
              "      <td>3.619710e-10</td>\n",
              "      <td>1.206050e-13</td>\n",
              "      <td>2.564229e-11</td>\n",
              "      <td>1.883859e-11</td>\n",
              "      <td>4.787142e-11</td>\n",
              "      <td>2.419291e-12</td>\n",
              "      <td>8.768307e-06</td>\n",
              "      <td>2.665896e-15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.524751e-09</td>\n",
              "      <td>5.440841e-10</td>\n",
              "      <td>1.609498e-13</td>\n",
              "      <td>2.327241e-15</td>\n",
              "      <td>2.890700e-15</td>\n",
              "      <td>7.153960e-15</td>\n",
              "      <td>4.964187e-11</td>\n",
              "      <td>2.039614e-13</td>\n",
              "      <td>7.101435e-12</td>\n",
              "      <td>2.846561e-15</td>\n",
              "      <td>9.956939e-01</td>\n",
              "      <td>3.634061e-03</td>\n",
              "      <td>2.319569e-07</td>\n",
              "      <td>3.381539e-11</td>\n",
              "      <td>2.781187e-15</td>\n",
              "      <td>8.025970e-09</td>\n",
              "      <td>1.476220e-04</td>\n",
              "      <td>8.749953e-07</td>\n",
              "      <td>6.799344e-11</td>\n",
              "      <td>2.727254e-12</td>\n",
              "      <td>1.937010e-12</td>\n",
              "      <td>1.765395e-08</td>\n",
              "      <td>2.349894e-11</td>\n",
              "      <td>4.903557e-12</td>\n",
              "      <td>2.161868e-11</td>\n",
              "      <td>7.415840e-11</td>\n",
              "      <td>1.173339e-13</td>\n",
              "      <td>2.591429e-13</td>\n",
              "      <td>9.513401e-16</td>\n",
              "      <td>3.739032e-17</td>\n",
              "      <td>5.879671e-09</td>\n",
              "      <td>6.603826e-15</td>\n",
              "      <td>3.438432e-16</td>\n",
              "      <td>5.775720e-17</td>\n",
              "      <td>6.268260e-13</td>\n",
              "      <td>8.919496e-11</td>\n",
              "      <td>5.573237e-07</td>\n",
              "      <td>2.375386e-13</td>\n",
              "      <td>6.727724e-14</td>\n",
              "      <td>7.638511e-09</td>\n",
              "      <td>...</td>\n",
              "      <td>1.419641e-07</td>\n",
              "      <td>3.393977e-13</td>\n",
              "      <td>1.316211e-13</td>\n",
              "      <td>6.646843e-13</td>\n",
              "      <td>3.389391e-11</td>\n",
              "      <td>4.053158e-14</td>\n",
              "      <td>3.664992e-12</td>\n",
              "      <td>8.754707e-12</td>\n",
              "      <td>4.318174e-14</td>\n",
              "      <td>5.844949e-12</td>\n",
              "      <td>1.625834e-09</td>\n",
              "      <td>6.972370e-13</td>\n",
              "      <td>5.792668e-11</td>\n",
              "      <td>7.212199e-16</td>\n",
              "      <td>1.372160e-15</td>\n",
              "      <td>3.947342e-17</td>\n",
              "      <td>2.917994e-15</td>\n",
              "      <td>1.324863e-11</td>\n",
              "      <td>1.867926e-15</td>\n",
              "      <td>1.632721e-15</td>\n",
              "      <td>5.604993e-10</td>\n",
              "      <td>6.208038e-11</td>\n",
              "      <td>4.034134e-09</td>\n",
              "      <td>5.288523e-15</td>\n",
              "      <td>1.739590e-13</td>\n",
              "      <td>7.370305e-19</td>\n",
              "      <td>2.653770e-13</td>\n",
              "      <td>3.919859e-14</td>\n",
              "      <td>8.759804e-13</td>\n",
              "      <td>1.491290e-13</td>\n",
              "      <td>9.864740e-12</td>\n",
              "      <td>3.710496e-12</td>\n",
              "      <td>2.067071e-13</td>\n",
              "      <td>6.804591e-15</td>\n",
              "      <td>2.256847e-14</td>\n",
              "      <td>3.751426e-14</td>\n",
              "      <td>1.265879e-14</td>\n",
              "      <td>2.454404e-13</td>\n",
              "      <td>4.012572e-08</td>\n",
              "      <td>6.894912e-17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.166581e-11</td>\n",
              "      <td>9.081477e-08</td>\n",
              "      <td>8.340741e-10</td>\n",
              "      <td>3.131184e-11</td>\n",
              "      <td>9.568355e-11</td>\n",
              "      <td>3.995507e-20</td>\n",
              "      <td>1.285312e-12</td>\n",
              "      <td>9.124410e-12</td>\n",
              "      <td>2.209190e-09</td>\n",
              "      <td>4.119476e-19</td>\n",
              "      <td>6.060996e-04</td>\n",
              "      <td>9.989052e-01</td>\n",
              "      <td>2.918979e-04</td>\n",
              "      <td>1.700211e-07</td>\n",
              "      <td>7.644567e-12</td>\n",
              "      <td>1.656337e-13</td>\n",
              "      <td>5.541377e-07</td>\n",
              "      <td>1.918086e-04</td>\n",
              "      <td>1.137890e-07</td>\n",
              "      <td>1.859300e-17</td>\n",
              "      <td>6.574854e-14</td>\n",
              "      <td>1.159157e-09</td>\n",
              "      <td>7.722726e-15</td>\n",
              "      <td>1.544887e-14</td>\n",
              "      <td>1.796670e-13</td>\n",
              "      <td>7.560784e-12</td>\n",
              "      <td>1.272226e-16</td>\n",
              "      <td>5.785769e-12</td>\n",
              "      <td>1.695872e-17</td>\n",
              "      <td>1.263680e-16</td>\n",
              "      <td>4.990914e-12</td>\n",
              "      <td>3.125139e-15</td>\n",
              "      <td>4.163067e-21</td>\n",
              "      <td>4.300044e-16</td>\n",
              "      <td>1.295522e-08</td>\n",
              "      <td>4.646100e-13</td>\n",
              "      <td>8.613747e-10</td>\n",
              "      <td>2.704951e-12</td>\n",
              "      <td>1.195894e-18</td>\n",
              "      <td>5.792005e-10</td>\n",
              "      <td>...</td>\n",
              "      <td>7.665417e-09</td>\n",
              "      <td>4.903117e-13</td>\n",
              "      <td>4.727328e-15</td>\n",
              "      <td>5.534487e-15</td>\n",
              "      <td>9.059536e-14</td>\n",
              "      <td>2.239288e-11</td>\n",
              "      <td>1.789853e-12</td>\n",
              "      <td>2.454866e-12</td>\n",
              "      <td>1.616513e-14</td>\n",
              "      <td>2.665338e-13</td>\n",
              "      <td>8.716035e-11</td>\n",
              "      <td>1.650681e-13</td>\n",
              "      <td>1.241203e-10</td>\n",
              "      <td>4.380288e-14</td>\n",
              "      <td>3.264054e-15</td>\n",
              "      <td>3.104969e-16</td>\n",
              "      <td>7.496385e-16</td>\n",
              "      <td>3.308557e-12</td>\n",
              "      <td>2.831604e-15</td>\n",
              "      <td>5.654400e-15</td>\n",
              "      <td>5.424807e-12</td>\n",
              "      <td>3.659250e-09</td>\n",
              "      <td>3.870525e-09</td>\n",
              "      <td>1.001226e-13</td>\n",
              "      <td>1.112721e-14</td>\n",
              "      <td>2.607413e-18</td>\n",
              "      <td>1.811037e-13</td>\n",
              "      <td>9.126924e-15</td>\n",
              "      <td>2.693699e-15</td>\n",
              "      <td>2.193953e-15</td>\n",
              "      <td>6.181311e-12</td>\n",
              "      <td>6.065839e-11</td>\n",
              "      <td>1.302875e-15</td>\n",
              "      <td>2.008108e-14</td>\n",
              "      <td>7.941301e-15</td>\n",
              "      <td>3.261228e-15</td>\n",
              "      <td>4.814622e-16</td>\n",
              "      <td>2.627826e-14</td>\n",
              "      <td>3.024903e-07</td>\n",
              "      <td>1.490195e-17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.967019e-11</td>\n",
              "      <td>2.739590e-08</td>\n",
              "      <td>9.084814e-11</td>\n",
              "      <td>1.393901e-11</td>\n",
              "      <td>3.255005e-13</td>\n",
              "      <td>9.514896e-19</td>\n",
              "      <td>2.290153e-12</td>\n",
              "      <td>1.109970e-11</td>\n",
              "      <td>3.027988e-10</td>\n",
              "      <td>1.775035e-18</td>\n",
              "      <td>6.276065e-04</td>\n",
              "      <td>9.989018e-01</td>\n",
              "      <td>2.278742e-06</td>\n",
              "      <td>5.840040e-09</td>\n",
              "      <td>6.238262e-15</td>\n",
              "      <td>5.767283e-12</td>\n",
              "      <td>5.722371e-07</td>\n",
              "      <td>4.657352e-04</td>\n",
              "      <td>5.218261e-10</td>\n",
              "      <td>4.441248e-18</td>\n",
              "      <td>4.092806e-14</td>\n",
              "      <td>1.636390e-10</td>\n",
              "      <td>8.897553e-15</td>\n",
              "      <td>2.091395e-13</td>\n",
              "      <td>5.808077e-13</td>\n",
              "      <td>1.072188e-12</td>\n",
              "      <td>6.287760e-15</td>\n",
              "      <td>3.236505e-12</td>\n",
              "      <td>2.422514e-17</td>\n",
              "      <td>3.086805e-17</td>\n",
              "      <td>8.294689e-11</td>\n",
              "      <td>1.159963e-15</td>\n",
              "      <td>1.816117e-20</td>\n",
              "      <td>4.302551e-17</td>\n",
              "      <td>8.540428e-09</td>\n",
              "      <td>1.012968e-12</td>\n",
              "      <td>1.897462e-09</td>\n",
              "      <td>1.926034e-12</td>\n",
              "      <td>1.002413e-17</td>\n",
              "      <td>2.009267e-10</td>\n",
              "      <td>...</td>\n",
              "      <td>3.444967e-09</td>\n",
              "      <td>1.322166e-13</td>\n",
              "      <td>5.567457e-15</td>\n",
              "      <td>1.406909e-16</td>\n",
              "      <td>1.690723e-13</td>\n",
              "      <td>3.815122e-11</td>\n",
              "      <td>1.293171e-13</td>\n",
              "      <td>5.039924e-13</td>\n",
              "      <td>1.360565e-16</td>\n",
              "      <td>1.165955e-13</td>\n",
              "      <td>4.762184e-11</td>\n",
              "      <td>3.247756e-14</td>\n",
              "      <td>7.712399e-12</td>\n",
              "      <td>9.937598e-15</td>\n",
              "      <td>3.692877e-15</td>\n",
              "      <td>1.623776e-17</td>\n",
              "      <td>2.860336e-16</td>\n",
              "      <td>2.462860e-11</td>\n",
              "      <td>6.081465e-16</td>\n",
              "      <td>1.736796e-15</td>\n",
              "      <td>1.071662e-11</td>\n",
              "      <td>2.835072e-11</td>\n",
              "      <td>7.917726e-10</td>\n",
              "      <td>4.619565e-15</td>\n",
              "      <td>3.941254e-14</td>\n",
              "      <td>1.260403e-18</td>\n",
              "      <td>3.020262e-14</td>\n",
              "      <td>1.700311e-15</td>\n",
              "      <td>1.399610e-15</td>\n",
              "      <td>4.123960e-17</td>\n",
              "      <td>3.150258e-12</td>\n",
              "      <td>2.802356e-12</td>\n",
              "      <td>2.593725e-15</td>\n",
              "      <td>3.018380e-15</td>\n",
              "      <td>1.522058e-15</td>\n",
              "      <td>1.719085e-15</td>\n",
              "      <td>5.645634e-16</td>\n",
              "      <td>1.376054e-14</td>\n",
              "      <td>6.670704e-08</td>\n",
              "      <td>3.200180e-18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.199972e-11</td>\n",
              "      <td>2.387229e-05</td>\n",
              "      <td>1.457371e-04</td>\n",
              "      <td>4.203625e-06</td>\n",
              "      <td>1.229533e-06</td>\n",
              "      <td>2.897319e-18</td>\n",
              "      <td>1.366991e-10</td>\n",
              "      <td>9.009617e-08</td>\n",
              "      <td>4.054251e-06</td>\n",
              "      <td>8.935805e-18</td>\n",
              "      <td>1.579049e-06</td>\n",
              "      <td>2.496028e-01</td>\n",
              "      <td>7.484688e-01</td>\n",
              "      <td>4.417144e-04</td>\n",
              "      <td>5.026354e-07</td>\n",
              "      <td>2.809859e-13</td>\n",
              "      <td>2.881217e-09</td>\n",
              "      <td>5.488946e-05</td>\n",
              "      <td>1.165760e-03</td>\n",
              "      <td>3.267201e-16</td>\n",
              "      <td>3.269861e-11</td>\n",
              "      <td>1.046955e-08</td>\n",
              "      <td>1.105552e-11</td>\n",
              "      <td>1.114015e-12</td>\n",
              "      <td>7.684585e-12</td>\n",
              "      <td>4.245478e-11</td>\n",
              "      <td>2.030351e-15</td>\n",
              "      <td>8.776484e-09</td>\n",
              "      <td>2.571257e-16</td>\n",
              "      <td>1.517704e-14</td>\n",
              "      <td>4.231502e-12</td>\n",
              "      <td>2.957742e-12</td>\n",
              "      <td>2.836134e-20</td>\n",
              "      <td>3.397533e-11</td>\n",
              "      <td>2.913260e-05</td>\n",
              "      <td>1.767974e-12</td>\n",
              "      <td>5.393226e-10</td>\n",
              "      <td>3.411859e-10</td>\n",
              "      <td>5.923005e-19</td>\n",
              "      <td>5.156496e-08</td>\n",
              "      <td>...</td>\n",
              "      <td>3.887361e-08</td>\n",
              "      <td>5.307233e-12</td>\n",
              "      <td>1.829840e-14</td>\n",
              "      <td>8.674579e-14</td>\n",
              "      <td>4.711843e-14</td>\n",
              "      <td>2.063605e-08</td>\n",
              "      <td>6.916800e-11</td>\n",
              "      <td>3.755401e-11</td>\n",
              "      <td>1.390058e-11</td>\n",
              "      <td>3.489064e-12</td>\n",
              "      <td>3.064394e-09</td>\n",
              "      <td>2.181633e-11</td>\n",
              "      <td>4.062532e-08</td>\n",
              "      <td>3.909556e-09</td>\n",
              "      <td>1.261538e-12</td>\n",
              "      <td>9.177946e-14</td>\n",
              "      <td>1.382137e-15</td>\n",
              "      <td>8.622250e-12</td>\n",
              "      <td>1.167703e-13</td>\n",
              "      <td>6.220261e-14</td>\n",
              "      <td>8.391583e-12</td>\n",
              "      <td>6.839596e-07</td>\n",
              "      <td>3.080693e-10</td>\n",
              "      <td>7.563647e-12</td>\n",
              "      <td>1.214085e-15</td>\n",
              "      <td>1.726633e-15</td>\n",
              "      <td>1.980348e-11</td>\n",
              "      <td>2.462120e-12</td>\n",
              "      <td>9.574828e-15</td>\n",
              "      <td>1.477323e-13</td>\n",
              "      <td>1.887798e-09</td>\n",
              "      <td>1.404931e-09</td>\n",
              "      <td>5.096717e-15</td>\n",
              "      <td>1.590207e-10</td>\n",
              "      <td>6.670696e-13</td>\n",
              "      <td>4.820074e-13</td>\n",
              "      <td>2.103714e-13</td>\n",
              "      <td>6.345705e-13</td>\n",
              "      <td>7.038193e-06</td>\n",
              "      <td>3.410425e-15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>212</th>\n",
              "      <td>7.049466e-19</td>\n",
              "      <td>5.143551e-20</td>\n",
              "      <td>1.255319e-18</td>\n",
              "      <td>2.090159e-18</td>\n",
              "      <td>4.274460e-15</td>\n",
              "      <td>8.246869e-11</td>\n",
              "      <td>2.741753e-12</td>\n",
              "      <td>1.442824e-13</td>\n",
              "      <td>1.805567e-13</td>\n",
              "      <td>3.888019e-15</td>\n",
              "      <td>3.843207e-15</td>\n",
              "      <td>1.887135e-19</td>\n",
              "      <td>4.553666e-18</td>\n",
              "      <td>2.285891e-16</td>\n",
              "      <td>1.117741e-14</td>\n",
              "      <td>2.771871e-13</td>\n",
              "      <td>1.221016e-15</td>\n",
              "      <td>1.257001e-13</td>\n",
              "      <td>5.055245e-14</td>\n",
              "      <td>6.670183e-13</td>\n",
              "      <td>1.914497e-18</td>\n",
              "      <td>1.961093e-16</td>\n",
              "      <td>2.281040e-13</td>\n",
              "      <td>1.501901e-16</td>\n",
              "      <td>2.008670e-16</td>\n",
              "      <td>3.242209e-17</td>\n",
              "      <td>6.105154e-11</td>\n",
              "      <td>6.659008e-16</td>\n",
              "      <td>2.457375e-14</td>\n",
              "      <td>1.768634e-12</td>\n",
              "      <td>1.423668e-11</td>\n",
              "      <td>3.848636e-15</td>\n",
              "      <td>3.985063e-12</td>\n",
              "      <td>4.053998e-12</td>\n",
              "      <td>3.502806e-20</td>\n",
              "      <td>2.176145e-11</td>\n",
              "      <td>5.526819e-17</td>\n",
              "      <td>5.008019e-16</td>\n",
              "      <td>4.764197e-15</td>\n",
              "      <td>6.351756e-17</td>\n",
              "      <td>...</td>\n",
              "      <td>1.309289e-15</td>\n",
              "      <td>8.505243e-15</td>\n",
              "      <td>1.159189e-10</td>\n",
              "      <td>1.794973e-15</td>\n",
              "      <td>1.243418e-13</td>\n",
              "      <td>5.931568e-17</td>\n",
              "      <td>4.457311e-15</td>\n",
              "      <td>6.411161e-14</td>\n",
              "      <td>1.018781e-10</td>\n",
              "      <td>6.734933e-14</td>\n",
              "      <td>8.925216e-16</td>\n",
              "      <td>8.313016e-11</td>\n",
              "      <td>2.085974e-14</td>\n",
              "      <td>9.834341e-18</td>\n",
              "      <td>1.772510e-15</td>\n",
              "      <td>2.278410e-12</td>\n",
              "      <td>1.930234e-10</td>\n",
              "      <td>3.059499e-12</td>\n",
              "      <td>3.655885e-17</td>\n",
              "      <td>1.322158e-09</td>\n",
              "      <td>6.605886e-14</td>\n",
              "      <td>2.595969e-15</td>\n",
              "      <td>1.581981e-14</td>\n",
              "      <td>1.044754e-17</td>\n",
              "      <td>7.114104e-14</td>\n",
              "      <td>1.065658e-09</td>\n",
              "      <td>1.274910e-08</td>\n",
              "      <td>9.286079e-11</td>\n",
              "      <td>4.273370e-08</td>\n",
              "      <td>1.303423e-06</td>\n",
              "      <td>5.368020e-10</td>\n",
              "      <td>2.182552e-12</td>\n",
              "      <td>1.676022e-12</td>\n",
              "      <td>4.693089e-12</td>\n",
              "      <td>2.407341e-10</td>\n",
              "      <td>1.044755e-09</td>\n",
              "      <td>9.999987e-01</td>\n",
              "      <td>5.233764e-11</td>\n",
              "      <td>3.028352e-11</td>\n",
              "      <td>2.324831e-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>213</th>\n",
              "      <td>1.842637e-20</td>\n",
              "      <td>2.551349e-22</td>\n",
              "      <td>2.278988e-19</td>\n",
              "      <td>2.973537e-10</td>\n",
              "      <td>1.652818e-02</td>\n",
              "      <td>1.953454e-17</td>\n",
              "      <td>6.393904e-17</td>\n",
              "      <td>6.249873e-15</td>\n",
              "      <td>4.989591e-11</td>\n",
              "      <td>6.407174e-17</td>\n",
              "      <td>1.439531e-18</td>\n",
              "      <td>1.199521e-20</td>\n",
              "      <td>1.980024e-19</td>\n",
              "      <td>2.309081e-16</td>\n",
              "      <td>2.947694e-06</td>\n",
              "      <td>1.116926e-13</td>\n",
              "      <td>1.963844e-12</td>\n",
              "      <td>7.440979e-11</td>\n",
              "      <td>7.455154e-10</td>\n",
              "      <td>2.412588e-18</td>\n",
              "      <td>6.646773e-21</td>\n",
              "      <td>4.510651e-16</td>\n",
              "      <td>4.383565e-18</td>\n",
              "      <td>2.598645e-18</td>\n",
              "      <td>2.351743e-17</td>\n",
              "      <td>1.341138e-17</td>\n",
              "      <td>5.090615e-09</td>\n",
              "      <td>2.047657e-11</td>\n",
              "      <td>2.395183e-15</td>\n",
              "      <td>7.181450e-10</td>\n",
              "      <td>1.020361e-18</td>\n",
              "      <td>1.297076e-14</td>\n",
              "      <td>7.220822e-12</td>\n",
              "      <td>8.409584e-10</td>\n",
              "      <td>3.327836e-17</td>\n",
              "      <td>1.549274e-11</td>\n",
              "      <td>2.033244e-17</td>\n",
              "      <td>4.298285e-16</td>\n",
              "      <td>5.124457e-16</td>\n",
              "      <td>3.409889e-17</td>\n",
              "      <td>...</td>\n",
              "      <td>2.541664e-15</td>\n",
              "      <td>2.247324e-10</td>\n",
              "      <td>4.015386e-17</td>\n",
              "      <td>5.714965e-16</td>\n",
              "      <td>7.029105e-18</td>\n",
              "      <td>9.012233e-22</td>\n",
              "      <td>3.399343e-14</td>\n",
              "      <td>9.829250e-16</td>\n",
              "      <td>1.193307e-13</td>\n",
              "      <td>1.782012e-11</td>\n",
              "      <td>1.253434e-15</td>\n",
              "      <td>1.035904e-11</td>\n",
              "      <td>1.771672e-15</td>\n",
              "      <td>2.497689e-11</td>\n",
              "      <td>1.912835e-09</td>\n",
              "      <td>9.243716e-15</td>\n",
              "      <td>7.271868e-13</td>\n",
              "      <td>3.333675e-17</td>\n",
              "      <td>6.106179e-19</td>\n",
              "      <td>1.954973e-12</td>\n",
              "      <td>1.678680e-12</td>\n",
              "      <td>2.224871e-13</td>\n",
              "      <td>6.712057e-13</td>\n",
              "      <td>1.542244e-18</td>\n",
              "      <td>3.737851e-16</td>\n",
              "      <td>1.418314e-11</td>\n",
              "      <td>5.340332e-07</td>\n",
              "      <td>1.412324e-05</td>\n",
              "      <td>4.056389e-07</td>\n",
              "      <td>9.401288e-01</td>\n",
              "      <td>1.467922e-04</td>\n",
              "      <td>5.518410e-08</td>\n",
              "      <td>4.201097e-15</td>\n",
              "      <td>6.288791e-10</td>\n",
              "      <td>2.150140e-04</td>\n",
              "      <td>4.157877e-13</td>\n",
              "      <td>6.555449e-06</td>\n",
              "      <td>3.878379e-02</td>\n",
              "      <td>3.991240e-09</td>\n",
              "      <td>1.410809e-11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>214</th>\n",
              "      <td>1.616218e-15</td>\n",
              "      <td>7.455305e-13</td>\n",
              "      <td>7.862151e-12</td>\n",
              "      <td>6.239963e-09</td>\n",
              "      <td>1.294858e-09</td>\n",
              "      <td>1.622653e-19</td>\n",
              "      <td>2.556371e-15</td>\n",
              "      <td>1.376105e-12</td>\n",
              "      <td>7.940474e-10</td>\n",
              "      <td>1.427810e-21</td>\n",
              "      <td>1.273662e-14</td>\n",
              "      <td>1.794507e-12</td>\n",
              "      <td>2.684314e-12</td>\n",
              "      <td>8.671386e-11</td>\n",
              "      <td>1.526629e-09</td>\n",
              "      <td>1.412145e-18</td>\n",
              "      <td>1.990060e-15</td>\n",
              "      <td>5.938160e-11</td>\n",
              "      <td>2.033407e-12</td>\n",
              "      <td>2.136905e-22</td>\n",
              "      <td>3.950482e-17</td>\n",
              "      <td>4.829268e-14</td>\n",
              "      <td>1.498738e-16</td>\n",
              "      <td>9.017354e-15</td>\n",
              "      <td>2.069009e-14</td>\n",
              "      <td>2.433592e-15</td>\n",
              "      <td>7.715991e-16</td>\n",
              "      <td>1.781240e-14</td>\n",
              "      <td>9.160915e-21</td>\n",
              "      <td>6.601867e-17</td>\n",
              "      <td>1.762364e-15</td>\n",
              "      <td>6.457677e-16</td>\n",
              "      <td>9.333011e-20</td>\n",
              "      <td>5.976515e-15</td>\n",
              "      <td>2.062851e-09</td>\n",
              "      <td>2.863477e-15</td>\n",
              "      <td>1.464699e-16</td>\n",
              "      <td>7.944729e-14</td>\n",
              "      <td>1.015740e-20</td>\n",
              "      <td>4.615030e-17</td>\n",
              "      <td>...</td>\n",
              "      <td>8.221069e-14</td>\n",
              "      <td>6.987337e-15</td>\n",
              "      <td>1.070439e-16</td>\n",
              "      <td>8.466658e-17</td>\n",
              "      <td>6.338834e-18</td>\n",
              "      <td>1.188827e-13</td>\n",
              "      <td>3.076123e-12</td>\n",
              "      <td>2.858701e-14</td>\n",
              "      <td>2.226419e-13</td>\n",
              "      <td>9.385376e-14</td>\n",
              "      <td>2.200409e-11</td>\n",
              "      <td>5.452916e-14</td>\n",
              "      <td>5.253594e-12</td>\n",
              "      <td>5.938048e-13</td>\n",
              "      <td>9.971105e-15</td>\n",
              "      <td>2.440351e-16</td>\n",
              "      <td>3.802982e-17</td>\n",
              "      <td>4.659837e-16</td>\n",
              "      <td>4.795385e-17</td>\n",
              "      <td>3.917697e-16</td>\n",
              "      <td>1.417491e-13</td>\n",
              "      <td>7.084013e-08</td>\n",
              "      <td>1.414979e-10</td>\n",
              "      <td>2.310049e-15</td>\n",
              "      <td>4.161938e-17</td>\n",
              "      <td>1.823423e-12</td>\n",
              "      <td>3.863259e-09</td>\n",
              "      <td>1.115575e-11</td>\n",
              "      <td>7.524625e-10</td>\n",
              "      <td>1.333796e-11</td>\n",
              "      <td>6.430555e-09</td>\n",
              "      <td>2.625489e-11</td>\n",
              "      <td>7.976671e-17</td>\n",
              "      <td>1.061094e-09</td>\n",
              "      <td>2.089401e-14</td>\n",
              "      <td>1.052839e-11</td>\n",
              "      <td>1.383622e-14</td>\n",
              "      <td>1.520363e-07</td>\n",
              "      <td>9.999984e-01</td>\n",
              "      <td>1.055389e-13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>215</th>\n",
              "      <td>7.124713e-13</td>\n",
              "      <td>2.565555e-11</td>\n",
              "      <td>3.265200e-09</td>\n",
              "      <td>1.801995e-08</td>\n",
              "      <td>1.724458e-06</td>\n",
              "      <td>4.815157e-14</td>\n",
              "      <td>4.179376e-13</td>\n",
              "      <td>1.303998e-10</td>\n",
              "      <td>1.680593e-07</td>\n",
              "      <td>3.919539e-14</td>\n",
              "      <td>1.350687e-11</td>\n",
              "      <td>1.870346e-09</td>\n",
              "      <td>2.732447e-09</td>\n",
              "      <td>1.325149e-08</td>\n",
              "      <td>9.334943e-09</td>\n",
              "      <td>9.135458e-15</td>\n",
              "      <td>7.839174e-12</td>\n",
              "      <td>4.614341e-10</td>\n",
              "      <td>6.181609e-10</td>\n",
              "      <td>9.484756e-13</td>\n",
              "      <td>2.614503e-13</td>\n",
              "      <td>3.356201e-10</td>\n",
              "      <td>1.247818e-12</td>\n",
              "      <td>1.556756e-12</td>\n",
              "      <td>2.808639e-12</td>\n",
              "      <td>1.559580e-11</td>\n",
              "      <td>2.161967e-11</td>\n",
              "      <td>1.527941e-11</td>\n",
              "      <td>1.739970e-14</td>\n",
              "      <td>3.746128e-12</td>\n",
              "      <td>2.403118e-11</td>\n",
              "      <td>7.586181e-14</td>\n",
              "      <td>7.033886e-14</td>\n",
              "      <td>1.190071e-11</td>\n",
              "      <td>1.032377e-08</td>\n",
              "      <td>6.557220e-12</td>\n",
              "      <td>6.583742e-13</td>\n",
              "      <td>4.419409e-13</td>\n",
              "      <td>3.137888e-14</td>\n",
              "      <td>1.727640e-11</td>\n",
              "      <td>...</td>\n",
              "      <td>3.933251e-11</td>\n",
              "      <td>4.353129e-12</td>\n",
              "      <td>7.863596e-13</td>\n",
              "      <td>1.977252e-12</td>\n",
              "      <td>7.255667e-15</td>\n",
              "      <td>3.972296e-12</td>\n",
              "      <td>3.685127e-11</td>\n",
              "      <td>4.507469e-13</td>\n",
              "      <td>8.831466e-12</td>\n",
              "      <td>5.432727e-12</td>\n",
              "      <td>5.443613e-11</td>\n",
              "      <td>6.802019e-14</td>\n",
              "      <td>9.297081e-13</td>\n",
              "      <td>1.728280e-14</td>\n",
              "      <td>3.942091e-12</td>\n",
              "      <td>7.922818e-14</td>\n",
              "      <td>9.206875e-14</td>\n",
              "      <td>3.096253e-12</td>\n",
              "      <td>4.844017e-15</td>\n",
              "      <td>2.417106e-14</td>\n",
              "      <td>1.200524e-09</td>\n",
              "      <td>5.196953e-07</td>\n",
              "      <td>6.688976e-10</td>\n",
              "      <td>7.958230e-13</td>\n",
              "      <td>7.547067e-15</td>\n",
              "      <td>5.083651e-10</td>\n",
              "      <td>3.477101e-05</td>\n",
              "      <td>8.730204e-08</td>\n",
              "      <td>3.335273e-06</td>\n",
              "      <td>1.789988e-07</td>\n",
              "      <td>3.545400e-06</td>\n",
              "      <td>4.900386e-08</td>\n",
              "      <td>2.114580e-11</td>\n",
              "      <td>1.081727e-08</td>\n",
              "      <td>8.365852e-10</td>\n",
              "      <td>9.782237e-10</td>\n",
              "      <td>4.479250e-10</td>\n",
              "      <td>5.614807e-05</td>\n",
              "      <td>9.998959e-01</td>\n",
              "      <td>1.309729e-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>216</th>\n",
              "      <td>5.129125e-16</td>\n",
              "      <td>6.455750e-17</td>\n",
              "      <td>1.519659e-18</td>\n",
              "      <td>1.322642e-18</td>\n",
              "      <td>2.156635e-18</td>\n",
              "      <td>8.649827e-16</td>\n",
              "      <td>3.789863e-14</td>\n",
              "      <td>3.070528e-15</td>\n",
              "      <td>2.015723e-15</td>\n",
              "      <td>1.536106e-13</td>\n",
              "      <td>2.859644e-16</td>\n",
              "      <td>1.260563e-15</td>\n",
              "      <td>1.344552e-16</td>\n",
              "      <td>2.644014e-18</td>\n",
              "      <td>6.593399e-17</td>\n",
              "      <td>1.105834e-14</td>\n",
              "      <td>2.495625e-15</td>\n",
              "      <td>1.033698e-12</td>\n",
              "      <td>8.632365e-13</td>\n",
              "      <td>3.789059e-14</td>\n",
              "      <td>7.341013e-16</td>\n",
              "      <td>1.498221e-15</td>\n",
              "      <td>2.047594e-13</td>\n",
              "      <td>2.301366e-17</td>\n",
              "      <td>1.430045e-17</td>\n",
              "      <td>1.737987e-19</td>\n",
              "      <td>4.111454e-18</td>\n",
              "      <td>1.302383e-17</td>\n",
              "      <td>2.859307e-15</td>\n",
              "      <td>4.047709e-14</td>\n",
              "      <td>8.017934e-15</td>\n",
              "      <td>6.305178e-16</td>\n",
              "      <td>5.680086e-20</td>\n",
              "      <td>3.349548e-16</td>\n",
              "      <td>7.298401e-16</td>\n",
              "      <td>6.752385e-07</td>\n",
              "      <td>1.107395e-14</td>\n",
              "      <td>3.476373e-15</td>\n",
              "      <td>3.963793e-14</td>\n",
              "      <td>1.250749e-11</td>\n",
              "      <td>...</td>\n",
              "      <td>1.167805e-08</td>\n",
              "      <td>2.849546e-10</td>\n",
              "      <td>5.789072e-09</td>\n",
              "      <td>8.690200e-09</td>\n",
              "      <td>1.211482e-07</td>\n",
              "      <td>9.909494e-07</td>\n",
              "      <td>4.660747e-09</td>\n",
              "      <td>5.405107e-09</td>\n",
              "      <td>1.178022e-07</td>\n",
              "      <td>2.441182e-11</td>\n",
              "      <td>2.143906e-08</td>\n",
              "      <td>7.882206e-09</td>\n",
              "      <td>1.233910e-08</td>\n",
              "      <td>5.038395e-12</td>\n",
              "      <td>1.316498e-05</td>\n",
              "      <td>3.688942e-10</td>\n",
              "      <td>3.909552e-07</td>\n",
              "      <td>2.142386e-08</td>\n",
              "      <td>1.311971e-12</td>\n",
              "      <td>3.777793e-10</td>\n",
              "      <td>2.157543e-09</td>\n",
              "      <td>7.170096e-07</td>\n",
              "      <td>4.144111e-11</td>\n",
              "      <td>9.154367e-12</td>\n",
              "      <td>1.619446e-08</td>\n",
              "      <td>2.811410e-10</td>\n",
              "      <td>1.403568e-05</td>\n",
              "      <td>5.503726e-08</td>\n",
              "      <td>1.713179e-09</td>\n",
              "      <td>3.571730e-07</td>\n",
              "      <td>5.736953e-11</td>\n",
              "      <td>3.516023e-16</td>\n",
              "      <td>8.837110e-06</td>\n",
              "      <td>3.955898e-14</td>\n",
              "      <td>9.494786e-14</td>\n",
              "      <td>9.038780e-05</td>\n",
              "      <td>6.682173e-10</td>\n",
              "      <td>8.262344e-15</td>\n",
              "      <td>6.801132e-14</td>\n",
              "      <td>3.525626e-01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>217 rows × 157 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          Label_0       Label_1  ...     Label_194     Label_195\n",
              "0    3.884743e-01  5.183269e-01  ...  8.768307e-06  2.665896e-15\n",
              "1    1.524751e-09  5.440841e-10  ...  4.012572e-08  6.894912e-17\n",
              "2    2.166581e-11  9.081477e-08  ...  3.024903e-07  1.490195e-17\n",
              "3    1.967019e-11  2.739590e-08  ...  6.670704e-08  3.200180e-18\n",
              "4    2.199972e-11  2.387229e-05  ...  7.038193e-06  3.410425e-15\n",
              "..            ...           ...  ...           ...           ...\n",
              "212  7.049466e-19  5.143551e-20  ...  3.028352e-11  2.324831e-08\n",
              "213  1.842637e-20  2.551349e-22  ...  3.991240e-09  1.410809e-11\n",
              "214  1.616218e-15  7.455305e-13  ...  9.999984e-01  1.055389e-13\n",
              "215  7.124713e-13  2.565555e-11  ...  9.998959e-01  1.309729e-10\n",
              "216  5.129125e-16  6.455750e-17  ...  6.801132e-14  3.525626e-01\n",
              "\n",
              "[217 rows x 157 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}