{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "million-monaco",
   "metadata": {
    "id": "million-monaco",
    "papermill": {
     "duration": 0.026241,
     "end_time": "2021-05-30T18:39:28.019714",
     "exception": false,
     "start_time": "2021-05-30T18:39:27.993473",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 처음부터 끝까지 곧바로 실행하시면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddaadda",
   "metadata": {
    "id": "million-monaco",
    "papermill": {
     "duration": 0.026241,
     "end_time": "2021-05-30T18:39:28.019714",
     "exception": false,
     "start_time": "2021-05-30T18:39:27.993473",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbbf0c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==0.10.0\r\n",
      "alembic==1.4.1\r\n",
      "argon2-cffi==20.1.0\r\n",
      "asn1crypto==0.24.0\r\n",
      "astunparse==1.6.3\r\n",
      "async-generator==1.10\r\n",
      "attrs==20.3.0\r\n",
      "backcall==0.2.0\r\n",
      "beautifulsoup4==4.6.0\r\n",
      "bleach==3.3.0\r\n",
      "blis==0.7.4\r\n",
      "Boruta==0.3\r\n",
      "cachetools==4.2.0\r\n",
      "catalogue==1.0.0\r\n",
      "catboost==0.24.4\r\n",
      "certifi==2020.12.5\r\n",
      "cffi==1.14.5\r\n",
      "chardet==4.0.0\r\n",
      "click==7.1.2\r\n",
      "cloudpickle==1.6.0\r\n",
      "colorama==0.4.4\r\n",
      "colorlover==0.3.0\r\n",
      "confuse==1.4.0\r\n",
      "cryptography==2.1.4\r\n",
      "cufflinks==0.17.3\r\n",
      "cycler==0.10.0\r\n",
      "cymem==2.0.5\r\n",
      "databricks-cli==0.14.2\r\n",
      "dataclasses==0.8\r\n",
      "decorator==4.4.2\r\n",
      "defusedxml==0.6.0\r\n",
      "dill==0.3.3\r\n",
      "docker==4.4.4\r\n",
      "entrypoints==0.3\r\n",
      "et-xmlfile==1.0.1\r\n",
      "filelock==3.4.1\r\n",
      "Flask==1.1.2\r\n",
      "flatbuffers==1.12\r\n",
      "funcy==1.15\r\n",
      "future==0.18.2\r\n",
      "gast==0.3.3\r\n",
      "gaussian==0.1\r\n",
      "gdown==4.4.0\r\n",
      "gensim==3.8.3\r\n",
      "gitdb==4.0.5\r\n",
      "GitPython==3.1.13\r\n",
      "google-auth==1.24.0\r\n",
      "google-auth-oauthlib==0.4.2\r\n",
      "google-pasta==0.2.0\r\n",
      "googleapis-common-protos==1.53.0\r\n",
      "graphviz==0.16\r\n",
      "grpcio==1.32.0\r\n",
      "gunicorn==20.0.4\r\n",
      "h5py==2.10.0\r\n",
      "htmlmin==0.1.12\r\n",
      "hyperopt==0.2.5\r\n",
      "idna==2.6\r\n",
      "ImageHash==4.2.0\r\n",
      "imbalanced-learn==0.7.0\r\n",
      "importlib-metadata==3.4.0\r\n",
      "importlib-resources==5.1.0\r\n",
      "ipykernel==5.5.0\r\n",
      "ipython==7.16.1\r\n",
      "ipython-genutils==0.2.0\r\n",
      "ipywidgets==7.6.3\r\n",
      "itsdangerous==1.1.0\r\n",
      "jdcal==1.4.1\r\n",
      "jedi==0.18.0\r\n",
      "Jinja2==2.11.3\r\n",
      "joblib==1.0.1\r\n",
      "JPype1==1.2.1\r\n",
      "jsonschema==3.2.0\r\n",
      "jupyter==1.0.0\r\n",
      "jupyter-client==6.1.11\r\n",
      "jupyter-console==6.2.0\r\n",
      "jupyter-core==4.7.1\r\n",
      "jupyterlab-pygments==0.1.2\r\n",
      "jupyterlab-widgets==1.0.0\r\n",
      "kaggle==1.5.10\r\n",
      "Keras==2.4.3\r\n",
      "Keras-Applications==1.0.8\r\n",
      "Keras-Preprocessing==1.1.2\r\n",
      "keyring==10.6.0\r\n",
      "keyrings.alt==3.0\r\n",
      "kiwisolver==1.3.1\r\n",
      "kmodes==0.11.0\r\n",
      "konlpy==0.5.2\r\n",
      "lab==6.3\r\n",
      "lightgbm==3.1.1\r\n",
      "llvmlite==0.35.0\r\n",
      "lxml==4.6.2\r\n",
      "Mako==1.1.4\r\n",
      "Markdown==3.3.3\r\n",
      "MarkupSafe==1.1.1\r\n",
      "matplotlib==3.3.4\r\n",
      "mecab-python===0.996-ko-0.9.2\r\n",
      "missingno==0.4.2\r\n",
      "mistune==0.8.4\r\n",
      "mlflow==1.14.0\r\n",
      "mlxtend==0.18.0\r\n",
      "murmurhash==1.0.5\r\n",
      "nbclient==0.5.3\r\n",
      "nbconvert==6.0.7\r\n",
      "nbformat==5.1.2\r\n",
      "nest-asyncio==1.5.1\r\n",
      "networkx==2.5\r\n",
      "nltk==3.5\r\n",
      "notebook==6.2.0\r\n",
      "numba==0.52.0\r\n",
      "numexpr==2.7.2\r\n",
      "numpy==1.19.5\r\n",
      "oauthlib==3.1.0\r\n",
      "opencv-python==4.5.5.64\r\n",
      "openpyxl==3.0.6\r\n",
      "opt-einsum==3.3.0\r\n",
      "packaging==20.9\r\n",
      "pandas==1.1.5\r\n",
      "pandas-profiling==2.11.0\r\n",
      "pandocfilters==1.4.3\r\n",
      "parso==0.8.1\r\n",
      "patsy==0.5.1\r\n",
      "pexpect==4.8.0\r\n",
      "phik==0.11.0\r\n",
      "pickle5==0.0.11\r\n",
      "pickleshare==0.7.5\r\n",
      "Pillow==8.1.0\r\n",
      "plac==1.1.3\r\n",
      "plotly==4.14.3\r\n",
      "preshed==3.0.5\r\n",
      "prometheus-client==0.9.0\r\n",
      "prometheus-flask-exporter==0.18.1\r\n",
      "promise==2.3\r\n",
      "prompt-toolkit==3.0.16\r\n",
      "protobuf==3.14.0\r\n",
      "ptyprocess==0.7.0\r\n",
      "py4j==0.10.9\r\n",
      "pyasn1==0.4.8\r\n",
      "pyasn1-modules==0.2.8\r\n",
      "pycaret==2.3.0\r\n",
      "pycparser==2.20\r\n",
      "pycrypto==2.6.1\r\n",
      "Pygments==2.8.0\r\n",
      "PyGObject==3.26.1\r\n",
      "pykospacing @ git+https://github.com/haven-jeon/PyKoSpacing.git@1a36be492cc396559e7dce7825843af020ea231f\r\n",
      "pyLDAvis==3.2.2\r\n",
      "pynndescent==0.5.2\r\n",
      "pyod==0.8.7\r\n",
      "pyparsing==2.4.7\r\n",
      "pyrsistent==0.17.3\r\n",
      "PySocks==1.7.1\r\n",
      "pyspark==3.1.1\r\n",
      "python-apt==1.6.5+ubuntu0.5\r\n",
      "python-dateutil==2.8.1\r\n",
      "python-editor==1.0.4\r\n",
      "python-slugify==4.0.1\r\n",
      "pytz==2021.1\r\n",
      "PyWavelets==1.1.1\r\n",
      "pyxdg==0.25\r\n",
      "PyYAML==5.4.1\r\n",
      "pyzmq==22.0.3\r\n",
      "qtconsole==5.0.2\r\n",
      "QtPy==1.9.0\r\n",
      "querystring-parser==1.2.4\r\n",
      "regex==2020.11.13\r\n",
      "requests==2.25.1\r\n",
      "requests-oauthlib==1.3.0\r\n",
      "retrying==1.3.3\r\n",
      "rsa==4.7\r\n",
      "scikit-learn==0.23.2\r\n",
      "scikit-plot==0.3.7\r\n",
      "scikit-surprise==1.1.1\r\n",
      "scipy==1.5.4\r\n",
      "seaborn==0.11.1\r\n",
      "SecretStorage==2.3.1\r\n",
      "Send2Trash==1.5.0\r\n",
      "shap==0.38.1\r\n",
      "simplejson==3.17.2\r\n",
      "six==1.15.0\r\n",
      "slicer==0.0.7\r\n",
      "smart-open==4.2.0\r\n",
      "smmap==3.0.5\r\n",
      "spacy==2.3.5\r\n",
      "SQLAlchemy==1.3.23\r\n",
      "sqlparse==0.4.1\r\n",
      "srsly==1.0.5\r\n",
      "ssh-import-id==5.7\r\n",
      "statsmodels==0.12.2\r\n",
      "surprise==0.1\r\n",
      "tabulate==0.8.9\r\n",
      "tangled-up-in-unicode==0.0.6\r\n",
      "tensorboard==2.4.1\r\n",
      "tensorboard-plugin-wit==1.8.0\r\n",
      "tensorflow==2.4.0\r\n",
      "tensorflow-datasets==4.2.0\r\n",
      "tensorflow-estimator==2.4.0\r\n",
      "tensorflow-examples===c3bf7340c5002d62f4d51da21d7b01a884082472-\r\n",
      "tensorflow-gpu==2.4.1\r\n",
      "tensorflow-metadata==0.28.0\r\n",
      "termcolor==1.1.0\r\n",
      "terminado==0.9.2\r\n",
      "testpath==0.4.4\r\n",
      "text-unidecode==1.3\r\n",
      "textblob==0.15.3\r\n",
      "thinc==7.4.5\r\n",
      "threadpoolctl==2.1.0\r\n",
      "timm==0.5.4\r\n",
      "torch==1.7.1+cu110\r\n",
      "torchaudio==0.7.2\r\n",
      "torchvision==0.8.2+cu110\r\n",
      "tornado==6.1\r\n",
      "tqdm==4.58.0\r\n",
      "traitlets==4.3.3\r\n",
      "tweepy==3.10.0\r\n",
      "txt2tags==3.7\r\n",
      "typing-extensions==3.7.4.3\r\n",
      "ufw==0.36\r\n",
      "umap-learn==0.5.1\r\n",
      "urllib3==1.26.2\r\n",
      "visions==0.6.0\r\n",
      "wasabi==0.8.2\r\n",
      "wcwidth==0.2.5\r\n",
      "webencodings==0.5.1\r\n",
      "websocket-client==0.57.0\r\n",
      "Werkzeug==1.0.1\r\n",
      "widgetsnbextension==3.5.1\r\n",
      "wordcloud==1.8.1\r\n",
      "wrapt==1.12.1\r\n",
      "xgboost==1.3.3\r\n",
      "xlrd==2.0.1\r\n",
      "yellowbrick==1.3.post1\r\n",
      "zipp==3.4.0\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcba341f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae6cd0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pykospacing@ git+https://github.com/haven-jeon/PyKoSpacing.git@1a36be492cc396559e7dce7825843af020ea231f\r\n",
      "  Cloning https://github.com/haven-jeon/PyKoSpacing.git (to revision 1a36be492cc396559e7dce7825843af020ea231f) to /tmp/pip-install-3d0ukjjt/pykospacing_b35d7768773246b9b4c88fa39830045a\r\n",
      "  Running command git clone --filter=blob:none -q https://github.com/haven-jeon/PyKoSpacing.git /tmp/pip-install-3d0ukjjt/pykospacing_b35d7768773246b9b4c88fa39830045a\r\n",
      "  Running command git rev-parse -q --verify 'sha^1a36be492cc396559e7dce7825843af020ea231f'\r\n",
      "  Running command git fetch -q https://github.com/haven-jeon/PyKoSpacing.git 1a36be492cc396559e7dce7825843af020ea231f\r\n",
      "  Running command git checkout -q 1a36be492cc396559e7dce7825843af020ea231f\r\n",
      "  Resolved https://github.com/haven-jeon/PyKoSpacing.git to commit 1a36be492cc396559e7dce7825843af020ea231f\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: absl-py==0.10.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (0.10.0)\r\n",
      "Requirement already satisfied: alembic==1.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (1.4.1)\r\n",
      "Requirement already satisfied: argon2-cffi==20.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (20.1.0)\r\n",
      "Requirement already satisfied: asn1crypto==0.24.0 in /usr/lib/python3/dist-packages (from -r requirements.txt (line 4)) (0.24.0)\r\n",
      "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (1.6.3)\r\n",
      "Requirement already satisfied: async-generator==1.10 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (1.10)\r\n",
      "Requirement already satisfied: attrs==20.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (20.3.0)\r\n",
      "Requirement already satisfied: backcall==0.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (0.2.0)\r\n",
      "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (4.6.0)\r\n",
      "Requirement already satisfied: bleach==3.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 10)) (3.3.0)\r\n",
      "Requirement already satisfied: blis==0.7.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 11)) (0.7.4)\r\n",
      "Requirement already satisfied: Boruta==0.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 12)) (0.3)\r\n",
      "Requirement already satisfied: cachetools==4.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 13)) (4.2.0)\r\n",
      "Requirement already satisfied: catalogue==1.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 14)) (1.0.0)\r\n",
      "Requirement already satisfied: catboost==0.24.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 15)) (0.24.4)\r\n",
      "Requirement already satisfied: certifi==2020.12.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 16)) (2020.12.5)\r\n",
      "Requirement already satisfied: cffi==1.14.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 17)) (1.14.5)\r\n",
      "Requirement already satisfied: chardet==4.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 18)) (4.0.0)\r\n",
      "Requirement already satisfied: click==7.1.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 19)) (7.1.2)\r\n",
      "Requirement already satisfied: cloudpickle==1.6.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 20)) (1.6.0)\r\n",
      "Requirement already satisfied: colorama==0.4.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 21)) (0.4.4)\r\n",
      "Requirement already satisfied: colorlover==0.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 22)) (0.3.0)\r\n",
      "Requirement already satisfied: confuse==1.4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 23)) (1.4.0)\r\n",
      "Requirement already satisfied: cryptography==2.1.4 in /usr/lib/python3/dist-packages (from -r requirements.txt (line 24)) (2.1.4)\r\n",
      "Requirement already satisfied: cufflinks==0.17.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 25)) (0.17.3)\r\n",
      "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 26)) (0.10.0)\r\n",
      "Requirement already satisfied: cymem==2.0.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 27)) (2.0.5)\r\n",
      "Requirement already satisfied: databricks-cli==0.14.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 28)) (0.14.2)\r\n",
      "Requirement already satisfied: dataclasses==0.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 29)) (0.8)\r\n",
      "Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 30)) (4.4.2)\r\n",
      "Requirement already satisfied: defusedxml==0.6.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 31)) (0.6.0)\r\n",
      "Requirement already satisfied: dill==0.3.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 32)) (0.3.3)\r\n",
      "Requirement already satisfied: docker==4.4.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 33)) (4.4.4)\r\n",
      "Requirement already satisfied: entrypoints==0.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 34)) (0.3)\r\n",
      "Requirement already satisfied: et-xmlfile==1.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 35)) (1.0.1)\r\n",
      "Requirement already satisfied: filelock==3.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 36)) (3.4.1)\r\n",
      "Requirement already satisfied: Flask==1.1.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 37)) (1.1.2)\r\n",
      "Requirement already satisfied: flatbuffers==1.12 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 38)) (1.12)\r\n",
      "Requirement already satisfied: funcy==1.15 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 39)) (1.15)\r\n",
      "Requirement already satisfied: future==0.18.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 40)) (0.18.2)\r\n",
      "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 41)) (0.3.3)\r\n",
      "Requirement already satisfied: gaussian==0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 42)) (0.1)\r\n",
      "Requirement already satisfied: gdown==4.4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 43)) (4.4.0)\r\n",
      "Requirement already satisfied: gensim==3.8.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 44)) (3.8.3)\r\n",
      "Requirement already satisfied: gitdb==4.0.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 45)) (4.0.5)\r\n",
      "Requirement already satisfied: GitPython==3.1.13 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 46)) (3.1.13)\r\n",
      "Requirement already satisfied: google-auth==1.24.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 47)) (1.24.0)\r\n",
      "Requirement already satisfied: google-auth-oauthlib==0.4.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 48)) (0.4.2)\r\n",
      "Requirement already satisfied: google-pasta==0.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 49)) (0.2.0)\r\n",
      "Requirement already satisfied: googleapis-common-protos==1.53.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 50)) (1.53.0)\r\n",
      "Requirement already satisfied: graphviz==0.16 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 51)) (0.16)\r\n",
      "Requirement already satisfied: grpcio==1.32.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 52)) (1.32.0)\r\n",
      "Requirement already satisfied: gunicorn==20.0.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 53)) (20.0.4)\r\n",
      "Requirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 54)) (2.10.0)\r\n",
      "Requirement already satisfied: htmlmin==0.1.12 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 55)) (0.1.12)\r\n",
      "Requirement already satisfied: hyperopt==0.2.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 56)) (0.2.5)\r\n",
      "Requirement already satisfied: idna==2.6 in /usr/lib/python3/dist-packages (from -r requirements.txt (line 57)) (2.6)\r\n",
      "Requirement already satisfied: ImageHash==4.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 58)) (4.2.0)\r\n",
      "Requirement already satisfied: imbalanced-learn==0.7.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 59)) (0.7.0)\r\n",
      "Requirement already satisfied: importlib-metadata==3.4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 60)) (3.4.0)\r\n",
      "Requirement already satisfied: importlib-resources==5.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 61)) (5.1.0)\r\n",
      "Requirement already satisfied: ipykernel==5.5.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 62)) (5.5.0)\r\n",
      "Requirement already satisfied: ipython==7.16.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 63)) (7.16.1)\r\n",
      "Requirement already satisfied: ipython-genutils==0.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 64)) (0.2.0)\r\n",
      "Requirement already satisfied: ipywidgets==7.6.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 65)) (7.6.3)\r\n",
      "Requirement already satisfied: itsdangerous==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 66)) (1.1.0)\r\n",
      "Requirement already satisfied: jdcal==1.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 67)) (1.4.1)\r\n",
      "Requirement already satisfied: jedi==0.18.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 68)) (0.18.0)\r\n",
      "Requirement already satisfied: Jinja2==2.11.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 69)) (2.11.3)\r\n",
      "Requirement already satisfied: joblib==1.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 70)) (1.0.1)\r\n",
      "Requirement already satisfied: JPype1==1.2.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 71)) (1.2.1)\r\n",
      "Requirement already satisfied: jsonschema==3.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 72)) (3.2.0)\r\n",
      "Requirement already satisfied: jupyter==1.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 73)) (1.0.0)\r\n",
      "Requirement already satisfied: jupyter-client==6.1.11 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 74)) (6.1.11)\r\n",
      "Requirement already satisfied: jupyter-console==6.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 75)) (6.2.0)\r\n",
      "Requirement already satisfied: jupyter-core==4.7.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 76)) (4.7.1)\r\n",
      "Requirement already satisfied: jupyterlab-pygments==0.1.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 77)) (0.1.2)\r\n",
      "Requirement already satisfied: jupyterlab-widgets==1.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 78)) (1.0.0)\r\n",
      "Requirement already satisfied: kaggle==1.5.10 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 79)) (1.5.10)\r\n",
      "Requirement already satisfied: Keras==2.4.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 80)) (2.4.3)\r\n",
      "Requirement already satisfied: Keras-Applications==1.0.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 81)) (1.0.8)\r\n",
      "Requirement already satisfied: Keras-Preprocessing==1.1.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 82)) (1.1.2)\r\n",
      "Requirement already satisfied: keyring==10.6.0 in /usr/lib/python3/dist-packages (from -r requirements.txt (line 83)) (10.6.0)\r\n",
      "Requirement already satisfied: keyrings.alt==3.0 in /usr/lib/python3/dist-packages (from -r requirements.txt (line 84)) (3.0)\r\n",
      "Requirement already satisfied: kiwisolver==1.3.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 85)) (1.3.1)\r\n",
      "Requirement already satisfied: kmodes==0.11.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 86)) (0.11.0)\r\n",
      "Requirement already satisfied: konlpy==0.5.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 87)) (0.5.2)\r\n",
      "Requirement already satisfied: lab==6.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 88)) (6.3)\r\n",
      "Requirement already satisfied: lightgbm==3.1.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 89)) (3.1.1)\r\n",
      "Requirement already satisfied: llvmlite==0.35.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 90)) (0.35.0)\r\n",
      "Requirement already satisfied: lxml==4.6.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 91)) (4.6.2)\r\n",
      "Requirement already satisfied: Mako==1.1.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 92)) (1.1.4)\r\n",
      "Requirement already satisfied: Markdown==3.3.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 93)) (3.3.3)\r\n",
      "Requirement already satisfied: MarkupSafe==1.1.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 94)) (1.1.1)\r\n",
      "Requirement already satisfied: matplotlib==3.3.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 95)) (3.3.4)\r\n",
      "Requirement already satisfied: mecab-python===0.996-ko-0.9.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 96)) (0.996-ko-0.9.2)\r\n",
      "Requirement already satisfied: missingno==0.4.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 97)) (0.4.2)\r\n",
      "Requirement already satisfied: mistune==0.8.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 98)) (0.8.4)\r\n",
      "Requirement already satisfied: mlflow==1.14.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 99)) (1.14.0)\r\n",
      "Requirement already satisfied: mlxtend==0.18.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 100)) (0.18.0)\r\n",
      "Requirement already satisfied: murmurhash==1.0.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 101)) (1.0.5)\r\n",
      "Requirement already satisfied: nbclient==0.5.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 102)) (0.5.3)\r\n",
      "Requirement already satisfied: nbconvert==6.0.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 103)) (6.0.7)\r\n",
      "Requirement already satisfied: nbformat==5.1.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 104)) (5.1.2)\r\n",
      "Requirement already satisfied: nest-asyncio==1.5.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 105)) (1.5.1)\r\n",
      "Requirement already satisfied: networkx==2.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 106)) (2.5)\r\n",
      "Requirement already satisfied: nltk==3.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 107)) (3.5)\r\n",
      "Requirement already satisfied: notebook==6.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 108)) (6.2.0)\r\n",
      "Requirement already satisfied: numba==0.52.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 109)) (0.52.0)\r\n",
      "Requirement already satisfied: numexpr==2.7.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 110)) (2.7.2)\r\n",
      "Requirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 111)) (1.19.5)\r\n",
      "Requirement already satisfied: oauthlib==3.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 112)) (3.1.0)\r\n",
      "Requirement already satisfied: opencv-python==4.5.5.64 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 113)) (4.5.5.64)\r\n",
      "Requirement already satisfied: openpyxl==3.0.6 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 114)) (3.0.6)\r\n",
      "Requirement already satisfied: opt-einsum==3.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 115)) (3.3.0)\r\n",
      "Requirement already satisfied: packaging==20.9 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 116)) (20.9)\r\n",
      "Requirement already satisfied: pandas==1.1.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 117)) (1.1.5)\r\n",
      "Requirement already satisfied: pandas-profiling==2.11.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 118)) (2.11.0)\r\n",
      "Requirement already satisfied: pandocfilters==1.4.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 119)) (1.4.3)\r\n",
      "Requirement already satisfied: parso==0.8.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 120)) (0.8.1)\r\n",
      "Requirement already satisfied: patsy==0.5.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 121)) (0.5.1)\r\n",
      "Requirement already satisfied: pexpect==4.8.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 122)) (4.8.0)\r\n",
      "Requirement already satisfied: phik==0.11.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 123)) (0.11.0)\r\n",
      "Requirement already satisfied: pickle5==0.0.11 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 124)) (0.0.11)\r\n",
      "Requirement already satisfied: pickleshare==0.7.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 125)) (0.7.5)\r\n",
      "Requirement already satisfied: Pillow==8.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 126)) (8.1.0)\r\n",
      "Requirement already satisfied: plac==1.1.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 127)) (1.1.3)\r\n",
      "Requirement already satisfied: plotly==4.14.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 128)) (4.14.3)\r\n",
      "Requirement already satisfied: preshed==3.0.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 129)) (3.0.5)\r\n",
      "Requirement already satisfied: prometheus-client==0.9.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 130)) (0.9.0)\r\n",
      "Requirement already satisfied: prometheus-flask-exporter==0.18.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 131)) (0.18.1)\r\n",
      "Requirement already satisfied: promise==2.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 132)) (2.3)\r\n",
      "Requirement already satisfied: prompt-toolkit==3.0.16 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 133)) (3.0.16)\r\n",
      "Requirement already satisfied: protobuf==3.14.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 134)) (3.14.0)\r\n",
      "Requirement already satisfied: ptyprocess==0.7.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 135)) (0.7.0)\r\n",
      "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 136)) (0.10.9)\r\n",
      "Requirement already satisfied: pyasn1==0.4.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 137)) (0.4.8)\r\n",
      "Requirement already satisfied: pyasn1-modules==0.2.8 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 138)) (0.2.8)\r\n",
      "Requirement already satisfied: pycaret==2.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 139)) (2.3.0)\r\n",
      "Requirement already satisfied: pycparser==2.20 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 140)) (2.20)\r\n",
      "Requirement already satisfied: pycrypto==2.6.1 in /usr/lib/python3/dist-packages (from -r requirements.txt (line 141)) (2.6.1)\r\n",
      "Requirement already satisfied: Pygments==2.8.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 142)) (2.8.0)\r\n",
      "Requirement already satisfied: PyGObject==3.26.1 in /usr/lib/python3/dist-packages (from -r requirements.txt (line 143)) (3.26.1)\r\n",
      "Requirement already satisfied: pyLDAvis==3.2.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 145)) (3.2.2)\r\n",
      "Requirement already satisfied: pynndescent==0.5.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 146)) (0.5.2)\r\n",
      "Requirement already satisfied: pyod==0.8.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 147)) (0.8.7)\r\n",
      "Requirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 148)) (2.4.7)\r\n",
      "Requirement already satisfied: pyrsistent==0.17.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 149)) (0.17.3)\r\n",
      "Requirement already satisfied: PySocks==1.7.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 150)) (1.7.1)\r\n",
      "Requirement already satisfied: pyspark==3.1.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 151)) (3.1.1)\r\n",
      "Requirement already satisfied: python-apt==1.6.5+ubuntu0.5 in /usr/lib/python3/dist-packages (from -r requirements.txt (line 152)) (1.6.5+ubuntu0.5)\r\n",
      "Requirement already satisfied: python-dateutil==2.8.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 153)) (2.8.1)\r\n",
      "Requirement already satisfied: python-editor==1.0.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 154)) (1.0.4)\r\n",
      "Requirement already satisfied: python-slugify==4.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 155)) (4.0.1)\r\n",
      "Requirement already satisfied: pytz==2021.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 156)) (2021.1)\r\n",
      "Requirement already satisfied: PyWavelets==1.1.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 157)) (1.1.1)\r\n",
      "Requirement already satisfied: pyxdg==0.25 in /usr/lib/python3/dist-packages (from -r requirements.txt (line 158)) (0.25)\r\n",
      "Requirement already satisfied: PyYAML==5.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 159)) (5.4.1)\r\n",
      "Requirement already satisfied: pyzmq==22.0.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 160)) (22.0.3)\r\n",
      "Requirement already satisfied: qtconsole==5.0.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 161)) (5.0.2)\r\n",
      "Requirement already satisfied: QtPy==1.9.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 162)) (1.9.0)\r\n",
      "Requirement already satisfied: querystring-parser==1.2.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 163)) (1.2.4)\r\n",
      "Requirement already satisfied: regex==2020.11.13 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 164)) (2020.11.13)\r\n",
      "Requirement already satisfied: requests==2.25.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 165)) (2.25.1)\r\n",
      "Requirement already satisfied: requests-oauthlib==1.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 166)) (1.3.0)\r\n",
      "Requirement already satisfied: retrying==1.3.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 167)) (1.3.3)\r\n",
      "Requirement already satisfied: rsa==4.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 168)) (4.7)\r\n",
      "Requirement already satisfied: scikit-learn==0.23.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 169)) (0.23.2)\r\n",
      "Requirement already satisfied: scikit-plot==0.3.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 170)) (0.3.7)\r\n",
      "Requirement already satisfied: scikit-surprise==1.1.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 171)) (1.1.1)\r\n",
      "Requirement already satisfied: scipy==1.5.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 172)) (1.5.4)\r\n",
      "Requirement already satisfied: seaborn==0.11.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 173)) (0.11.1)\r\n",
      "Requirement already satisfied: SecretStorage==2.3.1 in /usr/lib/python3/dist-packages (from -r requirements.txt (line 174)) (2.3.1)\r\n",
      "Requirement already satisfied: Send2Trash==1.5.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 175)) (1.5.0)\r\n",
      "Requirement already satisfied: shap==0.38.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 176)) (0.38.1)\r\n",
      "Requirement already satisfied: simplejson==3.17.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 177)) (3.17.2)\r\n",
      "Requirement already satisfied: six==1.15.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 178)) (1.15.0)\r\n",
      "Requirement already satisfied: slicer==0.0.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 179)) (0.0.7)\r\n",
      "Requirement already satisfied: smart-open==4.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 180)) (4.2.0)\r\n",
      "Requirement already satisfied: smmap==3.0.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 181)) (3.0.5)\r\n",
      "Requirement already satisfied: spacy==2.3.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 182)) (2.3.5)\r\n",
      "Requirement already satisfied: SQLAlchemy==1.3.23 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 183)) (1.3.23)\r\n",
      "Requirement already satisfied: sqlparse==0.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 184)) (0.4.1)\r\n",
      "Requirement already satisfied: srsly==1.0.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 185)) (1.0.5)\r\n",
      "Requirement already satisfied: ssh-import-id==5.7 in /usr/lib/python3/dist-packages (from -r requirements.txt (line 186)) (5.7)\r\n",
      "Requirement already satisfied: statsmodels==0.12.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 187)) (0.12.2)\r\n",
      "Requirement already satisfied: surprise==0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 188)) (0.1)\r\n",
      "Requirement already satisfied: tabulate==0.8.9 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 189)) (0.8.9)\r\n",
      "Requirement already satisfied: tangled-up-in-unicode==0.0.6 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 190)) (0.0.6)\r\n",
      "Requirement already satisfied: tensorboard==2.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 191)) (2.4.1)\r\n",
      "Requirement already satisfied: tensorboard-plugin-wit==1.8.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 192)) (1.8.0)\r\n",
      "Requirement already satisfied: tensorflow==2.4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 193)) (2.4.0)\r\n",
      "Requirement already satisfied: tensorflow-datasets==4.2.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 194)) (4.2.0)\r\n",
      "Requirement already satisfied: tensorflow-estimator==2.4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 195)) (2.4.0)\r\n",
      "Requirement already satisfied: tensorflow-examples===c3bf7340c5002d62f4d51da21d7b01a884082472- in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 196)) (c3bf7340c5002d62f4d51da21d7b01a884082472-)\r\n",
      "Requirement already satisfied: tensorflow-gpu==2.4.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 197)) (2.4.1)\r\n",
      "Requirement already satisfied: tensorflow-metadata==0.28.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 198)) (0.28.0)\r\n",
      "Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 199)) (1.1.0)\r\n",
      "Requirement already satisfied: terminado==0.9.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 200)) (0.9.2)\r\n",
      "Requirement already satisfied: testpath==0.4.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 201)) (0.4.4)\r\n",
      "Requirement already satisfied: text-unidecode==1.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 202)) (1.3)\r\n",
      "Requirement already satisfied: textblob==0.15.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 203)) (0.15.3)\r\n",
      "Requirement already satisfied: thinc==7.4.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 204)) (7.4.5)\r\n",
      "Requirement already satisfied: threadpoolctl==2.1.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 205)) (2.1.0)\r\n",
      "Requirement already satisfied: timm==0.5.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 206)) (0.5.4)\r\n",
      "Requirement already satisfied: torch==1.7.1+cu110 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 207)) (1.7.1+cu110)\r\n",
      "Requirement already satisfied: torchaudio==0.7.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 208)) (0.7.2)\r\n",
      "Requirement already satisfied: torchvision==0.8.2+cu110 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 209)) (0.8.2+cu110)\r\n",
      "Requirement already satisfied: tornado==6.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 210)) (6.1)\r\n",
      "Requirement already satisfied: tqdm==4.58.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 211)) (4.58.0)\r\n",
      "Requirement already satisfied: traitlets==4.3.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 212)) (4.3.3)\r\n",
      "Requirement already satisfied: tweepy==3.10.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 213)) (3.10.0)\r\n",
      "Requirement already satisfied: txt2tags==3.7 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 214)) (3.7)\r\n",
      "Requirement already satisfied: typing-extensions==3.7.4.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 215)) (3.7.4.3)\r\n",
      "Requirement already satisfied: ufw==0.36 in /usr/lib/python3/dist-packages (from -r requirements.txt (line 216)) (0.36)\r\n",
      "Requirement already satisfied: umap-learn==0.5.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 217)) (0.5.1)\r\n",
      "Requirement already satisfied: urllib3==1.26.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 218)) (1.26.2)\r\n",
      "Requirement already satisfied: visions==0.6.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 219)) (0.6.0)\r\n",
      "Requirement already satisfied: wasabi==0.8.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 220)) (0.8.2)\r\n",
      "Requirement already satisfied: wcwidth==0.2.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 221)) (0.2.5)\r\n",
      "Requirement already satisfied: webencodings==0.5.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 222)) (0.5.1)\r\n",
      "Requirement already satisfied: websocket-client==0.57.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 223)) (0.57.0)\r\n",
      "Requirement already satisfied: Werkzeug==1.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 224)) (1.0.1)\r\n",
      "Requirement already satisfied: widgetsnbextension==3.5.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 225)) (3.5.1)\r\n",
      "Requirement already satisfied: wordcloud==1.8.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 226)) (1.8.1)\r\n",
      "Requirement already satisfied: wrapt==1.12.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 227)) (1.12.1)\r\n",
      "Requirement already satisfied: xgboost==1.3.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 228)) (1.3.3)\r\n",
      "Requirement already satisfied: xlrd==2.0.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 229)) (2.0.1)\r\n",
      "Requirement already satisfied: yellowbrick==1.3.post1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 230)) (1.3.post1)\r\n",
      "Requirement already satisfied: zipp==3.4.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 231)) (3.4.0)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from astunparse==1.6.3->-r requirements.txt (line 5)) (0.36.2)\r\n",
      "Requirement already satisfied: setuptools>=34.4.1 in /usr/local/lib/python3.6/dist-packages (from cufflinks==0.17.3->-r requirements.txt (line 25)) (51.3.3)\r\n",
      "Collecting argparse>=1.4.0\r\n",
      "  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\r\n",
      "Installing collected packages: argparse\r\n",
      "Successfully installed argparse-1.4.0\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ts26D3gLnyLd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ts26D3gLnyLd",
    "outputId": "cb1a0caa-c636-4e9b-ed3d-5ca4a9a576c2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Apr  9 20:33:18 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:00:05.0 Off |                  Off |\r\n",
      "| N/A   31C    P0    39W / 300W |      0MiB / 32510MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22446e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15276642937648897868\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 31592913408\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 6753555642149719865\n",
      "physical_device_desc: \"device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:00:05.0, compute capability: 7.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "floating-cincinnati",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-30T18:39:28.086573Z",
     "iopub.status.busy": "2021-05-30T18:39:28.085923Z",
     "iopub.status.idle": "2021-05-30T18:39:32.847126Z",
     "shell.execute_reply": "2021-05-30T18:39:32.846068Z",
     "shell.execute_reply.started": "2021-05-30T09:28:36.967323Z"
    },
    "id": "floating-cincinnati",
    "papermill": {
     "duration": 4.80129,
     "end_time": "2021-05-30T18:39:32.847355",
     "exception": false,
     "start_time": "2021-05-30T18:39:28.046065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aff51ab",
   "metadata": {
    "id": "million-monaco",
    "papermill": {
     "duration": 0.026241,
     "end_time": "2021-05-30T18:39:28.019714",
     "exception": false,
     "start_time": "2021-05-30T18:39:27.993473",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 데이터 세트 다운로드, 압축 해제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf77f93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (4.4.0)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown) (4.58.0)\r\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown) (1.15.0)\r\n",
      "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.6/dist-packages (from gdown) (2.25.1)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from gdown) (4.6.0)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from gdown) (3.4.1)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown) (4.0.0)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests[socks]->gdown) (2.6)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown) (1.26.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown) (2020.12.5)\r\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.6/dist-packages (from requests[socks]->gdown) (1.7.1)\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\r\n",
      "/usr/local/lib/python3.6/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\r\n",
      "  category=FutureWarning,\r\n",
      "Downloading...\r\n",
      "From: https://drive.google.com/uc?id=1TOHms3kqGonSayOTse0MXiOc4zGSdtsI\r\n",
      "To: /home/ClsKLData.zip\r\n",
      "100%|████████████████████████████████████████| 591M/591M [00:05<00:00, 98.8MB/s]\r\n",
      "/usr/local/lib/python3.6/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\r\n",
      "  category=FutureWarning,\r\n",
      "Downloading...\r\n",
      "From: https://drive.google.com/uc?id=1lR_B5LSrtUg273ADJQswDHxpjzDfFxIR\r\n",
      "To: /home/KneeXray.zip\r\n",
      "100%|████████████████████████████████████████| 120M/120M [00:01<00:00, 69.3MB/s]\r\n",
      "/usr/local/lib/python3.6/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\r\n",
      "  category=FutureWarning,\r\n",
      "Downloading...\r\n",
      "From: https://drive.google.com/uc?id=1pI08Cs48tp9NXhuRMt7tssITx4-_PqH1\r\n",
      "To: /home/Lee_KneeData.zip\r\n",
      "100%|██████████████████████████████████████| 3.68M/3.68M [00:00<00:00, 13.0MB/s]\r\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown\n",
    "\n",
    "# train 데이터 세트_ClsKLData\n",
    "!gdown --id \"1TOHms3kqGonSayOTse0MXiOc4zGSdtsI\"\n",
    "\n",
    "# validation & test 데이터 세트_KneeXray\n",
    "!gdown --id \"1lR_B5LSrtUg273ADJQswDHxpjzDfFxIR\"\n",
    "\n",
    "# 추가 test 데이터 세트_Lee_kneeData\n",
    "!gdown --id \"1pI08Cs48tp9NXhuRMt7tssITx4-_PqH1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "763af922",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "         \n",
    "fantasy_zip = zipfile.ZipFile('./ClsKLData.zip')\n",
    "fantasy_zip.extractall('./ClsKLData')\n",
    "fantasy_zip.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d94a16bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "fantasy_zip = zipfile.ZipFile('./KneeXray.zip')\n",
    "fantasy_zip.extractall('./KneeXray') \n",
    "fantasy_zip.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8202f9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "fantasy_zip = zipfile.ZipFile('./Lee_KneeData.zip')\n",
    "fantasy_zip.extractall('./Lee_KneeData')\n",
    "fantasy_zip.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "encouraging-novel",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-30T18:39:36.090553Z",
     "iopub.status.busy": "2021-05-30T18:39:36.089729Z",
     "iopub.status.idle": "2021-05-30T18:39:36.092340Z",
     "shell.execute_reply": "2021-05-30T18:39:36.091828Z",
     "shell.execute_reply.started": "2021-05-30T09:28:44.189248Z"
    },
    "id": "encouraging-novel",
    "papermill": {
     "duration": 0.035021,
     "end_time": "2021-05-30T18:39:36.092447",
     "exception": false,
     "start_time": "2021-05-30T18:39:36.057426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_aug = ImageDataGenerator(rescale=1./255)\n",
    "valid_aug = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f1793e",
   "metadata": {
    "id": "million-monaco",
    "papermill": {
     "duration": 0.026241,
     "end_time": "2021-05-30T18:39:28.019714",
     "exception": false,
     "start_time": "2021-05-30T18:39:27.993473",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 학습 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "emotional-valuable",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-30T18:39:33.012157Z",
     "iopub.status.busy": "2021-05-30T18:39:33.011596Z",
     "iopub.status.idle": "2021-05-30T18:39:35.556063Z",
     "shell.execute_reply": "2021-05-30T18:39:35.555555Z",
     "shell.execute_reply.started": "2021-05-30T09:28:38.947982Z"
    },
    "id": "emotional-valuable",
    "papermill": {
     "duration": 2.57776,
     "end_time": "2021-05-30T18:39:35.556225",
     "exception": false,
     "start_time": "2021-05-30T18:39:32.978465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_class = 5\n",
    "\n",
    "root_path = \"./ClsKLData//kneeKL224//\"\n",
    "\n",
    "folder_list = os.listdir(root_path)\n",
    "image_path_list = []\n",
    "label_list = []\n",
    "\n",
    "for folder in folder_list:\n",
    "    for label in range(n_class):\n",
    "        image_list = os.listdir(f\"{root_path}{folder}/{label}\")\n",
    "        image_path_list += [ f\"{root_path}{folder}/{label}/\"+ path for path in image_list]\n",
    "        label_list += [label] * len(image_list)\n",
    "\n",
    "df_train_kaggle = pd.DataFrame({\"filepath\" : image_path_list, \"label\": label_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "signal-responsibility",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-30T18:39:35.618734Z",
     "iopub.status.busy": "2021-05-30T18:39:35.617931Z",
     "iopub.status.idle": "2021-05-30T18:39:35.621117Z",
     "shell.execute_reply": "2021-05-30T18:39:35.621576Z",
     "shell.execute_reply.started": "2021-05-30T09:28:40.244589Z"
    },
    "id": "signal-responsibility",
    "outputId": "dfd8ae1d-71d1-424c-e0af-c2e831cd3514",
    "papermill": {
     "duration": 0.03629,
     "end_time": "2021-05-30T18:39:35.621716",
     "exception": false,
     "start_time": "2021-05-30T18:39:35.585426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(9786, 2)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_kaggle.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-greene",
   "metadata": {
    "id": "hairy-greene",
    "papermill": {
     "duration": 0.030406,
     "end_time": "2021-05-30T18:39:35.680660",
     "exception": false,
     "start_time": "2021-05-30T18:39:35.650254",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 학습 데이터 분포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "therapeutic-spending",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "execution": {
     "iopub.execute_input": "2021-05-30T18:39:35.750000Z",
     "iopub.status.busy": "2021-05-30T18:39:35.749277Z",
     "iopub.status.idle": "2021-05-30T18:39:35.918362Z",
     "shell.execute_reply": "2021-05-30T18:39:35.918766Z",
     "shell.execute_reply.started": "2021-05-30T09:28:41.597346Z"
    },
    "id": "therapeutic-spending",
    "outputId": "9f2e2be5-47ca-48db-9250-e83add5b2f95",
    "papermill": {
     "duration": 0.208649,
     "end_time": "2021-05-30T18:39:35.918905",
     "exception": false,
     "start_time": "2021-05-30T18:39:35.710256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Text(0, 0.5, 'count')"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAECCAYAAAALqiumAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAARl0lEQVR4nO3df6zdd13H8eeLrnTKD2m3S8jUUpVtfzBA8EZkxkFxI7ISy68AMhCM22XMDN2Q0EV+2jEmCUIiMuyiMLZK1BA6sIDbwiw6yKRjCGEglDgQFXLpZY5uWNfy9o/7LT29u5/2nrbfc852n4/kZOf7/n7OOe/zTXZf/Xx/nVQVkiQt5iHjbkCSNLkMCUlSkyEhSWoyJCRJTYaEJKnJkJAkNZ3Q55snOQH4IPCDqnpVkrOBS4B7gG9X1aXduKHqLSeffHKtW7eur68jSQ9Kt9122/eqamqxdb2GBPAG4APAi5IEuAw4t6r2Jrk8yTnATcPUq+rG1oetW7eOnTt39vyVJOnBJck3W+t6292U5KXATuBrXek04I6q2tstbwPWH0VdkjQivYREkicDj6mqvx8onwTMDSzPdbVh6ws/aybJziQ7Z2dnj9M3kCRBf7ubXgI8Ksn7gEcATwG+BKweGLMG2N09hqkfoqq2AFsApqenvceIJB1HvYREVb3+wPMk65g/NvEe4MYkq7pdSBuBHcAu4Iwh6pKkEen7wDXAfmBfVe1PshnYmmQPMAvcUFU1TH0E/UqSOnkw3QV2enq6PLtJkoaT5Laqml5snRfTSZKaDAlJUpMhIUlqGsWB6weUdZu2j7sFAO68csO4W5AkZxKSpDZDQpLUZEhIkpoMCUlSkyEhSWoyJCRJTYaEJKnJkJAkNRkSkqQmQ0KS1GRISJKaDAlJUpMhIUlqMiQkSU2GhCSpqbffk0jyXmAl8DDga1X1liQ3AbsGhm2qqruSPAm4AtgD3AvMVNV9rXpfPUuSDtVbSFTVRQeeJ7kmyeld/cJFhl8BvLyq5pKcD7wSuPowdUnSCPS+uynJamAK+C6wJ8nmJNcmuaBbfyKwr6rmupdsA9a36n33K0k6qM/dTY8D3gqcCVxSVXcBz+3WBbgqyTeArwJ3Dbx0DljTPRarL/ycGWAGYO3atcf3S0jSMtfbTKKqdlXVecCpwHlJHjOwroCPAU8EdgOrB166hvlAaNUXfs6Wqpququmpqanj/0UkaRnrfXdTVe0DVgAPXbDqLOBzVbUXWNntlgLYCOxo1fvuV5J0UC+7m5I8BbiU+bOSHgl8uKq+leSdwMOBE4Fbq+qW7iWvB65OcjewD7j4CHVJ0gj0EhJV9XngZYvUX9sY/0XghUutS5JGw4vpJElNhoQkqcmQkCQ1GRKSpCZDQpLUZEhIkpoMCUlSkyEhSWoyJCRJTYaEJKnJkJAkNRkSkqQmQ0KS1GRISJKaDAlJUpMhIUlqMiQkSU2GhCSpyZCQJDX18hvXAEneC6wEHgZ8rarekuRs4BLgHuDbVXVpN3aouiRpNHqbSVTVRVV1QVW9FPi5JKcDlwHPr6oXAfcmOSdJhqn31a8k6f56392UZDUwBTwKuKOq9nartgHrgdOGrEuSRqS3kEjyuCRbgc8DW4AVwNzAkDngpO4xTH3h58wk2Zlk5+zs7PH9EpK0zPW5u2lXVZ0HnAqcx/zxidUDQ9YAu7vHMPWFn7Olqqaranpqaur4fglJWuZ6391UVfuYn0XcCZyRZFW3aiOwA9g1ZF2SNCK9nN2U5CnApcAe4JHAh6vqm0k2A1uT7AFmgRuqqoap99GvJGlxvYREVX0eeNki9ZuBm4+1LkkaDS+mkyQ1GRKSpCZDQpLUZEhIkpoMCUlSkyEhSWoyJCRJTYaEJKnJkJAkNRkSkqQmQ0KS1GRISJKaDAlJUpMhIUlqMiQkSU2GhCSpyZCQJDX18st0enBYt2n7uFsA4M4rN4y7BWnZciYhSWrqbSaR5CrgR8AaYHtVXZfkJmDXwLBNVXVXkicBVwB7gHuBmaq6r1Xvq2dJ0qF6C4mqejVAkgCfBq7r6hcuMvwK4OVVNZfkfOCVwNWHqUuSRmAUu5tWAXPd8z1JNie5NskFAElOBPZV1YEx24D1rfrCN08yk2Rnkp2zs7N9fg9JWnZGceD6cuAdAFX1XPjx7OKqJN8AvgrcNTB+jvldVGsa9UNU1RZgC8D09HQd7+YlaTnrdSaR5BLg9qq6ZbBeVQV8DHgisBtYPbB6DfOB0KpLkkakt5BIchFwT1VtbQw5C/hcVe0FViY5EAgbgR2tel/9SpLur5fdTUnOBDYBH0/yvq78xq72cOBE4NaBGcbrgauT3A3sAy4+Ql2SNAK9hERVfQZYu8iq1zbGfxF44VLrkqTR8GI6SVKTISFJajIkJElNhoQkqcmQkCQ1GRKSpCZDQpLUZEhIkpoMCUlS05JCIskZC5b9PUlJWgYOGxJJHp3kFOA1SU7pHo8Ffnc07UmSxulI9266HFgJ/HL3PMzfaO8jPfclSZoAhw2JqpoBSPKKqrpmNC1JkibFku4CW1XXJHkY8FNdaX9Vfbe/tiRJk2BJIZHkzcBTge9wcJfTBT32JUmaAEv9PYlTqurcXjuRJE2cpV4n8aNeu5AkTaSlziTWJHk/8PVueX9V/UlPPUmSJsRSQ+K9C5b3H+9GJEmTZ6lnN+0Y9o2TXMX8bqo1wPaqui7J2cAlwD3At6vq0m7sUHVJ0mgs9bYc25PckORTSf47yfVHek1Vvbqqfg94KfCqJAEuA55fVS8C7k1yzrD1o/yekqSjsKSQqKoNVfWsqnomcCrwX0N8xipgDjgNuKOq9nb1bcD6o6gfIslMkp1Jds7Ozg7RliTpSIa+C2xV7WH+Oomluhx4B3AS82FxwFxXG7a+sJ8tVTVdVdNTU1NDtCVJOpKlXkz3YmBFt3gK8NNLfN0lwO1VdUuS04HVA6vXALu7xzB1SdKILHUmsXLgsQs470gvSHIRcE9Vbe1Ku4AzkqzqljcCO46iLkkakaWe3XRdklOBJwBfqqofHm58kjOBTcDHk7yvK78R2AxsTbIHmAVuqKpKsuT6UXxHSdJRWurupt8BngHcArwpyc1V9YHW+Kr6DLB2kVU3d4+F44eqS5JGY6m7m9ZX1Su6g8SvAJ7ZZ1OSpMmw1JC45wjLkqQHoaWGxAndBW4nJHkW87cLlyQ9yC01JD7K/IVsHwHOArb31pEkaWIs9QZ/T6+qPzywkORdwMf6aUmSNCmWOpN4+ILlRx3nPiRJE2ipM4kvJ3kr8E/As4Cv9NeSJGlSLPViuj9L8nRgGvhkVX2q37akybJu02Qchrvzyg3jbkHLzFJnEgd+U8LbYkjSMjL0XWAlScuHISFJajIkJElNhoQkqcmQkCQ1GRKSpCZDQpLUZEhIkpoMCUlS05KvuB5WkhXAHwO/VFW/0dVuAnYNDNtUVXcleRJwBbAHuBeYqar7WvW+epYkHaq3kACew/zvUDx1sFhVFy4y9grg5VU1l+R84JXA1YepS5JGoLfdTVV1fVXduqC8J8nmJNcmuQAgyYnAvqqa68ZsA9a36n31K0m6vz5nEvdTVc8FSBLgqiTfAL4K3DUwbA5Y0z0Wqx8iyQwwA7B27doeupak5WssB66rqpj/ZbsnAruB1QOr1zAfCK36wvfaUlXTVTU9NTXVX9OStAyN8+yms4DPVdVeYGWSA4GwEdjRqo+hT0latkaxu+nHZyMleSfzP4V6InBrVd3SrXo9cHWSu4F9wMVHqEuSRqD3kKiqZw88f21jzBeBFy61LkkaDS+mkyQ1GRKSpCZDQpLUZEhIkpoMCUlSkyEhSWoyJCRJTSO9d5OkB751m7aPuwUA7rxyw7hbWBacSUiSmgwJSVKTISFJajIkJElNhoQkqcmQkCQ1GRKSpCZDQpLUZEhIkpoMCUlSkyEhSWrqLSSSrEjytiSfHKidnWR7kr9N8qdHW5ckjUafM4nnAB+lu4lgkgCXAc+vqhcB9yY5Z9h6j/1KkhboLSSq6vqqunWgdBpwR1Xt7Za3AeuPoi5JGpFRHpM4CZgbWJ7rasPWD5FkJsnOJDtnZ2ePe9OStJyNMiR2A6sHltd0tWHrh6iqLVU1XVXTU1NTx71pSVrORhkSu4AzkqzqljcCO46iLkkakVH8Mt19AFW1P8lmYGuSPcAscENV1TD1EfQrSer0HhJV9eyB5zcDNy8yZqi6JGk0vJhOktRkSEiSmgwJSVKTISFJajIkJElNhoQkqcmQkCQ1GRKSpCZDQpLUZEhIkpoMCUlSkyEhSWoyJCRJTYaEJKnJkJAkNRkSkqQmQ0KS1GRISJKaDAlJUlPvv3E9KMntwK3d4j7g4qqqJGcDlwD3AN+uqku78YvWJUmjMdKQAHZX1YWDhSQBLgPOraq9SS5Pcg5w02L1qrpxxD1L0rI16t1NK5K8PcnWJM/taqcBd1TV3m55G7D+MHVJ0oiMdCZRVesBkqwE/i7Jl4GTgLmBYXNdrVU/RJIZYAZg7dq1/TQuScvUWA5cV9V9wI3A44HdwOqB1Wu6Wqu+8L22VNV0VU1PTU3117QkLUPjPLvpacAXgF3AGUlWdfWNwI7D1CVJIzLqs5uuAX4IPBzYVlV3dvXNwNYke4BZ4IburKf71UfZryQtd6M+JvGKRv1m4Oal1iVJo+HFdJKkJkNCktRkSEiSmgwJSVKTISFJajIkJElNo77BnyQ9aKzbtH3cLQBw55UbentvZxKSpCZDQpLUZEhIkpoMCUlSkyEhSWoyJCRJTYaEJKnJkJAkNRkSkqQmQ0KS1GRISJKaDAlJUtPE3+AvyXnAi4H9wGer6h1jbkmSlo2JnkkkeQTwcmBjVT0PeEKSU8fcliQtGxMdEsCZwI1VVd3y9cD6MfYjSctKDv79nTxJXgqsqqr3d8vPBJ5aVW8fGDMDzHSLpwP/NvJG7+9k4HvjbmJCuC0Oclsc5LY4aBK2xWOramqxFZN+TGI38PiB5TVd7ceqaguwZZRNHUmSnVU1Pe4+JoHb4iC3xUFui4MmfVtM+u6mW4Gzk6Rb/k3g02PsR5KWlYmeSVTVXUmuBT6UZB/whar66rj7kqTlYqJDAqCqPgR8aNx9DGmidn+NmdviILfFQW6LgyZ6W0z0gWtJ0nhN+jEJSdIYGRKSpKaJPybxQJDkF5i/yO8k5k/R/XRVfW28XUnSsXMmcYySvA54E/B94F+6//5RkkvH2tgESXLSuHvQeCU5Nckju+ePTvIz4+5pEiS5cNw9HIkziWP3K1X1ggW1Dyf54Fi6GaPuZoyvYn429Y6q+my3ajNw0dga01gleRPw88DJSbYALwMekuTD3dmLy0aSq4AVBxaBM5M8BdhXVRP5/4ghcexa23DlSLuYDOdW1VlJTgQ2J1nX/RHIkV6oB7XTq+q8JKuBncCpVfWjJNfxwDu9/Vg9Eng/8PVu+W3dY//YOjoCQ+LY/U2SjwM3AnPM3zrk14Frx9rVeMwBVNX/Aq9L8gdJzgeW5XnWST7BwX81/rgM/F9VbRhDS+OyB6Cqvp/ks1X1o65+9xh7GpffBt4APLSqPp7k7qr65ribOhxD4hhV1V8n+Sjzd6w9Cfgy8FdV9T/j7WwsDvmDWFXvTnIRcO6Y+hm324Drq+pz425kzPYNPH/zwPNHjLqRcauq/cBbk/xWkkt4AMyyvZhOvUvytIHjE8tGkocAG6rqY+PuZRIleXJV3T7uPsYlyS8CL6mqTePu5XAMCUlSk6fASpKaDAlJUpMhIR2jJD+b5C8a634tyWVDvNcnjl9n0rEzJKRjt4L7n+q6lHWLWY7X12iCeQqsdJwkeQ7wDOZPa/xBVb2lWzWd5J3MX0j1w6p6TZKVwLuYv4hqNfD2qvrK6LuWDs+ZhHT8fBM4kfmLB1/QXWEMcEJVvbaqLmD+dhRPBc4H/rmqfh+4mPlbl0gTx5mEdHw8hPlfGHteVX0nyTrgYd26LwyMux1YBzwBWNGdKw/wg5F0KQ3JkJCOjwL+owuIn2D+CvwDfnXg+ZOAvwROAf69qraNrkVpeIaEdOz2M3/riW8leQ/wk8A/Mh8c+4H/TPJuYBXwnar61yRfB/48yYZuzEeq6h+A+8bQv9TkFdeSpCYPXEuSmgwJSVKTISFJajIkJElNhoQkqcmQkCQ1GRKSpKb/B5WVH/ij8wR1AAAAAElFTkSuQmCC\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train_kaggle.label.value_counts().plot.bar()\n",
    "plt.xlabel(\"label\")\n",
    "plt.ylabel(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "geological-philadelphia",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-30T18:39:36.172890Z",
     "iopub.status.busy": "2021-05-30T18:39:36.167698Z",
     "iopub.status.idle": "2021-05-30T18:39:39.954054Z",
     "shell.execute_reply": "2021-05-30T18:39:39.954888Z",
     "shell.execute_reply.started": "2021-05-30T09:28:47.478488Z"
    },
    "id": "geological-philadelphia",
    "outputId": "5a67acba-6da1-4b7b-c63c-db74aa3f3ab5",
    "papermill": {
     "duration": 3.835127,
     "end_time": "2021-05-30T18:39:39.955083",
     "exception": false,
     "start_time": "2021-05-30T18:39:36.119956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9786 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_aug.flow_from_dataframe(\n",
    "    dataframe = df_train_kaggle,\n",
    "    directory = None,\n",
    "    x_col=\"filepath\",\n",
    "    y_col=\"label\",\n",
    "    batch_size = 8,\n",
    "    seed = 42,\n",
    "    shuffle = True,\n",
    "    class_mode = \"raw\",\n",
    "    target_size = (224,224)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-chest",
   "metadata": {
    "id": "asian-chest",
    "papermill": {
     "duration": 0.027295,
     "end_time": "2021-05-30T18:39:40.011354",
     "exception": false,
     "start_time": "2021-05-30T18:39:39.984059",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 검증, 테스트 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "threaded-characterization",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "execution": {
     "iopub.execute_input": "2021-05-30T18:40:02.229132Z",
     "iopub.status.busy": "2021-05-30T18:40:02.228400Z",
     "iopub.status.idle": "2021-05-30T18:40:02.250117Z",
     "shell.execute_reply": "2021-05-30T18:40:02.250642Z",
     "shell.execute_reply.started": "2021-05-30T08:33:23.833280Z"
    },
    "id": "threaded-characterization",
    "outputId": "f6fc6702-0a3d-4689-ce71-a698cbea485e",
    "papermill": {
     "duration": 0.059732,
     "end_time": "2021-05-30T18:40:02.250775",
     "exception": false,
     "start_time": "2021-05-30T18:40:02.191043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                  filename  label\n0  ./KneeXray//KneeXray//train/Image_1.jpg      0\n1  ./KneeXray//KneeXray//train/Image_2.jpg      1\n2  ./KneeXray//KneeXray//train/Image_3.jpg      0\n3  ./KneeXray//KneeXray//train/Image_4.jpg      1\n4  ./KneeXray//KneeXray//train/Image_5.jpg      2",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>./KneeXray//KneeXray//train/Image_1.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>./KneeXray//KneeXray//train/Image_2.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>./KneeXray//KneeXray//train/Image_3.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>./KneeXray//KneeXray//train/Image_4.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>./KneeXray//KneeXray//train/Image_5.jpg</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compi_root_path= \"./KneeXray//KneeXray//\"\n",
    "df_val_compi = pd.read_csv(compi_root_path + \"Train.csv\")\n",
    "df_val_compi[\"filename\"] = df_val_compi.filename.apply(lambda x: compi_root_path+\"train/\" + x)\n",
    "df_val_compi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c720bcf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(7828, 2)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val_compi.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2810ec",
   "metadata": {
    "id": "hairy-greene",
    "papermill": {
     "duration": 0.030406,
     "end_time": "2021-05-30T18:39:35.680660",
     "exception": false,
     "start_time": "2021-05-30T18:39:35.650254",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 검증 데이터 분포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "suburban-shareware",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "execution": {
     "iopub.execute_input": "2021-05-30T18:40:02.339459Z",
     "iopub.status.busy": "2021-05-30T18:40:02.324109Z",
     "iopub.status.idle": "2021-05-30T18:40:02.442504Z",
     "shell.execute_reply": "2021-05-30T18:40:02.442056Z",
     "shell.execute_reply.started": "2021-05-30T08:33:23.865039Z"
    },
    "id": "suburban-shareware",
    "outputId": "0daca1fb-c665-4a25-8da7-9250f7756ff1",
    "papermill": {
     "duration": 0.158988,
     "end_time": "2021-05-30T18:40:02.442618",
     "exception": false,
     "start_time": "2021-05-30T18:40:02.283630",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Text(0, 0.5, 'count')"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_val_compi.label.value_counts().plot.bar()\n",
    "plt.xlabel(\"label\")\n",
    "plt.ylabel(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "saving-homework",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-30T18:40:02.529039Z",
     "iopub.status.busy": "2021-05-30T18:40:02.528305Z",
     "iopub.status.idle": "2021-05-30T18:40:02.574290Z",
     "shell.execute_reply": "2021-05-30T18:40:02.574903Z",
     "shell.execute_reply.started": "2021-05-30T08:54:21.600981Z"
    },
    "id": "saving-homework",
    "outputId": "21b896d0-bceb-4621-efb3-33908214d2d6",
    "papermill": {
     "duration": 0.098388,
     "end_time": "2021-05-30T18:40:02.575081",
     "exception": false,
     "start_time": "2021-05-30T18:40:02.476693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7828 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "valid_generator = valid_aug.flow_from_dataframe( \n",
    "    dataframe= df_val_compi,\n",
    "    x_col = \"filename\",\n",
    "    y_col = \"label\",\n",
    "    batch_size = 8,\n",
    "    seed = 42,\n",
    "    shuffle = True,\n",
    "    class_mode = \"raw\",\n",
    "    target_size = (224,224)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marked-belarus",
   "metadata": {
    "id": "marked-belarus",
    "papermill": {
     "duration": 0.033792,
     "end_time": "2021-05-30T18:40:02.643887",
     "exception": false,
     "start_time": "2021-05-30T18:40:02.610095",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 모델 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras_applications.imagenet_utils import _obtain_input_shape\n",
    "from keras.models import Model\n",
    "from keras.engine.topology import get_source_inputs\n",
    "from keras.layers import Activation, Add, Concatenate, GlobalAveragePooling2D,GlobalMaxPooling2D, Input, Dense\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, BatchNormalization, Lambda\n",
    "# from keras.applications.mobilenet import DepthwiseConv2D\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def ShuffleNet(include_top=True, input_tensor=None, scale_factor=1.0, pooling='max',\n",
    "               input_shape=(224,224,3), groups=1, load_model=None, num_shuffle_units=[3, 7, 3],\n",
    "               bottleneck_ratio=0.25, classes=1000):\n",
    "    \"\"\"\n",
    "    ShuffleNet implementation for Keras 2\n",
    "    ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices\n",
    "    Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, Jian Sun\n",
    "    https://arxiv.org/pdf/1707.01083.pdf\n",
    "    Note that only TensorFlow is supported for now, therefore it only works\n",
    "    with the data format `image_data_format='channels_last'` in your Keras\n",
    "    config at `~/.keras/keras.json`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    include_top: bool(True)\n",
    "         whether to include the fully-connected layer at the top of the network.\n",
    "    input_tensor:\n",
    "        optional Keras tensor (i.e. output of `layers.Input()`) to use as image input for the model.\n",
    "    scale_factor:\n",
    "        scales the number of output channels\n",
    "    input_shape:\n",
    "    pooling:\n",
    "        Optional pooling mode for feature extraction\n",
    "        when `include_top` is `False`.\n",
    "        - `None` means that the output of the model\n",
    "            will be the 4D tensor output of the\n",
    "            last convolutional layer.\n",
    "        - `avg` means that global average pooling\n",
    "            will be applied to the output of the\n",
    "            last convolutional layer, and thus\n",
    "            the output of the model will be a\n",
    "            2D tensor.\n",
    "        - `max` means that global max pooling will\n",
    "            be applied.\n",
    "    groups: int\n",
    "        number of groups per channel\n",
    "    num_shuffle_units: list([3,7,3])\n",
    "        number of stages (list length) and the number of shufflenet units in a\n",
    "        stage beginning with stage 2 because stage 1 is fixed\n",
    "        e.g. idx 0 contains 3 + 1 (first shuffle unit in each stage differs) shufflenet units for stage 2\n",
    "        idx 1 contains 7 + 1 Shufflenet Units for stage 3 and\n",
    "        idx 2 contains 3 + 1 Shufflenet Units\n",
    "    bottleneck_ratio:\n",
    "        bottleneck ratio implies the ratio of bottleneck channels to output channels.\n",
    "        For example, bottleneck ratio = 1 : 4 means the output feature map is 4 times\n",
    "        the width of the bottleneck feature map.\n",
    "    classes: int(1000)\n",
    "        number of classes to predict\n",
    "    Returns\n",
    "    -------\n",
    "        A Keras model instance\n",
    "    References\n",
    "    ----------\n",
    "    - [ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices]\n",
    "      (http://www.arxiv.org/pdf/1707.01083.pdf)\n",
    "    \"\"\"\n",
    "\n",
    "    if K.backend() != 'tensorflow':\n",
    "        raise RuntimeError('Only TensorFlow backend is currently supported, '\n",
    "                           'as other backends do not support ')\n",
    "\n",
    "    name = \"ShuffleNet_%.2gX_g%d_br_%.2g_%s\" % (scale_factor, groups, bottleneck_ratio, \"\".join([str(x) for x in num_shuffle_units]))\n",
    "\n",
    "    input_shape = _obtain_input_shape(input_shape,\n",
    "                                      default_size=224,\n",
    "                                      min_size=28,\n",
    "                                      require_flatten=include_top,\n",
    "                                      data_format=K.image_data_format())\n",
    "\n",
    "    out_dim_stage_two = {1: 144, 2: 200, 3: 240, 4: 272, 8: 384}\n",
    "    if groups not in out_dim_stage_two:\n",
    "        raise ValueError(\"Invalid number of groups.\")\n",
    "\n",
    "    if pooling not in ['max','avg']:\n",
    "        raise ValueError(\"Invalid value for pooling.\")\n",
    "\n",
    "    if not (float(scale_factor) * 4).is_integer():\n",
    "        raise ValueError(\"Invalid value for scale_factor. Should be x over 4.\")\n",
    "\n",
    "    exp = np.insert(np.arange(0, len(num_shuffle_units), dtype=np.float32), 0, 0)\n",
    "    out_channels_in_stage = 2 ** exp\n",
    "    out_channels_in_stage *= out_dim_stage_two[groups]  # calculate output channels for each stage\n",
    "    out_channels_in_stage[0] = 24  # first stage has always 24 output channels\n",
    "    out_channels_in_stage *= scale_factor\n",
    "    out_channels_in_stage = out_channels_in_stage.astype(int)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "\n",
    "    # create shufflenet architecture\n",
    "    x = Conv2D(filters=out_channels_in_stage[0], kernel_size=(3, 3), padding='same',\n",
    "               use_bias=False, strides=(2, 2), activation=\"relu\", name=\"conv1\")(img_input)\n",
    "    x = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same', name=\"maxpool1\")(x)\n",
    "\n",
    "    # create stages containing shufflenet units beginning at stage 2\n",
    "    for stage in range(0, len(num_shuffle_units)):\n",
    "        repeat = num_shuffle_units[stage]\n",
    "        x = _block(x, out_channels_in_stage, repeat=repeat,\n",
    "                   bottleneck_ratio=bottleneck_ratio,\n",
    "                   groups=groups, stage=stage + 2)\n",
    "\n",
    "    if pooling == 'avg':\n",
    "        x = GlobalAveragePooling2D(name=\"global_pool\")(x)\n",
    "    elif pooling == 'max':\n",
    "        x = GlobalMaxPooling2D(name=\"global_pool\")(x)\n",
    "\n",
    "    if include_top:\n",
    "        x = Dense(units=classes, name=\"fc\")(x)\n",
    "        x = Activation('softmax', name='softmax')(x)\n",
    "\n",
    "\n",
    "    if input_tensor is not None:\n",
    "        inputs = get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=x, name=name)\n",
    "\n",
    "    if load_model is not None:\n",
    "        model.load_weights('', by_name=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def _block(x, channel_map, bottleneck_ratio, repeat=1, groups=1, stage=1):\n",
    "    \"\"\"\n",
    "    creates a bottleneck block containing `repeat + 1` shuffle units\n",
    "    Parameters\n",
    "    ----------\n",
    "    x:\n",
    "        Input tensor of with `channels_last` data format\n",
    "    channel_map: list\n",
    "        list containing the number of output channels for a stage\n",
    "    repeat: int(1)\n",
    "        number of repetitions for a shuffle unit with stride 1\n",
    "    groups: int(1)\n",
    "        number of groups per channel\n",
    "    bottleneck_ratio: float\n",
    "        bottleneck ratio implies the ratio of bottleneck channels to output channels.\n",
    "        For example, bottleneck ratio = 1 : 4 means the output feature map is 4 times\n",
    "        the width of the bottleneck feature map.\n",
    "    stage: int(1)\n",
    "        stage number\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    x = _shuffle_unit(x, in_channels=channel_map[stage - 2],\n",
    "                      out_channels=channel_map[stage - 1], strides=2,\n",
    "                      groups=groups, bottleneck_ratio=bottleneck_ratio,\n",
    "                      stage=stage, block=1)\n",
    "\n",
    "    for i in range(1, repeat + 1):\n",
    "        x = _shuffle_unit(x, in_channels=channel_map[stage - 1],\n",
    "                          out_channels=channel_map[stage - 1], strides=1,\n",
    "                          groups=groups, bottleneck_ratio=bottleneck_ratio,\n",
    "                          stage=stage, block=(i + 1))\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def _shuffle_unit(inputs, in_channels, out_channels, groups, bottleneck_ratio, strides=2, stage=1, block=1):\n",
    "    \"\"\"\n",
    "    creates a shuffleunit\n",
    "    Parameters\n",
    "    ----------\n",
    "    inputs:\n",
    "        Input tensor of with `channels_last` data format\n",
    "    in_channels:\n",
    "        number of input channels\n",
    "    out_channels:\n",
    "        number of output channels\n",
    "    strides:\n",
    "        An integer or tuple/list of 2 integers,\n",
    "        specifying the strides of the convolution along the width and height.\n",
    "    groups: int(1)\n",
    "        number of groups per channel\n",
    "    bottleneck_ratio: float\n",
    "        bottleneck ratio implies the ratio of bottleneck channels to output channels.\n",
    "        For example, bottleneck ratio = 1 : 4 means the output feature map is 4 times\n",
    "        the width of the bottleneck feature map.\n",
    "    stage: int(1)\n",
    "        stage number\n",
    "    block: int(1)\n",
    "        block number\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        bn_axis = -1\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "\n",
    "    prefix = 'stage%d/block%d' % (stage, block)\n",
    "\n",
    "    #if strides >= 2:\n",
    "        #out_channels -= in_channels\n",
    "\n",
    "    # default: 1/4 of the output channel of a ShuffleNet Unit\n",
    "    bottleneck_channels = int(out_channels * bottleneck_ratio)\n",
    "    groups = (1 if stage == 2 and block == 1 else groups)\n",
    "\n",
    "    x = _group_conv(inputs, in_channels, out_channels=bottleneck_channels,\n",
    "                    groups=(1 if stage == 2 and block == 1 else groups),\n",
    "                    name='%s/1x1_gconv_1' % prefix)\n",
    "    x = BatchNormalization(axis=bn_axis, name='%s/bn_gconv_1' % prefix)(x)\n",
    "    x = Activation('relu', name='%s/relu_gconv_1' % prefix)(x)\n",
    "\n",
    "    x = Lambda(channel_shuffle, arguments={'groups': groups}, name='%s/channel_shuffle' % prefix)(x)\n",
    "    x = layers.DepthwiseConv2D(kernel_size=(3, 3), padding=\"same\", use_bias=False,\n",
    "                        strides=strides, name='%s/1x1_dwconv_1' % prefix)(x)\n",
    "    x = BatchNormalization(axis=bn_axis, name='%s/bn_dwconv_1' % prefix)(x)\n",
    "\n",
    "    x = _group_conv(x, bottleneck_channels, out_channels=out_channels if strides == 1 else out_channels - in_channels,\n",
    "                    groups=groups, name='%s/1x1_gconv_2' % prefix)\n",
    "    x = BatchNormalization(axis=bn_axis, name='%s/bn_gconv_2' % prefix)(x)\n",
    "\n",
    "    if strides < 2:\n",
    "        ret = Add(name='%s/add' % prefix)([x, inputs])\n",
    "    else:\n",
    "        avg = AveragePooling2D(pool_size=3, strides=2, padding='same', name='%s/avg_pool' % prefix)(inputs)\n",
    "        ret = Concatenate(bn_axis, name='%s/concat' % prefix)([x, avg])\n",
    "\n",
    "    ret = Activation('relu', name='%s/relu_out' % prefix)(ret)\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def _group_conv(x, in_channels, out_channels, groups, kernel=1, stride=1, name=''):\n",
    "    \"\"\"\n",
    "    grouped convolution\n",
    "    Parameters\n",
    "    ----------\n",
    "    x:\n",
    "        Input tensor of with `channels_last` data format\n",
    "    in_channels:\n",
    "        number of input channels\n",
    "    out_channels:\n",
    "        number of output channels\n",
    "    groups:\n",
    "        number of groups per channel\n",
    "    kernel: int(1)\n",
    "        An integer or tuple/list of 2 integers, specifying the\n",
    "        width and height of the 2D convolution window.\n",
    "        Can be a single integer to specify the same value for\n",
    "        all spatial dimensions.\n",
    "    stride: int(1)\n",
    "        An integer or tuple/list of 2 integers,\n",
    "        specifying the strides of the convolution along the width and height.\n",
    "        Can be a single integer to specify the same value for all spatial dimensions.\n",
    "    name: str\n",
    "        A string to specifies the layer name\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    if groups == 1:\n",
    "        return Conv2D(filters=out_channels, kernel_size=kernel, padding='same',\n",
    "                      use_bias=False, strides=stride, name=name)(x)\n",
    "\n",
    "    # number of intput channels per group\n",
    "    ig = in_channels // groups\n",
    "    group_list = []\n",
    "\n",
    "    assert out_channels % groups == 0\n",
    "\n",
    "    for i in range(groups):\n",
    "        offset = i * ig\n",
    "        group = Lambda(lambda z: z[:, :, :, offset: offset + ig], name='%s/g%d_slice' % (name, i))(x)\n",
    "        group_list.append(Conv2D(int(0.5 + out_channels / groups), kernel_size=kernel, strides=stride,\n",
    "                                 use_bias=False, padding='same', name='%s_/g%d' % (name, i))(group))\n",
    "    return Concatenate(name='%s/concat' % name)(group_list)\n",
    "\n",
    "\n",
    "def channel_shuffle(x, groups):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    x:\n",
    "        Input tensor of with `channels_last` data format\n",
    "    groups: int\n",
    "        number of groups per channel\n",
    "    Returns\n",
    "    -------\n",
    "        channel shuffled output tensor\n",
    "    Examples\n",
    "    --------\n",
    "    Example for a 1D Array with 3 groups\n",
    "    >>> d = np.array([0,1,2,3,4,5,6,7,8])\n",
    "    >>> x = np.reshape(d, (3,3))\n",
    "    >>> x = np.transpose(x, [1,0])\n",
    "    >>> x = np.reshape(x, (9,))\n",
    "    '[0 1 2 3 4 5 6 7 8] --> [0 3 6 1 4 7 2 5 8]'\n",
    "    \"\"\"\n",
    "    height, width, in_channels = x.shape.as_list()[1:]\n",
    "    channels_per_group = in_channels // groups\n",
    "\n",
    "    x = K.reshape(x, [-1, height, width, groups, channels_per_group])\n",
    "    x = K.permute_dimensions(x, (0, 1, 2, 4, 3))  # transpose\n",
    "    x = K.reshape(x, [-1, height, width, in_channels])\n",
    "\n",
    "    return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "buried-tablet",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-30T18:40:02.724467Z",
     "iopub.status.busy": "2021-05-30T18:40:02.723830Z",
     "iopub.status.idle": "2021-05-30T18:40:08.132744Z",
     "shell.execute_reply": "2021-05-30T18:40:08.131632Z",
     "shell.execute_reply.started": "2021-05-30T09:28:56.068101Z"
    },
    "id": "buried-tablet",
    "outputId": "da82c4ac-8660-4b77-94f7-5e1342b4b10e",
    "papermill": {
     "duration": 5.455093,
     "end_time": "2021-05-30T18:40:08.132878",
     "exception": false,
     "start_time": "2021-05-30T18:40:02.677785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "xception_model = ShuffleNet(groups=3, classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb92055e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ShuffleNet_1X_g3_br_0.25_373\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 112, 112, 24) 648         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "maxpool1 (MaxPooling2D)         (None, 56, 56, 24)   0           conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block1/1x1_gconv_1 (Conv (None, 56, 56, 60)   1440        maxpool1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block1/bn_gconv_1 (Batch (None, 56, 56, 60)   240         stage2/block1/1x1_gconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block1/relu_gconv_1 (Act (None, 56, 56, 60)   0           stage2/block1/bn_gconv_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block1/channel_shuffle ( (None, 56, 56, 60)   0           stage2/block1/relu_gconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block1/1x1_dwconv_1 (Dep (None, 28, 28, 60)   540         stage2/block1/channel_shuffle[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage2/block1/bn_dwconv_1 (Batc (None, 28, 28, 60)   240         stage2/block1/1x1_dwconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block1/1x1_gconv_2 (Conv (None, 28, 28, 216)  12960       stage2/block1/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block1/bn_gconv_2 (Batch (None, 28, 28, 216)  864         stage2/block1/1x1_gconv_2[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block1/avg_pool (Average (None, 28, 28, 24)   0           maxpool1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block1/concat (Concatena (None, 28, 28, 240)  0           stage2/block1/bn_gconv_2[0][0]   \n",
      "                                                                 stage2/block1/avg_pool[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block1/relu_out (Activat (None, 28, 28, 240)  0           stage2/block1/concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block2/1x1_gconv_1/g0_sl (None, 28, 28, 80)   0           stage2/block1/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block2/1x1_gconv_1/g1_sl (None, 28, 28, 80)   0           stage2/block1/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block2/1x1_gconv_1/g2_sl (None, 28, 28, 80)   0           stage2/block1/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block2/1x1_gconv_1_/g0 ( (None, 28, 28, 20)   1600        stage2/block2/1x1_gconv_1/g0_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage2/block2/1x1_gconv_1_/g1 ( (None, 28, 28, 20)   1600        stage2/block2/1x1_gconv_1/g1_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage2/block2/1x1_gconv_1_/g2 ( (None, 28, 28, 20)   1600        stage2/block2/1x1_gconv_1/g2_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage2/block2/1x1_gconv_1/conca (None, 28, 28, 60)   0           stage2/block2/1x1_gconv_1_/g0[0][\n",
      "                                                                 stage2/block2/1x1_gconv_1_/g1[0][\n",
      "                                                                 stage2/block2/1x1_gconv_1_/g2[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage2/block2/bn_gconv_1 (Batch (None, 28, 28, 60)   240         stage2/block2/1x1_gconv_1/concat[\n",
      "__________________________________________________________________________________________________\n",
      "stage2/block2/relu_gconv_1 (Act (None, 28, 28, 60)   0           stage2/block2/bn_gconv_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block2/channel_shuffle ( (None, 28, 28, 60)   0           stage2/block2/relu_gconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block2/1x1_dwconv_1 (Dep (None, 28, 28, 60)   540         stage2/block2/channel_shuffle[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage2/block2/bn_dwconv_1 (Batc (None, 28, 28, 60)   240         stage2/block2/1x1_dwconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block2/1x1_gconv_2/g0_sl (None, 28, 28, 20)   0           stage2/block2/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block2/1x1_gconv_2/g1_sl (None, 28, 28, 20)   0           stage2/block2/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block2/1x1_gconv_2/g2_sl (None, 28, 28, 20)   0           stage2/block2/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block2/1x1_gconv_2_/g0 ( (None, 28, 28, 80)   1600        stage2/block2/1x1_gconv_2/g0_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage2/block2/1x1_gconv_2_/g1 ( (None, 28, 28, 80)   1600        stage2/block2/1x1_gconv_2/g1_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage2/block2/1x1_gconv_2_/g2 ( (None, 28, 28, 80)   1600        stage2/block2/1x1_gconv_2/g2_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage2/block2/1x1_gconv_2/conca (None, 28, 28, 240)  0           stage2/block2/1x1_gconv_2_/g0[0][\n",
      "                                                                 stage2/block2/1x1_gconv_2_/g1[0][\n",
      "                                                                 stage2/block2/1x1_gconv_2_/g2[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage2/block2/bn_gconv_2 (Batch (None, 28, 28, 240)  960         stage2/block2/1x1_gconv_2/concat[\n",
      "__________________________________________________________________________________________________\n",
      "stage2/block2/add (Add)         (None, 28, 28, 240)  0           stage2/block2/bn_gconv_2[0][0]   \n",
      "                                                                 stage2/block1/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block2/relu_out (Activat (None, 28, 28, 240)  0           stage2/block2/add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block3/1x1_gconv_1/g0_sl (None, 28, 28, 80)   0           stage2/block2/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block3/1x1_gconv_1/g1_sl (None, 28, 28, 80)   0           stage2/block2/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block3/1x1_gconv_1/g2_sl (None, 28, 28, 80)   0           stage2/block2/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block3/1x1_gconv_1_/g0 ( (None, 28, 28, 20)   1600        stage2/block3/1x1_gconv_1/g0_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage2/block3/1x1_gconv_1_/g1 ( (None, 28, 28, 20)   1600        stage2/block3/1x1_gconv_1/g1_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage2/block3/1x1_gconv_1_/g2 ( (None, 28, 28, 20)   1600        stage2/block3/1x1_gconv_1/g2_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage2/block3/1x1_gconv_1/conca (None, 28, 28, 60)   0           stage2/block3/1x1_gconv_1_/g0[0][\n",
      "                                                                 stage2/block3/1x1_gconv_1_/g1[0][\n",
      "                                                                 stage2/block3/1x1_gconv_1_/g2[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage2/block3/bn_gconv_1 (Batch (None, 28, 28, 60)   240         stage2/block3/1x1_gconv_1/concat[\n",
      "__________________________________________________________________________________________________\n",
      "stage2/block3/relu_gconv_1 (Act (None, 28, 28, 60)   0           stage2/block3/bn_gconv_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block3/channel_shuffle ( (None, 28, 28, 60)   0           stage2/block3/relu_gconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block3/1x1_dwconv_1 (Dep (None, 28, 28, 60)   540         stage2/block3/channel_shuffle[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage2/block3/bn_dwconv_1 (Batc (None, 28, 28, 60)   240         stage2/block3/1x1_dwconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block3/1x1_gconv_2/g0_sl (None, 28, 28, 20)   0           stage2/block3/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block3/1x1_gconv_2/g1_sl (None, 28, 28, 20)   0           stage2/block3/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block3/1x1_gconv_2/g2_sl (None, 28, 28, 20)   0           stage2/block3/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block3/1x1_gconv_2_/g0 ( (None, 28, 28, 80)   1600        stage2/block3/1x1_gconv_2/g0_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage2/block3/1x1_gconv_2_/g1 ( (None, 28, 28, 80)   1600        stage2/block3/1x1_gconv_2/g1_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage2/block3/1x1_gconv_2_/g2 ( (None, 28, 28, 80)   1600        stage2/block3/1x1_gconv_2/g2_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage2/block3/1x1_gconv_2/conca (None, 28, 28, 240)  0           stage2/block3/1x1_gconv_2_/g0[0][\n",
      "                                                                 stage2/block3/1x1_gconv_2_/g1[0][\n",
      "                                                                 stage2/block3/1x1_gconv_2_/g2[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage2/block3/bn_gconv_2 (Batch (None, 28, 28, 240)  960         stage2/block3/1x1_gconv_2/concat[\n",
      "__________________________________________________________________________________________________\n",
      "stage2/block3/add (Add)         (None, 28, 28, 240)  0           stage2/block3/bn_gconv_2[0][0]   \n",
      "                                                                 stage2/block2/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block3/relu_out (Activat (None, 28, 28, 240)  0           stage2/block3/add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block4/1x1_gconv_1/g0_sl (None, 28, 28, 80)   0           stage2/block3/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block4/1x1_gconv_1/g1_sl (None, 28, 28, 80)   0           stage2/block3/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block4/1x1_gconv_1/g2_sl (None, 28, 28, 80)   0           stage2/block3/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block4/1x1_gconv_1_/g0 ( (None, 28, 28, 20)   1600        stage2/block4/1x1_gconv_1/g0_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage2/block4/1x1_gconv_1_/g1 ( (None, 28, 28, 20)   1600        stage2/block4/1x1_gconv_1/g1_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage2/block4/1x1_gconv_1_/g2 ( (None, 28, 28, 20)   1600        stage2/block4/1x1_gconv_1/g2_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage2/block4/1x1_gconv_1/conca (None, 28, 28, 60)   0           stage2/block4/1x1_gconv_1_/g0[0][\n",
      "                                                                 stage2/block4/1x1_gconv_1_/g1[0][\n",
      "                                                                 stage2/block4/1x1_gconv_1_/g2[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage2/block4/bn_gconv_1 (Batch (None, 28, 28, 60)   240         stage2/block4/1x1_gconv_1/concat[\n",
      "__________________________________________________________________________________________________\n",
      "stage2/block4/relu_gconv_1 (Act (None, 28, 28, 60)   0           stage2/block4/bn_gconv_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block4/channel_shuffle ( (None, 28, 28, 60)   0           stage2/block4/relu_gconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block4/1x1_dwconv_1 (Dep (None, 28, 28, 60)   540         stage2/block4/channel_shuffle[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage2/block4/bn_dwconv_1 (Batc (None, 28, 28, 60)   240         stage2/block4/1x1_dwconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block4/1x1_gconv_2/g0_sl (None, 28, 28, 20)   0           stage2/block4/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block4/1x1_gconv_2/g1_sl (None, 28, 28, 20)   0           stage2/block4/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block4/1x1_gconv_2/g2_sl (None, 28, 28, 20)   0           stage2/block4/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block4/1x1_gconv_2_/g0 ( (None, 28, 28, 80)   1600        stage2/block4/1x1_gconv_2/g0_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage2/block4/1x1_gconv_2_/g1 ( (None, 28, 28, 80)   1600        stage2/block4/1x1_gconv_2/g1_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage2/block4/1x1_gconv_2_/g2 ( (None, 28, 28, 80)   1600        stage2/block4/1x1_gconv_2/g2_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage2/block4/1x1_gconv_2/conca (None, 28, 28, 240)  0           stage2/block4/1x1_gconv_2_/g0[0][\n",
      "                                                                 stage2/block4/1x1_gconv_2_/g1[0][\n",
      "                                                                 stage2/block4/1x1_gconv_2_/g2[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage2/block4/bn_gconv_2 (Batch (None, 28, 28, 240)  960         stage2/block4/1x1_gconv_2/concat[\n",
      "__________________________________________________________________________________________________\n",
      "stage2/block4/add (Add)         (None, 28, 28, 240)  0           stage2/block4/bn_gconv_2[0][0]   \n",
      "                                                                 stage2/block3/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage2/block4/relu_out (Activat (None, 28, 28, 240)  0           stage2/block4/add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block1/1x1_gconv_1/g0_sl (None, 28, 28, 80)   0           stage2/block4/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block1/1x1_gconv_1/g1_sl (None, 28, 28, 80)   0           stage2/block4/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block1/1x1_gconv_1/g2_sl (None, 28, 28, 80)   0           stage2/block4/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block1/1x1_gconv_1_/g0 ( (None, 28, 28, 40)   3200        stage3/block1/1x1_gconv_1/g0_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block1/1x1_gconv_1_/g1 ( (None, 28, 28, 40)   3200        stage3/block1/1x1_gconv_1/g1_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block1/1x1_gconv_1_/g2 ( (None, 28, 28, 40)   3200        stage3/block1/1x1_gconv_1/g2_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block1/1x1_gconv_1/conca (None, 28, 28, 120)  0           stage3/block1/1x1_gconv_1_/g0[0][\n",
      "                                                                 stage3/block1/1x1_gconv_1_/g1[0][\n",
      "                                                                 stage3/block1/1x1_gconv_1_/g2[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block1/bn_gconv_1 (Batch (None, 28, 28, 120)  480         stage3/block1/1x1_gconv_1/concat[\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block1/relu_gconv_1 (Act (None, 28, 28, 120)  0           stage3/block1/bn_gconv_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block1/channel_shuffle ( (None, 28, 28, 120)  0           stage3/block1/relu_gconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block1/1x1_dwconv_1 (Dep (None, 14, 14, 120)  1080        stage3/block1/channel_shuffle[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block1/bn_dwconv_1 (Batc (None, 14, 14, 120)  480         stage3/block1/1x1_dwconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block1/1x1_gconv_2/g0_sl (None, 14, 14, 40)   0           stage3/block1/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block1/1x1_gconv_2/g1_sl (None, 14, 14, 40)   0           stage3/block1/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block1/1x1_gconv_2/g2_sl (None, 14, 14, 40)   0           stage3/block1/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block1/1x1_gconv_2_/g0 ( (None, 14, 14, 80)   3200        stage3/block1/1x1_gconv_2/g0_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block1/1x1_gconv_2_/g1 ( (None, 14, 14, 80)   3200        stage3/block1/1x1_gconv_2/g1_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block1/1x1_gconv_2_/g2 ( (None, 14, 14, 80)   3200        stage3/block1/1x1_gconv_2/g2_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block1/1x1_gconv_2/conca (None, 14, 14, 240)  0           stage3/block1/1x1_gconv_2_/g0[0][\n",
      "                                                                 stage3/block1/1x1_gconv_2_/g1[0][\n",
      "                                                                 stage3/block1/1x1_gconv_2_/g2[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block1/bn_gconv_2 (Batch (None, 14, 14, 240)  960         stage3/block1/1x1_gconv_2/concat[\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block1/avg_pool (Average (None, 14, 14, 240)  0           stage2/block4/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block1/concat (Concatena (None, 14, 14, 480)  0           stage3/block1/bn_gconv_2[0][0]   \n",
      "                                                                 stage3/block1/avg_pool[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block1/relu_out (Activat (None, 14, 14, 480)  0           stage3/block1/concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block2/1x1_gconv_1/g0_sl (None, 14, 14, 160)  0           stage3/block1/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block2/1x1_gconv_1/g1_sl (None, 14, 14, 160)  0           stage3/block1/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block2/1x1_gconv_1/g2_sl (None, 14, 14, 160)  0           stage3/block1/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block2/1x1_gconv_1_/g0 ( (None, 14, 14, 40)   6400        stage3/block2/1x1_gconv_1/g0_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block2/1x1_gconv_1_/g1 ( (None, 14, 14, 40)   6400        stage3/block2/1x1_gconv_1/g1_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block2/1x1_gconv_1_/g2 ( (None, 14, 14, 40)   6400        stage3/block2/1x1_gconv_1/g2_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block2/1x1_gconv_1/conca (None, 14, 14, 120)  0           stage3/block2/1x1_gconv_1_/g0[0][\n",
      "                                                                 stage3/block2/1x1_gconv_1_/g1[0][\n",
      "                                                                 stage3/block2/1x1_gconv_1_/g2[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block2/bn_gconv_1 (Batch (None, 14, 14, 120)  480         stage3/block2/1x1_gconv_1/concat[\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block2/relu_gconv_1 (Act (None, 14, 14, 120)  0           stage3/block2/bn_gconv_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block2/channel_shuffle ( (None, 14, 14, 120)  0           stage3/block2/relu_gconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block2/1x1_dwconv_1 (Dep (None, 14, 14, 120)  1080        stage3/block2/channel_shuffle[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block2/bn_dwconv_1 (Batc (None, 14, 14, 120)  480         stage3/block2/1x1_dwconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block2/1x1_gconv_2/g0_sl (None, 14, 14, 40)   0           stage3/block2/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block2/1x1_gconv_2/g1_sl (None, 14, 14, 40)   0           stage3/block2/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block2/1x1_gconv_2/g2_sl (None, 14, 14, 40)   0           stage3/block2/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block2/1x1_gconv_2_/g0 ( (None, 14, 14, 160)  6400        stage3/block2/1x1_gconv_2/g0_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block2/1x1_gconv_2_/g1 ( (None, 14, 14, 160)  6400        stage3/block2/1x1_gconv_2/g1_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block2/1x1_gconv_2_/g2 ( (None, 14, 14, 160)  6400        stage3/block2/1x1_gconv_2/g2_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block2/1x1_gconv_2/conca (None, 14, 14, 480)  0           stage3/block2/1x1_gconv_2_/g0[0][\n",
      "                                                                 stage3/block2/1x1_gconv_2_/g1[0][\n",
      "                                                                 stage3/block2/1x1_gconv_2_/g2[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block2/bn_gconv_2 (Batch (None, 14, 14, 480)  1920        stage3/block2/1x1_gconv_2/concat[\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block2/add (Add)         (None, 14, 14, 480)  0           stage3/block2/bn_gconv_2[0][0]   \n",
      "                                                                 stage3/block1/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block2/relu_out (Activat (None, 14, 14, 480)  0           stage3/block2/add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block3/1x1_gconv_1/g0_sl (None, 14, 14, 160)  0           stage3/block2/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block3/1x1_gconv_1/g1_sl (None, 14, 14, 160)  0           stage3/block2/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block3/1x1_gconv_1/g2_sl (None, 14, 14, 160)  0           stage3/block2/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block3/1x1_gconv_1_/g0 ( (None, 14, 14, 40)   6400        stage3/block3/1x1_gconv_1/g0_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block3/1x1_gconv_1_/g1 ( (None, 14, 14, 40)   6400        stage3/block3/1x1_gconv_1/g1_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block3/1x1_gconv_1_/g2 ( (None, 14, 14, 40)   6400        stage3/block3/1x1_gconv_1/g2_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block3/1x1_gconv_1/conca (None, 14, 14, 120)  0           stage3/block3/1x1_gconv_1_/g0[0][\n",
      "                                                                 stage3/block3/1x1_gconv_1_/g1[0][\n",
      "                                                                 stage3/block3/1x1_gconv_1_/g2[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block3/bn_gconv_1 (Batch (None, 14, 14, 120)  480         stage3/block3/1x1_gconv_1/concat[\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block3/relu_gconv_1 (Act (None, 14, 14, 120)  0           stage3/block3/bn_gconv_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block3/channel_shuffle ( (None, 14, 14, 120)  0           stage3/block3/relu_gconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block3/1x1_dwconv_1 (Dep (None, 14, 14, 120)  1080        stage3/block3/channel_shuffle[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block3/bn_dwconv_1 (Batc (None, 14, 14, 120)  480         stage3/block3/1x1_dwconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block3/1x1_gconv_2/g0_sl (None, 14, 14, 40)   0           stage3/block3/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block3/1x1_gconv_2/g1_sl (None, 14, 14, 40)   0           stage3/block3/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block3/1x1_gconv_2/g2_sl (None, 14, 14, 40)   0           stage3/block3/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block3/1x1_gconv_2_/g0 ( (None, 14, 14, 160)  6400        stage3/block3/1x1_gconv_2/g0_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block3/1x1_gconv_2_/g1 ( (None, 14, 14, 160)  6400        stage3/block3/1x1_gconv_2/g1_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block3/1x1_gconv_2_/g2 ( (None, 14, 14, 160)  6400        stage3/block3/1x1_gconv_2/g2_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block3/1x1_gconv_2/conca (None, 14, 14, 480)  0           stage3/block3/1x1_gconv_2_/g0[0][\n",
      "                                                                 stage3/block3/1x1_gconv_2_/g1[0][\n",
      "                                                                 stage3/block3/1x1_gconv_2_/g2[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block3/bn_gconv_2 (Batch (None, 14, 14, 480)  1920        stage3/block3/1x1_gconv_2/concat[\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block3/add (Add)         (None, 14, 14, 480)  0           stage3/block3/bn_gconv_2[0][0]   \n",
      "                                                                 stage3/block2/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block3/relu_out (Activat (None, 14, 14, 480)  0           stage3/block3/add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block4/1x1_gconv_1/g0_sl (None, 14, 14, 160)  0           stage3/block3/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block4/1x1_gconv_1/g1_sl (None, 14, 14, 160)  0           stage3/block3/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block4/1x1_gconv_1/g2_sl (None, 14, 14, 160)  0           stage3/block3/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block4/1x1_gconv_1_/g0 ( (None, 14, 14, 40)   6400        stage3/block4/1x1_gconv_1/g0_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block4/1x1_gconv_1_/g1 ( (None, 14, 14, 40)   6400        stage3/block4/1x1_gconv_1/g1_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block4/1x1_gconv_1_/g2 ( (None, 14, 14, 40)   6400        stage3/block4/1x1_gconv_1/g2_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block4/1x1_gconv_1/conca (None, 14, 14, 120)  0           stage3/block4/1x1_gconv_1_/g0[0][\n",
      "                                                                 stage3/block4/1x1_gconv_1_/g1[0][\n",
      "                                                                 stage3/block4/1x1_gconv_1_/g2[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block4/bn_gconv_1 (Batch (None, 14, 14, 120)  480         stage3/block4/1x1_gconv_1/concat[\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block4/relu_gconv_1 (Act (None, 14, 14, 120)  0           stage3/block4/bn_gconv_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block4/channel_shuffle ( (None, 14, 14, 120)  0           stage3/block4/relu_gconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block4/1x1_dwconv_1 (Dep (None, 14, 14, 120)  1080        stage3/block4/channel_shuffle[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block4/bn_dwconv_1 (Batc (None, 14, 14, 120)  480         stage3/block4/1x1_dwconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block4/1x1_gconv_2/g0_sl (None, 14, 14, 40)   0           stage3/block4/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block4/1x1_gconv_2/g1_sl (None, 14, 14, 40)   0           stage3/block4/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block4/1x1_gconv_2/g2_sl (None, 14, 14, 40)   0           stage3/block4/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block4/1x1_gconv_2_/g0 ( (None, 14, 14, 160)  6400        stage3/block4/1x1_gconv_2/g0_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block4/1x1_gconv_2_/g1 ( (None, 14, 14, 160)  6400        stage3/block4/1x1_gconv_2/g1_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block4/1x1_gconv_2_/g2 ( (None, 14, 14, 160)  6400        stage3/block4/1x1_gconv_2/g2_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block4/1x1_gconv_2/conca (None, 14, 14, 480)  0           stage3/block4/1x1_gconv_2_/g0[0][\n",
      "                                                                 stage3/block4/1x1_gconv_2_/g1[0][\n",
      "                                                                 stage3/block4/1x1_gconv_2_/g2[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block4/bn_gconv_2 (Batch (None, 14, 14, 480)  1920        stage3/block4/1x1_gconv_2/concat[\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block4/add (Add)         (None, 14, 14, 480)  0           stage3/block4/bn_gconv_2[0][0]   \n",
      "                                                                 stage3/block3/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block4/relu_out (Activat (None, 14, 14, 480)  0           stage3/block4/add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block5/1x1_gconv_1/g0_sl (None, 14, 14, 160)  0           stage3/block4/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block5/1x1_gconv_1/g1_sl (None, 14, 14, 160)  0           stage3/block4/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block5/1x1_gconv_1/g2_sl (None, 14, 14, 160)  0           stage3/block4/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block5/1x1_gconv_1_/g0 ( (None, 14, 14, 40)   6400        stage3/block5/1x1_gconv_1/g0_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block5/1x1_gconv_1_/g1 ( (None, 14, 14, 40)   6400        stage3/block5/1x1_gconv_1/g1_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block5/1x1_gconv_1_/g2 ( (None, 14, 14, 40)   6400        stage3/block5/1x1_gconv_1/g2_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block5/1x1_gconv_1/conca (None, 14, 14, 120)  0           stage3/block5/1x1_gconv_1_/g0[0][\n",
      "                                                                 stage3/block5/1x1_gconv_1_/g1[0][\n",
      "                                                                 stage3/block5/1x1_gconv_1_/g2[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block5/bn_gconv_1 (Batch (None, 14, 14, 120)  480         stage3/block5/1x1_gconv_1/concat[\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block5/relu_gconv_1 (Act (None, 14, 14, 120)  0           stage3/block5/bn_gconv_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block5/channel_shuffle ( (None, 14, 14, 120)  0           stage3/block5/relu_gconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block5/1x1_dwconv_1 (Dep (None, 14, 14, 120)  1080        stage3/block5/channel_shuffle[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block5/bn_dwconv_1 (Batc (None, 14, 14, 120)  480         stage3/block5/1x1_dwconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block5/1x1_gconv_2/g0_sl (None, 14, 14, 40)   0           stage3/block5/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block5/1x1_gconv_2/g1_sl (None, 14, 14, 40)   0           stage3/block5/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block5/1x1_gconv_2/g2_sl (None, 14, 14, 40)   0           stage3/block5/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block5/1x1_gconv_2_/g0 ( (None, 14, 14, 160)  6400        stage3/block5/1x1_gconv_2/g0_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block5/1x1_gconv_2_/g1 ( (None, 14, 14, 160)  6400        stage3/block5/1x1_gconv_2/g1_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block5/1x1_gconv_2_/g2 ( (None, 14, 14, 160)  6400        stage3/block5/1x1_gconv_2/g2_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block5/1x1_gconv_2/conca (None, 14, 14, 480)  0           stage3/block5/1x1_gconv_2_/g0[0][\n",
      "                                                                 stage3/block5/1x1_gconv_2_/g1[0][\n",
      "                                                                 stage3/block5/1x1_gconv_2_/g2[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block5/bn_gconv_2 (Batch (None, 14, 14, 480)  1920        stage3/block5/1x1_gconv_2/concat[\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block5/add (Add)         (None, 14, 14, 480)  0           stage3/block5/bn_gconv_2[0][0]   \n",
      "                                                                 stage3/block4/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block5/relu_out (Activat (None, 14, 14, 480)  0           stage3/block5/add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block6/1x1_gconv_1/g0_sl (None, 14, 14, 160)  0           stage3/block5/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block6/1x1_gconv_1/g1_sl (None, 14, 14, 160)  0           stage3/block5/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block6/1x1_gconv_1/g2_sl (None, 14, 14, 160)  0           stage3/block5/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block6/1x1_gconv_1_/g0 ( (None, 14, 14, 40)   6400        stage3/block6/1x1_gconv_1/g0_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block6/1x1_gconv_1_/g1 ( (None, 14, 14, 40)   6400        stage3/block6/1x1_gconv_1/g1_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block6/1x1_gconv_1_/g2 ( (None, 14, 14, 40)   6400        stage3/block6/1x1_gconv_1/g2_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block6/1x1_gconv_1/conca (None, 14, 14, 120)  0           stage3/block6/1x1_gconv_1_/g0[0][\n",
      "                                                                 stage3/block6/1x1_gconv_1_/g1[0][\n",
      "                                                                 stage3/block6/1x1_gconv_1_/g2[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block6/bn_gconv_1 (Batch (None, 14, 14, 120)  480         stage3/block6/1x1_gconv_1/concat[\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block6/relu_gconv_1 (Act (None, 14, 14, 120)  0           stage3/block6/bn_gconv_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block6/channel_shuffle ( (None, 14, 14, 120)  0           stage3/block6/relu_gconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block6/1x1_dwconv_1 (Dep (None, 14, 14, 120)  1080        stage3/block6/channel_shuffle[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block6/bn_dwconv_1 (Batc (None, 14, 14, 120)  480         stage3/block6/1x1_dwconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block6/1x1_gconv_2/g0_sl (None, 14, 14, 40)   0           stage3/block6/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block6/1x1_gconv_2/g1_sl (None, 14, 14, 40)   0           stage3/block6/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block6/1x1_gconv_2/g2_sl (None, 14, 14, 40)   0           stage3/block6/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block6/1x1_gconv_2_/g0 ( (None, 14, 14, 160)  6400        stage3/block6/1x1_gconv_2/g0_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block6/1x1_gconv_2_/g1 ( (None, 14, 14, 160)  6400        stage3/block6/1x1_gconv_2/g1_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block6/1x1_gconv_2_/g2 ( (None, 14, 14, 160)  6400        stage3/block6/1x1_gconv_2/g2_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block6/1x1_gconv_2/conca (None, 14, 14, 480)  0           stage3/block6/1x1_gconv_2_/g0[0][\n",
      "                                                                 stage3/block6/1x1_gconv_2_/g1[0][\n",
      "                                                                 stage3/block6/1x1_gconv_2_/g2[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block6/bn_gconv_2 (Batch (None, 14, 14, 480)  1920        stage3/block6/1x1_gconv_2/concat[\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block6/add (Add)         (None, 14, 14, 480)  0           stage3/block6/bn_gconv_2[0][0]   \n",
      "                                                                 stage3/block5/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block6/relu_out (Activat (None, 14, 14, 480)  0           stage3/block6/add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block7/1x1_gconv_1/g0_sl (None, 14, 14, 160)  0           stage3/block6/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block7/1x1_gconv_1/g1_sl (None, 14, 14, 160)  0           stage3/block6/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block7/1x1_gconv_1/g2_sl (None, 14, 14, 160)  0           stage3/block6/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block7/1x1_gconv_1_/g0 ( (None, 14, 14, 40)   6400        stage3/block7/1x1_gconv_1/g0_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block7/1x1_gconv_1_/g1 ( (None, 14, 14, 40)   6400        stage3/block7/1x1_gconv_1/g1_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block7/1x1_gconv_1_/g2 ( (None, 14, 14, 40)   6400        stage3/block7/1x1_gconv_1/g2_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block7/1x1_gconv_1/conca (None, 14, 14, 120)  0           stage3/block7/1x1_gconv_1_/g0[0][\n",
      "                                                                 stage3/block7/1x1_gconv_1_/g1[0][\n",
      "                                                                 stage3/block7/1x1_gconv_1_/g2[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block7/bn_gconv_1 (Batch (None, 14, 14, 120)  480         stage3/block7/1x1_gconv_1/concat[\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block7/relu_gconv_1 (Act (None, 14, 14, 120)  0           stage3/block7/bn_gconv_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block7/channel_shuffle ( (None, 14, 14, 120)  0           stage3/block7/relu_gconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block7/1x1_dwconv_1 (Dep (None, 14, 14, 120)  1080        stage3/block7/channel_shuffle[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block7/bn_dwconv_1 (Batc (None, 14, 14, 120)  480         stage3/block7/1x1_dwconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block7/1x1_gconv_2/g0_sl (None, 14, 14, 40)   0           stage3/block7/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block7/1x1_gconv_2/g1_sl (None, 14, 14, 40)   0           stage3/block7/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block7/1x1_gconv_2/g2_sl (None, 14, 14, 40)   0           stage3/block7/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block7/1x1_gconv_2_/g0 ( (None, 14, 14, 160)  6400        stage3/block7/1x1_gconv_2/g0_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block7/1x1_gconv_2_/g1 ( (None, 14, 14, 160)  6400        stage3/block7/1x1_gconv_2/g1_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block7/1x1_gconv_2_/g2 ( (None, 14, 14, 160)  6400        stage3/block7/1x1_gconv_2/g2_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block7/1x1_gconv_2/conca (None, 14, 14, 480)  0           stage3/block7/1x1_gconv_2_/g0[0][\n",
      "                                                                 stage3/block7/1x1_gconv_2_/g1[0][\n",
      "                                                                 stage3/block7/1x1_gconv_2_/g2[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block7/bn_gconv_2 (Batch (None, 14, 14, 480)  1920        stage3/block7/1x1_gconv_2/concat[\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block7/add (Add)         (None, 14, 14, 480)  0           stage3/block7/bn_gconv_2[0][0]   \n",
      "                                                                 stage3/block6/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block7/relu_out (Activat (None, 14, 14, 480)  0           stage3/block7/add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block8/1x1_gconv_1/g0_sl (None, 14, 14, 160)  0           stage3/block7/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block8/1x1_gconv_1/g1_sl (None, 14, 14, 160)  0           stage3/block7/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block8/1x1_gconv_1/g2_sl (None, 14, 14, 160)  0           stage3/block7/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block8/1x1_gconv_1_/g0 ( (None, 14, 14, 40)   6400        stage3/block8/1x1_gconv_1/g0_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block8/1x1_gconv_1_/g1 ( (None, 14, 14, 40)   6400        stage3/block8/1x1_gconv_1/g1_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block8/1x1_gconv_1_/g2 ( (None, 14, 14, 40)   6400        stage3/block8/1x1_gconv_1/g2_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block8/1x1_gconv_1/conca (None, 14, 14, 120)  0           stage3/block8/1x1_gconv_1_/g0[0][\n",
      "                                                                 stage3/block8/1x1_gconv_1_/g1[0][\n",
      "                                                                 stage3/block8/1x1_gconv_1_/g2[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block8/bn_gconv_1 (Batch (None, 14, 14, 120)  480         stage3/block8/1x1_gconv_1/concat[\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block8/relu_gconv_1 (Act (None, 14, 14, 120)  0           stage3/block8/bn_gconv_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block8/channel_shuffle ( (None, 14, 14, 120)  0           stage3/block8/relu_gconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block8/1x1_dwconv_1 (Dep (None, 14, 14, 120)  1080        stage3/block8/channel_shuffle[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block8/bn_dwconv_1 (Batc (None, 14, 14, 120)  480         stage3/block8/1x1_dwconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block8/1x1_gconv_2/g0_sl (None, 14, 14, 40)   0           stage3/block8/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block8/1x1_gconv_2/g1_sl (None, 14, 14, 40)   0           stage3/block8/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block8/1x1_gconv_2/g2_sl (None, 14, 14, 40)   0           stage3/block8/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block8/1x1_gconv_2_/g0 ( (None, 14, 14, 160)  6400        stage3/block8/1x1_gconv_2/g0_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block8/1x1_gconv_2_/g1 ( (None, 14, 14, 160)  6400        stage3/block8/1x1_gconv_2/g1_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block8/1x1_gconv_2_/g2 ( (None, 14, 14, 160)  6400        stage3/block8/1x1_gconv_2/g2_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block8/1x1_gconv_2/conca (None, 14, 14, 480)  0           stage3/block8/1x1_gconv_2_/g0[0][\n",
      "                                                                 stage3/block8/1x1_gconv_2_/g1[0][\n",
      "                                                                 stage3/block8/1x1_gconv_2_/g2[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block8/bn_gconv_2 (Batch (None, 14, 14, 480)  1920        stage3/block8/1x1_gconv_2/concat[\n",
      "__________________________________________________________________________________________________\n",
      "stage3/block8/add (Add)         (None, 14, 14, 480)  0           stage3/block8/bn_gconv_2[0][0]   \n",
      "                                                                 stage3/block7/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage3/block8/relu_out (Activat (None, 14, 14, 480)  0           stage3/block8/add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block1/1x1_gconv_1/g0_sl (None, 14, 14, 160)  0           stage3/block8/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block1/1x1_gconv_1/g1_sl (None, 14, 14, 160)  0           stage3/block8/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block1/1x1_gconv_1/g2_sl (None, 14, 14, 160)  0           stage3/block8/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block1/1x1_gconv_1_/g0 ( (None, 14, 14, 80)   12800       stage4/block1/1x1_gconv_1/g0_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block1/1x1_gconv_1_/g1 ( (None, 14, 14, 80)   12800       stage4/block1/1x1_gconv_1/g1_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block1/1x1_gconv_1_/g2 ( (None, 14, 14, 80)   12800       stage4/block1/1x1_gconv_1/g2_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block1/1x1_gconv_1/conca (None, 14, 14, 240)  0           stage4/block1/1x1_gconv_1_/g0[0][\n",
      "                                                                 stage4/block1/1x1_gconv_1_/g1[0][\n",
      "                                                                 stage4/block1/1x1_gconv_1_/g2[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block1/bn_gconv_1 (Batch (None, 14, 14, 240)  960         stage4/block1/1x1_gconv_1/concat[\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block1/relu_gconv_1 (Act (None, 14, 14, 240)  0           stage4/block1/bn_gconv_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block1/channel_shuffle ( (None, 14, 14, 240)  0           stage4/block1/relu_gconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block1/1x1_dwconv_1 (Dep (None, 7, 7, 240)    2160        stage4/block1/channel_shuffle[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block1/bn_dwconv_1 (Batc (None, 7, 7, 240)    960         stage4/block1/1x1_dwconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block1/1x1_gconv_2/g0_sl (None, 7, 7, 80)     0           stage4/block1/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block1/1x1_gconv_2/g1_sl (None, 7, 7, 80)     0           stage4/block1/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block1/1x1_gconv_2/g2_sl (None, 7, 7, 80)     0           stage4/block1/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block1/1x1_gconv_2_/g0 ( (None, 7, 7, 160)    12800       stage4/block1/1x1_gconv_2/g0_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block1/1x1_gconv_2_/g1 ( (None, 7, 7, 160)    12800       stage4/block1/1x1_gconv_2/g1_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block1/1x1_gconv_2_/g2 ( (None, 7, 7, 160)    12800       stage4/block1/1x1_gconv_2/g2_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block1/1x1_gconv_2/conca (None, 7, 7, 480)    0           stage4/block1/1x1_gconv_2_/g0[0][\n",
      "                                                                 stage4/block1/1x1_gconv_2_/g1[0][\n",
      "                                                                 stage4/block1/1x1_gconv_2_/g2[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block1/bn_gconv_2 (Batch (None, 7, 7, 480)    1920        stage4/block1/1x1_gconv_2/concat[\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block1/avg_pool (Average (None, 7, 7, 480)    0           stage3/block8/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block1/concat (Concatena (None, 7, 7, 960)    0           stage4/block1/bn_gconv_2[0][0]   \n",
      "                                                                 stage4/block1/avg_pool[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block1/relu_out (Activat (None, 7, 7, 960)    0           stage4/block1/concat[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block2/1x1_gconv_1/g0_sl (None, 7, 7, 320)    0           stage4/block1/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block2/1x1_gconv_1/g1_sl (None, 7, 7, 320)    0           stage4/block1/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block2/1x1_gconv_1/g2_sl (None, 7, 7, 320)    0           stage4/block1/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block2/1x1_gconv_1_/g0 ( (None, 7, 7, 80)     25600       stage4/block2/1x1_gconv_1/g0_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block2/1x1_gconv_1_/g1 ( (None, 7, 7, 80)     25600       stage4/block2/1x1_gconv_1/g1_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block2/1x1_gconv_1_/g2 ( (None, 7, 7, 80)     25600       stage4/block2/1x1_gconv_1/g2_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block2/1x1_gconv_1/conca (None, 7, 7, 240)    0           stage4/block2/1x1_gconv_1_/g0[0][\n",
      "                                                                 stage4/block2/1x1_gconv_1_/g1[0][\n",
      "                                                                 stage4/block2/1x1_gconv_1_/g2[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block2/bn_gconv_1 (Batch (None, 7, 7, 240)    960         stage4/block2/1x1_gconv_1/concat[\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block2/relu_gconv_1 (Act (None, 7, 7, 240)    0           stage4/block2/bn_gconv_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block2/channel_shuffle ( (None, 7, 7, 240)    0           stage4/block2/relu_gconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block2/1x1_dwconv_1 (Dep (None, 7, 7, 240)    2160        stage4/block2/channel_shuffle[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block2/bn_dwconv_1 (Batc (None, 7, 7, 240)    960         stage4/block2/1x1_dwconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block2/1x1_gconv_2/g0_sl (None, 7, 7, 80)     0           stage4/block2/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block2/1x1_gconv_2/g1_sl (None, 7, 7, 80)     0           stage4/block2/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block2/1x1_gconv_2/g2_sl (None, 7, 7, 80)     0           stage4/block2/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block2/1x1_gconv_2_/g0 ( (None, 7, 7, 320)    25600       stage4/block2/1x1_gconv_2/g0_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block2/1x1_gconv_2_/g1 ( (None, 7, 7, 320)    25600       stage4/block2/1x1_gconv_2/g1_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block2/1x1_gconv_2_/g2 ( (None, 7, 7, 320)    25600       stage4/block2/1x1_gconv_2/g2_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block2/1x1_gconv_2/conca (None, 7, 7, 960)    0           stage4/block2/1x1_gconv_2_/g0[0][\n",
      "                                                                 stage4/block2/1x1_gconv_2_/g1[0][\n",
      "                                                                 stage4/block2/1x1_gconv_2_/g2[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block2/bn_gconv_2 (Batch (None, 7, 7, 960)    3840        stage4/block2/1x1_gconv_2/concat[\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block2/add (Add)         (None, 7, 7, 960)    0           stage4/block2/bn_gconv_2[0][0]   \n",
      "                                                                 stage4/block1/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block2/relu_out (Activat (None, 7, 7, 960)    0           stage4/block2/add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block3/1x1_gconv_1/g0_sl (None, 7, 7, 320)    0           stage4/block2/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block3/1x1_gconv_1/g1_sl (None, 7, 7, 320)    0           stage4/block2/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block3/1x1_gconv_1/g2_sl (None, 7, 7, 320)    0           stage4/block2/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block3/1x1_gconv_1_/g0 ( (None, 7, 7, 80)     25600       stage4/block3/1x1_gconv_1/g0_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block3/1x1_gconv_1_/g1 ( (None, 7, 7, 80)     25600       stage4/block3/1x1_gconv_1/g1_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block3/1x1_gconv_1_/g2 ( (None, 7, 7, 80)     25600       stage4/block3/1x1_gconv_1/g2_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block3/1x1_gconv_1/conca (None, 7, 7, 240)    0           stage4/block3/1x1_gconv_1_/g0[0][\n",
      "                                                                 stage4/block3/1x1_gconv_1_/g1[0][\n",
      "                                                                 stage4/block3/1x1_gconv_1_/g2[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block3/bn_gconv_1 (Batch (None, 7, 7, 240)    960         stage4/block3/1x1_gconv_1/concat[\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block3/relu_gconv_1 (Act (None, 7, 7, 240)    0           stage4/block3/bn_gconv_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block3/channel_shuffle ( (None, 7, 7, 240)    0           stage4/block3/relu_gconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block3/1x1_dwconv_1 (Dep (None, 7, 7, 240)    2160        stage4/block3/channel_shuffle[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block3/bn_dwconv_1 (Batc (None, 7, 7, 240)    960         stage4/block3/1x1_dwconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block3/1x1_gconv_2/g0_sl (None, 7, 7, 80)     0           stage4/block3/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block3/1x1_gconv_2/g1_sl (None, 7, 7, 80)     0           stage4/block3/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block3/1x1_gconv_2/g2_sl (None, 7, 7, 80)     0           stage4/block3/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block3/1x1_gconv_2_/g0 ( (None, 7, 7, 320)    25600       stage4/block3/1x1_gconv_2/g0_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block3/1x1_gconv_2_/g1 ( (None, 7, 7, 320)    25600       stage4/block3/1x1_gconv_2/g1_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block3/1x1_gconv_2_/g2 ( (None, 7, 7, 320)    25600       stage4/block3/1x1_gconv_2/g2_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block3/1x1_gconv_2/conca (None, 7, 7, 960)    0           stage4/block3/1x1_gconv_2_/g0[0][\n",
      "                                                                 stage4/block3/1x1_gconv_2_/g1[0][\n",
      "                                                                 stage4/block3/1x1_gconv_2_/g2[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block3/bn_gconv_2 (Batch (None, 7, 7, 960)    3840        stage4/block3/1x1_gconv_2/concat[\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block3/add (Add)         (None, 7, 7, 960)    0           stage4/block3/bn_gconv_2[0][0]   \n",
      "                                                                 stage4/block2/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block3/relu_out (Activat (None, 7, 7, 960)    0           stage4/block3/add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block4/1x1_gconv_1/g0_sl (None, 7, 7, 320)    0           stage4/block3/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block4/1x1_gconv_1/g1_sl (None, 7, 7, 320)    0           stage4/block3/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block4/1x1_gconv_1/g2_sl (None, 7, 7, 320)    0           stage4/block3/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block4/1x1_gconv_1_/g0 ( (None, 7, 7, 80)     25600       stage4/block4/1x1_gconv_1/g0_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block4/1x1_gconv_1_/g1 ( (None, 7, 7, 80)     25600       stage4/block4/1x1_gconv_1/g1_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block4/1x1_gconv_1_/g2 ( (None, 7, 7, 80)     25600       stage4/block4/1x1_gconv_1/g2_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block4/1x1_gconv_1/conca (None, 7, 7, 240)    0           stage4/block4/1x1_gconv_1_/g0[0][\n",
      "                                                                 stage4/block4/1x1_gconv_1_/g1[0][\n",
      "                                                                 stage4/block4/1x1_gconv_1_/g2[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block4/bn_gconv_1 (Batch (None, 7, 7, 240)    960         stage4/block4/1x1_gconv_1/concat[\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block4/relu_gconv_1 (Act (None, 7, 7, 240)    0           stage4/block4/bn_gconv_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block4/channel_shuffle ( (None, 7, 7, 240)    0           stage4/block4/relu_gconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block4/1x1_dwconv_1 (Dep (None, 7, 7, 240)    2160        stage4/block4/channel_shuffle[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block4/bn_dwconv_1 (Batc (None, 7, 7, 240)    960         stage4/block4/1x1_dwconv_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block4/1x1_gconv_2/g0_sl (None, 7, 7, 80)     0           stage4/block4/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block4/1x1_gconv_2/g1_sl (None, 7, 7, 80)     0           stage4/block4/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block4/1x1_gconv_2/g2_sl (None, 7, 7, 80)     0           stage4/block4/bn_dwconv_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block4/1x1_gconv_2_/g0 ( (None, 7, 7, 320)    25600       stage4/block4/1x1_gconv_2/g0_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block4/1x1_gconv_2_/g1 ( (None, 7, 7, 320)    25600       stage4/block4/1x1_gconv_2/g1_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block4/1x1_gconv_2_/g2 ( (None, 7, 7, 320)    25600       stage4/block4/1x1_gconv_2/g2_slic\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block4/1x1_gconv_2/conca (None, 7, 7, 960)    0           stage4/block4/1x1_gconv_2_/g0[0][\n",
      "                                                                 stage4/block4/1x1_gconv_2_/g1[0][\n",
      "                                                                 stage4/block4/1x1_gconv_2_/g2[0][\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block4/bn_gconv_2 (Batch (None, 7, 7, 960)    3840        stage4/block4/1x1_gconv_2/concat[\n",
      "__________________________________________________________________________________________________\n",
      "stage4/block4/add (Add)         (None, 7, 7, 960)    0           stage4/block4/bn_gconv_2[0][0]   \n",
      "                                                                 stage4/block3/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "stage4/block4/relu_out (Activat (None, 7, 7, 960)    0           stage4/block4/add[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_pool (GlobalMaxPooling2D (None, 960)          0           stage4/block4/relu_out[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "fc (Dense)                      (None, 5)            4805        global_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Activation)            (None, 5)            0           fc[0][0]                         \n",
      "==================================================================================================\n",
      "Total params: 942,557\n",
      "Trainable params: 918,125\n",
      "Non-trainable params: 24,432\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "xception_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "norman-detector",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-30T18:40:08.391663Z",
     "iopub.status.busy": "2021-05-30T18:40:08.385701Z",
     "iopub.status.idle": "2021-05-30T18:40:08.398432Z",
     "shell.execute_reply": "2021-05-30T18:40:08.397997Z",
     "shell.execute_reply.started": "2021-05-30T09:28:58.465217Z"
    },
    "id": "norman-detector",
    "papermill": {
     "duration": 0.226948,
     "end_time": "2021-05-30T18:40:08.398547",
     "exception": false,
     "start_time": "2021-05-30T18:40:08.171599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "xception_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001,decay=0.0001),\n",
    "                 metrics=[\"acc\"],\n",
    "                 loss= tf.keras.losses.sparse_categorical_crossentropy)\n",
    "\n",
    "checkpoint_path = \"xception_best.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "my_callbacks = [\n",
    "               ModelCheckpoint(\n",
    "                   checkpoint_path,\n",
    "                   monitor = 'val_acc',\n",
    "                   verbose = 1,\n",
    "                   save_weights_only=True,\n",
    "                   save_best_only = True,\n",
    "                   mode=\"max\"\n",
    "               ),\n",
    "               EarlyStopping(\n",
    "                   monitor='val_loss', \n",
    "                   patience=10,\n",
    "                   verbose=0\n",
    "               ),\n",
    "               ReduceLROnPlateau(\n",
    "                   monitor='val_loss', \n",
    "                   patience=10,\n",
    "                   verbose=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-administration",
   "metadata": {
    "id": "crucial-administration",
    "papermill": {
     "duration": 0.038495,
     "end_time": "2021-05-30T18:40:08.475329",
     "exception": false,
     "start_time": "2021-05-30T18:40:08.436834",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 가중치 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "regional-indie",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-30T18:40:08.558435Z",
     "iopub.status.busy": "2021-05-30T18:40:08.557616Z",
     "iopub.status.idle": "2021-05-30T18:40:09.060821Z",
     "shell.execute_reply": "2021-05-30T18:40:09.060357Z",
     "shell.execute_reply.started": "2021-05-30T09:30:11.007618Z"
    },
    "id": "regional-indie",
    "papermill": {
     "duration": 0.546357,
     "end_time": "2021-05-30T18:40:09.060960",
     "exception": false,
     "start_time": "2021-05-30T18:40:08.514603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    'balanced',\n",
    "    classes= np.unique(df_train_kaggle.label.values),\n",
    "    y= df_train_kaggle.label.values\n",
    ")\n",
    "\n",
    "class_weights = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-better",
   "metadata": {
    "id": "square-better",
    "papermill": {
     "duration": 0.037503,
     "end_time": "2021-05-30T18:40:09.136492",
     "exception": false,
     "start_time": "2021-05-30T18:40:09.098989",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "hidden-stephen",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-30T18:40:09.218272Z",
     "iopub.status.busy": "2021-05-30T18:40:09.217728Z",
     "iopub.status.idle": "2021-05-30T20:12:12.156502Z",
     "shell.execute_reply": "2021-05-30T20:12:12.154537Z",
     "shell.execute_reply.started": "2021-05-26T18:49:57.904339Z"
    },
    "id": "hidden-stephen",
    "outputId": "e45b2b3b-c166-43eb-962b-f6c613220558",
    "papermill": {
     "duration": 5522.981999,
     "end_time": "2021-05-30T20:12:12.156678",
     "exception": false,
     "start_time": "2021-05-30T18:40:09.174679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "979/979 [==============================] - 76s 65ms/step - loss: 3.5303 - acc: 0.2466 - val_loss: 1.9988 - val_acc: 0.2376\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.23758, saving model to xception_best.ckpt\n",
      "Epoch 2/300\n",
      "979/979 [==============================] - 62s 63ms/step - loss: 1.9915 - acc: 0.2255 - val_loss: 2.0495 - val_acc: 0.2230\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.23758\n",
      "Epoch 3/300\n",
      "979/979 [==============================] - 62s 63ms/step - loss: 1.9019 - acc: 0.2378 - val_loss: 1.8417 - val_acc: 0.2733\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.23758 to 0.27335, saving model to xception_best.ckpt\n",
      "Epoch 4/300\n",
      "979/979 [==============================] - 62s 63ms/step - loss: 1.8334 - acc: 0.2662 - val_loss: 1.9341 - val_acc: 0.2557\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.27335\n",
      "Epoch 5/300\n",
      "979/979 [==============================] - 62s 63ms/step - loss: 1.8043 - acc: 0.2650 - val_loss: 1.7911 - val_acc: 0.2822\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.27335 to 0.28224, saving model to xception_best.ckpt\n",
      "Epoch 6/300\n",
      "979/979 [==============================] - 61s 62ms/step - loss: 1.6722 - acc: 0.2856 - val_loss: 1.7621 - val_acc: 0.3012\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.28224 to 0.30125, saving model to xception_best.ckpt\n",
      "Epoch 7/300\n",
      "979/979 [==============================] - 62s 63ms/step - loss: 1.6799 - acc: 0.2903 - val_loss: 1.7374 - val_acc: 0.2925\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.30125\n",
      "Epoch 8/300\n",
      "979/979 [==============================] - 59s 60ms/step - loss: 1.6395 - acc: 0.2962 - val_loss: 1.7091 - val_acc: 0.3163\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.30125 to 0.31627, saving model to xception_best.ckpt\n",
      "Epoch 9/300\n",
      "979/979 [==============================] - 59s 60ms/step - loss: 1.6186 - acc: 0.3057 - val_loss: 1.6762 - val_acc: 0.3187\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.31627 to 0.31872, saving model to xception_best.ckpt\n",
      "Epoch 10/300\n",
      "979/979 [==============================] - 59s 61ms/step - loss: 1.6007 - acc: 0.3192 - val_loss: 1.6812 - val_acc: 0.3191\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.31872 to 0.31913, saving model to xception_best.ckpt\n",
      "Epoch 11/300\n",
      "979/979 [==============================] - 58s 60ms/step - loss: 1.5531 - acc: 0.3179 - val_loss: 1.6312 - val_acc: 0.3255\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.31913 to 0.32546, saving model to xception_best.ckpt\n",
      "Epoch 12/300\n",
      "979/979 [==============================] - 59s 60ms/step - loss: 1.5556 - acc: 0.3147 - val_loss: 1.5962 - val_acc: 0.3420\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.32546 to 0.34202, saving model to xception_best.ckpt\n",
      "Epoch 13/300\n",
      "979/979 [==============================] - 57s 59ms/step - loss: 1.4914 - acc: 0.3399 - val_loss: 1.6447 - val_acc: 0.3164\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.34202\n",
      "Epoch 14/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.5088 - acc: 0.3283 - val_loss: 1.5774 - val_acc: 0.3478\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.34202 to 0.34784, saving model to xception_best.ckpt\n",
      "Epoch 15/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 1.4757 - acc: 0.3245 - val_loss: 1.5512 - val_acc: 0.3510\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.34784 to 0.35101, saving model to xception_best.ckpt\n",
      "Epoch 16/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.4663 - acc: 0.3414 - val_loss: 1.5274 - val_acc: 0.3692\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.35101 to 0.36920, saving model to xception_best.ckpt\n",
      "Epoch 17/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.4251 - acc: 0.3556 - val_loss: 1.6078 - val_acc: 0.3321\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.36920\n",
      "Epoch 18/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 1.4062 - acc: 0.3563 - val_loss: 1.5315 - val_acc: 0.3595\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.36920\n",
      "Epoch 19/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.3963 - acc: 0.3601 - val_loss: 1.5209 - val_acc: 0.3657\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.36920\n",
      "Epoch 20/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.3987 - acc: 0.3645 - val_loss: 1.4944 - val_acc: 0.3696\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.36920 to 0.36961, saving model to xception_best.ckpt\n",
      "Epoch 21/300\n",
      "979/979 [==============================] - 57s 59ms/step - loss: 1.3888 - acc: 0.3582 - val_loss: 1.4831 - val_acc: 0.3838\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.36961 to 0.38381, saving model to xception_best.ckpt\n",
      "Epoch 22/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 1.3901 - acc: 0.3756 - val_loss: 1.4482 - val_acc: 0.3954\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.38381 to 0.39536, saving model to xception_best.ckpt\n",
      "Epoch 23/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.3571 - acc: 0.3680 - val_loss: 1.5401 - val_acc: 0.3604\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.39536\n",
      "Epoch 24/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.3543 - acc: 0.3637 - val_loss: 1.4617 - val_acc: 0.3832\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.39536\n",
      "Epoch 25/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 1.3237 - acc: 0.3952 - val_loss: 1.4797 - val_acc: 0.3865\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.39536\n",
      "Epoch 26/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.3509 - acc: 0.3747 - val_loss: 1.4178 - val_acc: 0.4069\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.39536 to 0.40691, saving model to xception_best.ckpt\n",
      "Epoch 27/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.3210 - acc: 0.3899 - val_loss: 1.4362 - val_acc: 0.3948\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.40691\n",
      "Epoch 28/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 1.3103 - acc: 0.4038 - val_loss: 1.4115 - val_acc: 0.4087\n",
      "\n",
      "Epoch 00028: val_acc improved from 0.40691 to 0.40875, saving model to xception_best.ckpt\n",
      "Epoch 29/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.2814 - acc: 0.3961 - val_loss: 1.3997 - val_acc: 0.4091\n",
      "\n",
      "Epoch 00029: val_acc improved from 0.40875 to 0.40905, saving model to xception_best.ckpt\n",
      "Epoch 30/300\n",
      "979/979 [==============================] - 57s 59ms/step - loss: 1.2915 - acc: 0.3975 - val_loss: 1.4305 - val_acc: 0.4055\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.40905\n",
      "Epoch 31/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.2587 - acc: 0.4053 - val_loss: 1.4225 - val_acc: 0.4060\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.40905\n",
      "Epoch 32/300\n",
      "979/979 [==============================] - 57s 59ms/step - loss: 1.2629 - acc: 0.3977 - val_loss: 1.4082 - val_acc: 0.4160\n",
      "\n",
      "Epoch 00032: val_acc improved from 0.40905 to 0.41600, saving model to xception_best.ckpt\n",
      "Epoch 33/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.2568 - acc: 0.4120 - val_loss: 1.3995 - val_acc: 0.4165\n",
      "\n",
      "Epoch 00033: val_acc improved from 0.41600 to 0.41651, saving model to xception_best.ckpt\n",
      "Epoch 34/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.2457 - acc: 0.4062 - val_loss: 1.4051 - val_acc: 0.4130\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.41651\n",
      "Epoch 35/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 1.2569 - acc: 0.4018 - val_loss: 1.3549 - val_acc: 0.4263\n",
      "\n",
      "Epoch 00035: val_acc improved from 0.41651 to 0.42632, saving model to xception_best.ckpt\n",
      "Epoch 36/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.2326 - acc: 0.4244 - val_loss: 1.3752 - val_acc: 0.4218\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.42632\n",
      "Epoch 37/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.2415 - acc: 0.4211 - val_loss: 1.3566 - val_acc: 0.4259\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.42632\n",
      "Epoch 38/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.2016 - acc: 0.4318 - val_loss: 1.3778 - val_acc: 0.4249\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.42632\n",
      "Epoch 39/300\n",
      "979/979 [==============================] - 57s 59ms/step - loss: 1.2389 - acc: 0.4048 - val_loss: 1.3254 - val_acc: 0.4374\n",
      "\n",
      "Epoch 00039: val_acc improved from 0.42632 to 0.43736, saving model to xception_best.ckpt\n",
      "Epoch 40/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 1.2145 - acc: 0.4288 - val_loss: 1.3643 - val_acc: 0.4293\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.43736\n",
      "Epoch 41/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.1764 - acc: 0.4242 - val_loss: 1.3667 - val_acc: 0.4246\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.43736\n",
      "Epoch 42/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.2013 - acc: 0.4291 - val_loss: 1.3512 - val_acc: 0.4275\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.43736\n",
      "Epoch 43/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.1681 - acc: 0.4324 - val_loss: 1.3601 - val_acc: 0.4263\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.43736\n",
      "Epoch 44/300\n",
      "979/979 [==============================] - 57s 59ms/step - loss: 1.1862 - acc: 0.4265 - val_loss: 1.3355 - val_acc: 0.4349\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.43736\n",
      "Epoch 45/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.1819 - acc: 0.4368 - val_loss: 1.3351 - val_acc: 0.4399\n",
      "\n",
      "Epoch 00045: val_acc improved from 0.43736 to 0.43991, saving model to xception_best.ckpt\n",
      "Epoch 46/300\n",
      "979/979 [==============================] - 57s 59ms/step - loss: 1.1688 - acc: 0.4462 - val_loss: 1.3142 - val_acc: 0.4512\n",
      "\n",
      "Epoch 00046: val_acc improved from 0.43991 to 0.45115, saving model to xception_best.ckpt\n",
      "Epoch 47/300\n",
      "979/979 [==============================] - 57s 59ms/step - loss: 1.1617 - acc: 0.4429 - val_loss: 1.3506 - val_acc: 0.4300\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.45115\n",
      "Epoch 48/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.1493 - acc: 0.4414 - val_loss: 1.3395 - val_acc: 0.4361\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.45115\n",
      "Epoch 49/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 1.1819 - acc: 0.4341 - val_loss: 1.3134 - val_acc: 0.4440\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.45115\n",
      "Epoch 50/300\n",
      "979/979 [==============================] - 57s 59ms/step - loss: 1.1657 - acc: 0.4434 - val_loss: 1.3100 - val_acc: 0.4508\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.45115\n",
      "Epoch 51/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.1462 - acc: 0.4521 - val_loss: 1.3332 - val_acc: 0.4345\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.45115\n",
      "Epoch 52/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.1546 - acc: 0.4367 - val_loss: 1.3250 - val_acc: 0.4457\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.45115\n",
      "Epoch 53/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.1439 - acc: 0.4573 - val_loss: 1.2885 - val_acc: 0.4553\n",
      "\n",
      "Epoch 00053: val_acc improved from 0.45115 to 0.45534, saving model to xception_best.ckpt\n",
      "Epoch 54/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.1310 - acc: 0.4524 - val_loss: 1.3048 - val_acc: 0.4525\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.45534\n",
      "Epoch 55/300\n",
      "979/979 [==============================] - 57s 59ms/step - loss: 1.0953 - acc: 0.4814 - val_loss: 1.3180 - val_acc: 0.4446\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.45534\n",
      "Epoch 56/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 1.1533 - acc: 0.4482 - val_loss: 1.2914 - val_acc: 0.4585\n",
      "\n",
      "Epoch 00056: val_acc improved from 0.45534 to 0.45851, saving model to xception_best.ckpt\n",
      "Epoch 57/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.1117 - acc: 0.4675 - val_loss: 1.2921 - val_acc: 0.4557\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.45851\n",
      "Epoch 58/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.1350 - acc: 0.4515 - val_loss: 1.3007 - val_acc: 0.4503\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.45851\n",
      "Epoch 59/300\n",
      "979/979 [==============================] - 57s 59ms/step - loss: 1.1078 - acc: 0.4514 - val_loss: 1.3027 - val_acc: 0.4492\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.45851\n",
      "Epoch 60/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 1.1309 - acc: 0.4435 - val_loss: 1.2835 - val_acc: 0.4612\n",
      "\n",
      "Epoch 00060: val_acc improved from 0.45851 to 0.46117, saving model to xception_best.ckpt\n",
      "Epoch 61/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.1172 - acc: 0.4723 - val_loss: 1.2902 - val_acc: 0.4561\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.46117\n",
      "Epoch 62/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.1301 - acc: 0.4618 - val_loss: 1.2928 - val_acc: 0.4497\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.46117\n",
      "Epoch 63/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 1.0880 - acc: 0.4765 - val_loss: 1.2669 - val_acc: 0.4669\n",
      "\n",
      "Epoch 00063: val_acc improved from 0.46117 to 0.46689, saving model to xception_best.ckpt\n",
      "Epoch 64/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.1046 - acc: 0.4625 - val_loss: 1.2620 - val_acc: 0.4677\n",
      "\n",
      "Epoch 00064: val_acc improved from 0.46689 to 0.46771, saving model to xception_best.ckpt\n",
      "Epoch 65/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.0910 - acc: 0.4670 - val_loss: 1.2715 - val_acc: 0.4670\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.46771\n",
      "Epoch 66/300\n",
      "979/979 [==============================] - 57s 59ms/step - loss: 1.1129 - acc: 0.4638 - val_loss: 1.2588 - val_acc: 0.4681\n",
      "\n",
      "Epoch 00066: val_acc improved from 0.46771 to 0.46812, saving model to xception_best.ckpt\n",
      "Epoch 67/300\n",
      "979/979 [==============================] - 57s 59ms/step - loss: 1.1072 - acc: 0.4755 - val_loss: 1.2593 - val_acc: 0.4695\n",
      "\n",
      "Epoch 00067: val_acc improved from 0.46812 to 0.46955, saving model to xception_best.ckpt\n",
      "Epoch 68/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.1004 - acc: 0.4594 - val_loss: 1.2395 - val_acc: 0.4752\n",
      "\n",
      "Epoch 00068: val_acc improved from 0.46955 to 0.47517, saving model to xception_best.ckpt\n",
      "Epoch 69/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 1.0807 - acc: 0.4717 - val_loss: 1.2454 - val_acc: 0.4753\n",
      "\n",
      "Epoch 00069: val_acc improved from 0.47517 to 0.47527, saving model to xception_best.ckpt\n",
      "Epoch 70/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.0910 - acc: 0.4810 - val_loss: 1.2612 - val_acc: 0.4713\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.47527\n",
      "Epoch 71/300\n",
      "979/979 [==============================] - 57s 59ms/step - loss: 1.0811 - acc: 0.4653 - val_loss: 1.2615 - val_acc: 0.4692\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.47527\n",
      "Epoch 72/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.0954 - acc: 0.4660 - val_loss: 1.2536 - val_acc: 0.4771\n",
      "\n",
      "Epoch 00072: val_acc improved from 0.47527 to 0.47711, saving model to xception_best.ckpt\n",
      "Epoch 73/300\n",
      "979/979 [==============================] - 57s 59ms/step - loss: 1.0615 - acc: 0.4818 - val_loss: 1.2464 - val_acc: 0.4793\n",
      "\n",
      "Epoch 00073: val_acc improved from 0.47711 to 0.47926, saving model to xception_best.ckpt\n",
      "Epoch 74/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 1.0560 - acc: 0.4708 - val_loss: 1.2821 - val_acc: 0.4596\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.47926\n",
      "Epoch 75/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.0760 - acc: 0.4721 - val_loss: 1.2469 - val_acc: 0.4767\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.47926\n",
      "Epoch 76/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.0636 - acc: 0.4910 - val_loss: 1.2326 - val_acc: 0.4811\n",
      "\n",
      "Epoch 00076: val_acc improved from 0.47926 to 0.48110, saving model to xception_best.ckpt\n",
      "Epoch 77/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 1.0335 - acc: 0.4949 - val_loss: 1.2629 - val_acc: 0.4727\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.48110\n",
      "Epoch 78/300\n",
      "979/979 [==============================] - 57s 59ms/step - loss: 1.0506 - acc: 0.4890 - val_loss: 1.2289 - val_acc: 0.4850\n",
      "\n",
      "Epoch 00078: val_acc improved from 0.48110 to 0.48498, saving model to xception_best.ckpt\n",
      "Epoch 79/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.0606 - acc: 0.4901 - val_loss: 1.2474 - val_acc: 0.4732\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.48498\n",
      "Epoch 80/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 1.0685 - acc: 0.4776 - val_loss: 1.2216 - val_acc: 0.4861\n",
      "\n",
      "Epoch 00080: val_acc improved from 0.48498 to 0.48610, saving model to xception_best.ckpt\n",
      "Epoch 81/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.0553 - acc: 0.4778 - val_loss: 1.2097 - val_acc: 0.4895\n",
      "\n",
      "Epoch 00081: val_acc improved from 0.48610 to 0.48947, saving model to xception_best.ckpt\n",
      "Epoch 82/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.0662 - acc: 0.4822 - val_loss: 1.2344 - val_acc: 0.4847\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.48947\n",
      "Epoch 83/300\n",
      "979/979 [==============================] - 57s 59ms/step - loss: 1.0552 - acc: 0.4981 - val_loss: 1.2075 - val_acc: 0.4898\n",
      "\n",
      "Epoch 00083: val_acc improved from 0.48947 to 0.48978, saving model to xception_best.ckpt\n",
      "Epoch 84/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 1.0493 - acc: 0.4962 - val_loss: 1.2212 - val_acc: 0.4923\n",
      "\n",
      "Epoch 00084: val_acc improved from 0.48978 to 0.49234, saving model to xception_best.ckpt\n",
      "Epoch 85/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.0514 - acc: 0.4873 - val_loss: 1.2158 - val_acc: 0.4882\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.49234\n",
      "Epoch 86/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.0696 - acc: 0.4920 - val_loss: 1.2020 - val_acc: 0.4944\n",
      "\n",
      "Epoch 00086: val_acc improved from 0.49234 to 0.49438, saving model to xception_best.ckpt\n",
      "Epoch 87/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.0412 - acc: 0.4873 - val_loss: 1.2105 - val_acc: 0.4910\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.49438\n",
      "Epoch 88/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.0451 - acc: 0.4825 - val_loss: 1.1958 - val_acc: 0.4975\n",
      "\n",
      "Epoch 00088: val_acc improved from 0.49438 to 0.49755, saving model to xception_best.ckpt\n",
      "Epoch 89/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.0603 - acc: 0.4830 - val_loss: 1.2034 - val_acc: 0.4939\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.49755\n",
      "Epoch 90/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.0447 - acc: 0.5001 - val_loss: 1.2003 - val_acc: 0.4966\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.49755\n",
      "Epoch 91/300\n",
      "979/979 [==============================] - 57s 59ms/step - loss: 1.0594 - acc: 0.4890 - val_loss: 1.2130 - val_acc: 0.4906\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.49755\n",
      "Epoch 92/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.0569 - acc: 0.4865 - val_loss: 1.1963 - val_acc: 0.4964\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.49755\n",
      "Epoch 93/300\n",
      "979/979 [==============================] - 57s 59ms/step - loss: 1.0191 - acc: 0.4883 - val_loss: 1.1885 - val_acc: 0.4978\n",
      "\n",
      "Epoch 00093: val_acc improved from 0.49755 to 0.49775, saving model to xception_best.ckpt\n",
      "Epoch 94/300\n",
      "979/979 [==============================] - 57s 59ms/step - loss: 1.0346 - acc: 0.4948 - val_loss: 1.1985 - val_acc: 0.4940\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.49775\n",
      "Epoch 95/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.0116 - acc: 0.4980 - val_loss: 1.1947 - val_acc: 0.4983\n",
      "\n",
      "Epoch 00095: val_acc improved from 0.49775 to 0.49826, saving model to xception_best.ckpt\n",
      "Epoch 96/300\n",
      "979/979 [==============================] - 57s 59ms/step - loss: 1.0397 - acc: 0.4888 - val_loss: 1.2088 - val_acc: 0.4921\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.49826\n",
      "Epoch 97/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9978 - acc: 0.4971 - val_loss: 1.2003 - val_acc: 0.5001\n",
      "\n",
      "Epoch 00097: val_acc improved from 0.49826 to 0.50010, saving model to xception_best.ckpt\n",
      "Epoch 98/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.0570 - acc: 0.4911 - val_loss: 1.2010 - val_acc: 0.4953\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.50010\n",
      "Epoch 99/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.0129 - acc: 0.5101 - val_loss: 1.1943 - val_acc: 0.4997\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.50010\n",
      "Epoch 100/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.0344 - acc: 0.5133 - val_loss: 1.1871 - val_acc: 0.5037\n",
      "\n",
      "Epoch 00100: val_acc improved from 0.50010 to 0.50368, saving model to xception_best.ckpt\n",
      "Epoch 101/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 0.9969 - acc: 0.5069 - val_loss: 1.1887 - val_acc: 0.5025\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.50368\n",
      "Epoch 102/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 0.9996 - acc: 0.5204 - val_loss: 1.1955 - val_acc: 0.4986\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.50368\n",
      "Epoch 103/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.0156 - acc: 0.4991 - val_loss: 1.1792 - val_acc: 0.5065\n",
      "\n",
      "Epoch 00103: val_acc improved from 0.50368 to 0.50654, saving model to xception_best.ckpt\n",
      "Epoch 104/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 0.9964 - acc: 0.5133 - val_loss: 1.1928 - val_acc: 0.4998\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.50654\n",
      "Epoch 105/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.0181 - acc: 0.5041 - val_loss: 1.1773 - val_acc: 0.5098\n",
      "\n",
      "Epoch 00105: val_acc improved from 0.50654 to 0.50981, saving model to xception_best.ckpt\n",
      "Epoch 106/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.0134 - acc: 0.5047 - val_loss: 1.1875 - val_acc: 0.4979\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.50981\n",
      "Epoch 107/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 0.9971 - acc: 0.5163 - val_loss: 1.1916 - val_acc: 0.4953\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.50981\n",
      "Epoch 108/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 1.0157 - acc: 0.4982 - val_loss: 1.1752 - val_acc: 0.5064\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.50981\n",
      "Epoch 109/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 0.9868 - acc: 0.5162 - val_loss: 1.1669 - val_acc: 0.5106\n",
      "\n",
      "Epoch 00109: val_acc improved from 0.50981 to 0.51063, saving model to xception_best.ckpt\n",
      "Epoch 110/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.0028 - acc: 0.5011 - val_loss: 1.1695 - val_acc: 0.5103\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.51063\n",
      "Epoch 111/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9989 - acc: 0.5056 - val_loss: 1.1615 - val_acc: 0.5082\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.51063\n",
      "Epoch 112/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 1.0178 - acc: 0.5151 - val_loss: 1.1776 - val_acc: 0.5063\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.51063\n",
      "Epoch 113/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 0.9801 - acc: 0.5222 - val_loss: 1.1722 - val_acc: 0.5101\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.51063\n",
      "Epoch 114/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9730 - acc: 0.5309 - val_loss: 1.1662 - val_acc: 0.5124\n",
      "\n",
      "Epoch 00114: val_acc improved from 0.51063 to 0.51236, saving model to xception_best.ckpt\n",
      "Epoch 115/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.0230 - acc: 0.5039 - val_loss: 1.1608 - val_acc: 0.5142\n",
      "\n",
      "Epoch 00115: val_acc improved from 0.51236 to 0.51420, saving model to xception_best.ckpt\n",
      "Epoch 116/300\n",
      "979/979 [==============================] - 58s 60ms/step - loss: 0.9768 - acc: 0.5200 - val_loss: 1.1817 - val_acc: 0.5018\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.51420\n",
      "Epoch 117/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 1.0153 - acc: 0.4974 - val_loss: 1.1806 - val_acc: 0.5015\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.51420\n",
      "Epoch 118/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9678 - acc: 0.5246 - val_loss: 1.1644 - val_acc: 0.5099\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.51420\n",
      "Epoch 119/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 0.9985 - acc: 0.5140 - val_loss: 1.1479 - val_acc: 0.5161\n",
      "\n",
      "Epoch 00119: val_acc improved from 0.51420 to 0.51615, saving model to xception_best.ckpt\n",
      "Epoch 120/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 0.9994 - acc: 0.5173 - val_loss: 1.1555 - val_acc: 0.5159\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.51615\n",
      "Epoch 121/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 1.0009 - acc: 0.5202 - val_loss: 1.1473 - val_acc: 0.5201\n",
      "\n",
      "Epoch 00121: val_acc improved from 0.51615 to 0.52013, saving model to xception_best.ckpt\n",
      "Epoch 122/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9993 - acc: 0.5214 - val_loss: 1.1501 - val_acc: 0.5184\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.52013\n",
      "Epoch 123/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 0.9818 - acc: 0.5292 - val_loss: 1.1614 - val_acc: 0.5146\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.52013\n",
      "Epoch 124/300\n",
      "979/979 [==============================] - 57s 59ms/step - loss: 0.9651 - acc: 0.5236 - val_loss: 1.1447 - val_acc: 0.5206\n",
      "\n",
      "Epoch 00124: val_acc improved from 0.52013 to 0.52064, saving model to xception_best.ckpt\n",
      "Epoch 125/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 0.9865 - acc: 0.5238 - val_loss: 1.1507 - val_acc: 0.5185\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.52064\n",
      "Epoch 126/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 0.9664 - acc: 0.5244 - val_loss: 1.1491 - val_acc: 0.5197\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.52064\n",
      "Epoch 127/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 0.9529 - acc: 0.5395 - val_loss: 1.1403 - val_acc: 0.5237\n",
      "\n",
      "Epoch 00127: val_acc improved from 0.52064 to 0.52371, saving model to xception_best.ckpt\n",
      "Epoch 128/300\n",
      "979/979 [==============================] - 57s 59ms/step - loss: 0.9849 - acc: 0.5200 - val_loss: 1.1443 - val_acc: 0.5209\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.52371\n",
      "Epoch 129/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 0.9593 - acc: 0.5285 - val_loss: 1.1449 - val_acc: 0.5182\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.52371\n",
      "Epoch 130/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 0.9577 - acc: 0.5302 - val_loss: 1.1487 - val_acc: 0.5168\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.52371\n",
      "Epoch 131/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9695 - acc: 0.5307 - val_loss: 1.1379 - val_acc: 0.5229\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.52371\n",
      "Epoch 132/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 0.9597 - acc: 0.5267 - val_loss: 1.1583 - val_acc: 0.5102\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.52371\n",
      "Epoch 133/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 0.9515 - acc: 0.5306 - val_loss: 1.1415 - val_acc: 0.5199\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.52371\n",
      "Epoch 134/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 0.9618 - acc: 0.5261 - val_loss: 1.1515 - val_acc: 0.5158\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.52371\n",
      "Epoch 135/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 0.9788 - acc: 0.5316 - val_loss: 1.1508 - val_acc: 0.5186\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.52371\n",
      "Epoch 136/300\n",
      "979/979 [==============================] - 57s 59ms/step - loss: 0.9563 - acc: 0.5269 - val_loss: 1.1364 - val_acc: 0.5217\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.52371\n",
      "Epoch 137/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 0.9601 - acc: 0.5237 - val_loss: 1.1401 - val_acc: 0.5227\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.52371\n",
      "Epoch 138/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 0.9960 - acc: 0.5249 - val_loss: 1.1352 - val_acc: 0.5222\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.52371\n",
      "Epoch 139/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 0.9661 - acc: 0.5238 - val_loss: 1.1336 - val_acc: 0.5265\n",
      "\n",
      "Epoch 00139: val_acc improved from 0.52371 to 0.52647, saving model to xception_best.ckpt\n",
      "Epoch 140/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 0.9517 - acc: 0.5416 - val_loss: 1.1386 - val_acc: 0.5231\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.52647\n",
      "Epoch 141/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 0.9547 - acc: 0.5257 - val_loss: 1.1454 - val_acc: 0.5180\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.52647\n",
      "Epoch 142/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9804 - acc: 0.5234 - val_loss: 1.1273 - val_acc: 0.5267\n",
      "\n",
      "Epoch 00142: val_acc improved from 0.52647 to 0.52667, saving model to xception_best.ckpt\n",
      "Epoch 143/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9325 - acc: 0.5310 - val_loss: 1.1487 - val_acc: 0.5216\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.52667\n",
      "Epoch 144/300\n",
      "979/979 [==============================] - 58s 60ms/step - loss: 0.9698 - acc: 0.5220 - val_loss: 1.1315 - val_acc: 0.5297\n",
      "\n",
      "Epoch 00144: val_acc improved from 0.52667 to 0.52974, saving model to xception_best.ckpt\n",
      "Epoch 145/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 0.9500 - acc: 0.5343 - val_loss: 1.1383 - val_acc: 0.5262\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.52974\n",
      "Epoch 146/300\n",
      "979/979 [==============================] - 56s 58ms/step - loss: 0.9401 - acc: 0.5325 - val_loss: 1.1344 - val_acc: 0.5252\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.52974\n",
      "Epoch 147/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9582 - acc: 0.5344 - val_loss: 1.1344 - val_acc: 0.5279\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.52974\n",
      "Epoch 148/300\n",
      "979/979 [==============================] - 57s 59ms/step - loss: 0.9533 - acc: 0.5363 - val_loss: 1.1288 - val_acc: 0.5253\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.52974\n",
      "Epoch 149/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9381 - acc: 0.5427 - val_loss: 1.1279 - val_acc: 0.5284\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.52974\n",
      "Epoch 150/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9689 - acc: 0.5188 - val_loss: 1.1324 - val_acc: 0.5251\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.52974\n",
      "Epoch 151/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9569 - acc: 0.5364 - val_loss: 1.1151 - val_acc: 0.5335\n",
      "\n",
      "Epoch 00151: val_acc improved from 0.52974 to 0.53352, saving model to xception_best.ckpt\n",
      "Epoch 152/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9303 - acc: 0.5350 - val_loss: 1.1208 - val_acc: 0.5327\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 0.53352\n",
      "Epoch 153/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9552 - acc: 0.5335 - val_loss: 1.1209 - val_acc: 0.5355\n",
      "\n",
      "Epoch 00153: val_acc improved from 0.53352 to 0.53546, saving model to xception_best.ckpt\n",
      "Epoch 154/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9456 - acc: 0.5421 - val_loss: 1.1082 - val_acc: 0.5353\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 0.53546\n",
      "Epoch 155/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9499 - acc: 0.5425 - val_loss: 1.1187 - val_acc: 0.5323\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 0.53546\n",
      "Epoch 156/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9453 - acc: 0.5348 - val_loss: 1.1192 - val_acc: 0.5308\n",
      "\n",
      "Epoch 00156: val_acc did not improve from 0.53546\n",
      "Epoch 157/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9426 - acc: 0.5366 - val_loss: 1.1147 - val_acc: 0.5338\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 0.53546\n",
      "Epoch 158/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9261 - acc: 0.5364 - val_loss: 1.1337 - val_acc: 0.5264\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 0.53546\n",
      "Epoch 159/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9663 - acc: 0.5469 - val_loss: 1.1116 - val_acc: 0.5367\n",
      "\n",
      "Epoch 00159: val_acc improved from 0.53546 to 0.53669, saving model to xception_best.ckpt\n",
      "Epoch 160/300\n",
      "979/979 [==============================] - 57s 59ms/step - loss: 0.9519 - acc: 0.5308 - val_loss: 1.1165 - val_acc: 0.5320\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 0.53669\n",
      "Epoch 161/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9157 - acc: 0.5568 - val_loss: 1.1166 - val_acc: 0.5360\n",
      "\n",
      "Epoch 00161: val_acc did not improve from 0.53669\n",
      "Epoch 162/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9330 - acc: 0.5376 - val_loss: 1.1037 - val_acc: 0.5386\n",
      "\n",
      "Epoch 00162: val_acc improved from 0.53669 to 0.53863, saving model to xception_best.ckpt\n",
      "Epoch 163/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9444 - acc: 0.5399 - val_loss: 1.1097 - val_acc: 0.5365\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 0.53863\n",
      "Epoch 164/300\n",
      "979/979 [==============================] - 56s 57ms/step - loss: 0.9272 - acc: 0.5416 - val_loss: 1.1271 - val_acc: 0.5355\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 0.53863\n",
      "Epoch 165/300\n",
      "979/979 [==============================] - 54s 55ms/step - loss: 0.9523 - acc: 0.5341 - val_loss: 1.1172 - val_acc: 0.5370\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 0.53863\n",
      "Epoch 166/300\n",
      "979/979 [==============================] - 54s 55ms/step - loss: 0.9267 - acc: 0.5371 - val_loss: 1.1140 - val_acc: 0.5324\n",
      "\n",
      "Epoch 00166: val_acc did not improve from 0.53863\n",
      "Epoch 167/300\n",
      "979/979 [==============================] - 54s 55ms/step - loss: 0.9304 - acc: 0.5402 - val_loss: 1.1060 - val_acc: 0.5376\n",
      "\n",
      "Epoch 00167: val_acc did not improve from 0.53863\n",
      "Epoch 168/300\n",
      "979/979 [==============================] - 54s 55ms/step - loss: 0.9589 - acc: 0.5537 - val_loss: 1.0964 - val_acc: 0.5437\n",
      "\n",
      "Epoch 00168: val_acc improved from 0.53863 to 0.54374, saving model to xception_best.ckpt\n",
      "Epoch 169/300\n",
      "979/979 [==============================] - 54s 55ms/step - loss: 0.8788 - acc: 0.5683 - val_loss: 1.1127 - val_acc: 0.5340\n",
      "\n",
      "Epoch 00169: val_acc did not improve from 0.54374\n",
      "Epoch 170/300\n",
      "979/979 [==============================] - 54s 55ms/step - loss: 0.9402 - acc: 0.5332 - val_loss: 1.1047 - val_acc: 0.5408\n",
      "\n",
      "Epoch 00170: val_acc did not improve from 0.54374\n",
      "Epoch 171/300\n",
      "979/979 [==============================] - 54s 55ms/step - loss: 0.9081 - acc: 0.5556 - val_loss: 1.1031 - val_acc: 0.5411\n",
      "\n",
      "Epoch 00171: val_acc did not improve from 0.54374\n",
      "Epoch 172/300\n",
      "979/979 [==============================] - 54s 55ms/step - loss: 0.9325 - acc: 0.5487 - val_loss: 1.0947 - val_acc: 0.5427\n",
      "\n",
      "Epoch 00172: val_acc did not improve from 0.54374\n",
      "Epoch 173/300\n",
      "979/979 [==============================] - 54s 55ms/step - loss: 0.9230 - acc: 0.5509 - val_loss: 1.1087 - val_acc: 0.5348\n",
      "\n",
      "Epoch 00173: val_acc did not improve from 0.54374\n",
      "Epoch 174/300\n",
      "979/979 [==============================] - 56s 57ms/step - loss: 0.9385 - acc: 0.5388 - val_loss: 1.1213 - val_acc: 0.5276\n",
      "\n",
      "Epoch 00174: val_acc did not improve from 0.54374\n",
      "Epoch 175/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9168 - acc: 0.5419 - val_loss: 1.0968 - val_acc: 0.5434\n",
      "\n",
      "Epoch 00175: val_acc did not improve from 0.54374\n",
      "Epoch 176/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9119 - acc: 0.5522 - val_loss: 1.1128 - val_acc: 0.5383\n",
      "\n",
      "Epoch 00176: val_acc did not improve from 0.54374\n",
      "Epoch 177/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9565 - acc: 0.5374 - val_loss: 1.0937 - val_acc: 0.5465\n",
      "\n",
      "Epoch 00177: val_acc improved from 0.54374 to 0.54650, saving model to xception_best.ckpt\n",
      "Epoch 178/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9493 - acc: 0.5456 - val_loss: 1.0914 - val_acc: 0.5476\n",
      "\n",
      "Epoch 00178: val_acc improved from 0.54650 to 0.54762, saving model to xception_best.ckpt\n",
      "Epoch 179/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9246 - acc: 0.5436 - val_loss: 1.1018 - val_acc: 0.5372\n",
      "\n",
      "Epoch 00179: val_acc did not improve from 0.54762\n",
      "Epoch 180/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9148 - acc: 0.5501 - val_loss: 1.1001 - val_acc: 0.5408\n",
      "\n",
      "Epoch 00180: val_acc did not improve from 0.54762\n",
      "Epoch 181/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9189 - acc: 0.5542 - val_loss: 1.1002 - val_acc: 0.5439\n",
      "\n",
      "Epoch 00181: val_acc did not improve from 0.54762\n",
      "Epoch 182/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.8972 - acc: 0.5713 - val_loss: 1.0898 - val_acc: 0.5472\n",
      "\n",
      "Epoch 00182: val_acc did not improve from 0.54762\n",
      "Epoch 183/300\n",
      "979/979 [==============================] - 57s 59ms/step - loss: 0.9382 - acc: 0.5389 - val_loss: 1.0958 - val_acc: 0.5463\n",
      "\n",
      "Epoch 00183: val_acc did not improve from 0.54762\n",
      "Epoch 184/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9390 - acc: 0.5495 - val_loss: 1.0895 - val_acc: 0.5472\n",
      "\n",
      "Epoch 00184: val_acc did not improve from 0.54762\n",
      "Epoch 185/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9116 - acc: 0.5415 - val_loss: 1.0889 - val_acc: 0.5436\n",
      "\n",
      "Epoch 00185: val_acc did not improve from 0.54762\n",
      "Epoch 186/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 0.9192 - acc: 0.5491 - val_loss: 1.0933 - val_acc: 0.5421\n",
      "\n",
      "Epoch 00186: val_acc did not improve from 0.54762\n",
      "Epoch 187/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9245 - acc: 0.5450 - val_loss: 1.1019 - val_acc: 0.5406\n",
      "\n",
      "Epoch 00187: val_acc did not improve from 0.54762\n",
      "Epoch 188/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 0.9489 - acc: 0.5293 - val_loss: 1.0854 - val_acc: 0.5479\n",
      "\n",
      "Epoch 00188: val_acc improved from 0.54762 to 0.54793, saving model to xception_best.ckpt\n",
      "Epoch 189/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9107 - acc: 0.5496 - val_loss: 1.0880 - val_acc: 0.5460\n",
      "\n",
      "Epoch 00189: val_acc did not improve from 0.54793\n",
      "Epoch 190/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.8905 - acc: 0.5404 - val_loss: 1.0909 - val_acc: 0.5447\n",
      "\n",
      "Epoch 00190: val_acc did not improve from 0.54793\n",
      "Epoch 191/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9232 - acc: 0.5497 - val_loss: 1.0898 - val_acc: 0.5463\n",
      "\n",
      "Epoch 00191: val_acc did not improve from 0.54793\n",
      "Epoch 192/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 0.9206 - acc: 0.5453 - val_loss: 1.0933 - val_acc: 0.5422\n",
      "\n",
      "Epoch 00192: val_acc did not improve from 0.54793\n",
      "Epoch 193/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9144 - acc: 0.5515 - val_loss: 1.0809 - val_acc: 0.5536\n",
      "\n",
      "Epoch 00193: val_acc improved from 0.54793 to 0.55365, saving model to xception_best.ckpt\n",
      "Epoch 194/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.8916 - acc: 0.5589 - val_loss: 1.0880 - val_acc: 0.5502\n",
      "\n",
      "Epoch 00194: val_acc did not improve from 0.55365\n",
      "Epoch 195/300\n",
      "979/979 [==============================] - 57s 59ms/step - loss: 0.9253 - acc: 0.5503 - val_loss: 1.0854 - val_acc: 0.5460\n",
      "\n",
      "Epoch 00195: val_acc did not improve from 0.55365\n",
      "Epoch 196/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9149 - acc: 0.5547 - val_loss: 1.0808 - val_acc: 0.5477\n",
      "\n",
      "Epoch 00196: val_acc did not improve from 0.55365\n",
      "Epoch 197/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9000 - acc: 0.5483 - val_loss: 1.0853 - val_acc: 0.5476\n",
      "\n",
      "Epoch 00197: val_acc did not improve from 0.55365\n",
      "Epoch 198/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9398 - acc: 0.5427 - val_loss: 1.0793 - val_acc: 0.5510\n",
      "\n",
      "Epoch 00198: val_acc did not improve from 0.55365\n",
      "Epoch 199/300\n",
      "979/979 [==============================] - 57s 59ms/step - loss: 0.9046 - acc: 0.5524 - val_loss: 1.0846 - val_acc: 0.5534\n",
      "\n",
      "Epoch 00199: val_acc did not improve from 0.55365\n",
      "Epoch 200/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9110 - acc: 0.5450 - val_loss: 1.0853 - val_acc: 0.5499\n",
      "\n",
      "Epoch 00200: val_acc did not improve from 0.55365\n",
      "Epoch 201/300\n",
      "979/979 [==============================] - 57s 59ms/step - loss: 0.9450 - acc: 0.5424 - val_loss: 1.0801 - val_acc: 0.5526\n",
      "\n",
      "Epoch 00201: val_acc did not improve from 0.55365\n",
      "Epoch 202/300\n",
      "979/979 [==============================] - 57s 59ms/step - loss: 0.9145 - acc: 0.5536 - val_loss: 1.0892 - val_acc: 0.5462\n",
      "\n",
      "Epoch 00202: val_acc did not improve from 0.55365\n",
      "Epoch 203/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 0.8943 - acc: 0.5576 - val_loss: 1.0850 - val_acc: 0.5490\n",
      "\n",
      "Epoch 00203: val_acc did not improve from 0.55365\n",
      "Epoch 204/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9286 - acc: 0.5533 - val_loss: 1.0908 - val_acc: 0.5531\n",
      "\n",
      "Epoch 00204: val_acc did not improve from 0.55365\n",
      "Epoch 205/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9101 - acc: 0.5566 - val_loss: 1.0781 - val_acc: 0.5563\n",
      "\n",
      "Epoch 00205: val_acc improved from 0.55365 to 0.55630, saving model to xception_best.ckpt\n",
      "Epoch 206/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9388 - acc: 0.5448 - val_loss: 1.0751 - val_acc: 0.5530\n",
      "\n",
      "Epoch 00206: val_acc did not improve from 0.55630\n",
      "Epoch 207/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 0.8941 - acc: 0.5647 - val_loss: 1.0815 - val_acc: 0.5495\n",
      "\n",
      "Epoch 00207: val_acc did not improve from 0.55630\n",
      "Epoch 208/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.8922 - acc: 0.5678 - val_loss: 1.0706 - val_acc: 0.5529\n",
      "\n",
      "Epoch 00208: val_acc did not improve from 0.55630\n",
      "Epoch 209/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.9125 - acc: 0.5562 - val_loss: 1.0713 - val_acc: 0.5553\n",
      "\n",
      "Epoch 00209: val_acc did not improve from 0.55630\n",
      "Epoch 210/300\n",
      "979/979 [==============================] - 57s 59ms/step - loss: 0.9077 - acc: 0.5651 - val_loss: 1.0691 - val_acc: 0.5563\n",
      "\n",
      "Epoch 00210: val_acc did not improve from 0.55630\n",
      "Epoch 211/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.8970 - acc: 0.5498 - val_loss: 1.0779 - val_acc: 0.5519\n",
      "\n",
      "Epoch 00211: val_acc did not improve from 0.55630\n",
      "Epoch 212/300\n",
      "979/979 [==============================] - 57s 59ms/step - loss: 0.8878 - acc: 0.5663 - val_loss: 1.0734 - val_acc: 0.5563\n",
      "\n",
      "Epoch 00212: val_acc did not improve from 0.55630\n",
      "Epoch 213/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.8880 - acc: 0.5536 - val_loss: 1.0820 - val_acc: 0.5524\n",
      "\n",
      "Epoch 00213: val_acc did not improve from 0.55630\n",
      "Epoch 214/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 0.9010 - acc: 0.5584 - val_loss: 1.0703 - val_acc: 0.5571\n",
      "\n",
      "Epoch 00214: val_acc improved from 0.55630 to 0.55712, saving model to xception_best.ckpt\n",
      "Epoch 215/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.8934 - acc: 0.5541 - val_loss: 1.0805 - val_acc: 0.5523\n",
      "\n",
      "Epoch 00215: val_acc did not improve from 0.55712\n",
      "Epoch 216/300\n",
      "979/979 [==============================] - 58s 59ms/step - loss: 0.8892 - acc: 0.5575 - val_loss: 1.0780 - val_acc: 0.5538\n",
      "\n",
      "Epoch 00216: val_acc did not improve from 0.55712\n",
      "Epoch 217/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.8748 - acc: 0.5589 - val_loss: 1.0805 - val_acc: 0.5484\n",
      "\n",
      "Epoch 00217: val_acc did not improve from 0.55712\n",
      "Epoch 218/300\n",
      "979/979 [==============================] - 57s 59ms/step - loss: 0.9140 - acc: 0.5543 - val_loss: 1.0891 - val_acc: 0.5494\n",
      "\n",
      "Epoch 00218: val_acc did not improve from 0.55712\n",
      "Epoch 219/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.8665 - acc: 0.5660 - val_loss: 1.0743 - val_acc: 0.5526\n",
      "\n",
      "Epoch 00219: val_acc did not improve from 0.55712\n",
      "Epoch 220/300\n",
      "979/979 [==============================] - 57s 58ms/step - loss: 0.8725 - acc: 0.5615 - val_loss: 1.0721 - val_acc: 0.5570\n",
      "\n",
      "Epoch 00220: val_acc did not improve from 0.55712\n",
      "\n",
      "Epoch 00220: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f5d88188eb8>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xception_model.fit(\n",
    "        valid_generator,\n",
    "        epochs = 300,\n",
    "        validation_data = train_generator,\n",
    "        callbacks = [my_callbacks],\n",
    "        class_weight = class_weights\n",
    ")\n",
    "\n",
    "xception_model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-morris",
   "metadata": {
    "id": "pretty-morris",
    "papermill": {
     "duration": 3.201635,
     "end_time": "2021-05-30T20:12:18.344310",
     "exception": false,
     "start_time": "2021-05-30T20:12:15.142675",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 위의 학습에서 사용하였던 검증 데이터 세트에 대한 추가 학습\n",
    "\n",
    "### 검증 데이터 세트를 학습, 검증 데이터 세트로 재구성하여 추가 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cloudy-idaho",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-30T20:12:24.238372Z",
     "iopub.status.busy": "2021-05-30T20:12:24.237660Z",
     "iopub.status.idle": "2021-05-30T20:12:24.305208Z",
     "shell.execute_reply": "2021-05-30T20:12:24.305579Z",
     "shell.execute_reply.started": "2021-05-30T09:30:16.948040Z"
    },
    "id": "cloudy-idaho",
    "papermill": {
     "duration": 3.027306,
     "end_time": "2021-05-30T20:12:24.305720",
     "exception": false,
     "start_time": "2021-05-30T20:12:21.278414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test = train_test_split(\n",
    "    df_train_kaggle, \n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify= df_train_kaggle.label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "nervous-crisis",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-30T20:12:30.429807Z",
     "iopub.status.busy": "2021-05-30T20:12:30.429001Z",
     "iopub.status.idle": "2021-05-30T20:12:30.472297Z",
     "shell.execute_reply": "2021-05-30T20:12:30.471634Z",
     "shell.execute_reply.started": "2021-05-30T09:30:18.098841Z"
    },
    "id": "nervous-crisis",
    "outputId": "4e7db473-44a8-44a1-aead-472c63d3370c",
    "papermill": {
     "duration": 3.239148,
     "end_time": "2021-05-30T20:12:30.472465",
     "exception": false,
     "start_time": "2021-05-30T20:12:27.233317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8807 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_aug.flow_from_dataframe(\n",
    "    dataframe = X_train,\n",
    "    x_col = \"filepath\",\n",
    "    y_col = \"label\",\n",
    "    batch_size = 8,\n",
    "    seed = 42,\n",
    "    shuffle = True,\n",
    "    class_mode = \"raw\",\n",
    "    target_size = (224,224)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "instrumental-indie",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-30T20:12:36.309467Z",
     "iopub.status.busy": "2021-05-30T20:12:36.308631Z",
     "iopub.status.idle": "2021-05-30T20:12:36.318017Z",
     "shell.execute_reply": "2021-05-30T20:12:36.317530Z",
     "shell.execute_reply.started": "2021-05-30T09:30:23.206485Z"
    },
    "id": "instrumental-indie",
    "outputId": "aaa9c990-6529-4f11-8bef-cd7be0713256",
    "papermill": {
     "duration": 2.927421,
     "end_time": "2021-05-30T20:12:36.318129",
     "exception": false,
     "start_time": "2021-05-30T20:12:33.390708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 979 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "valid_generator = valid_aug.flow_from_dataframe( \n",
    "    dataframe=X_test,\n",
    "    x_col = \"filepath\",\n",
    "    y_col = \"label\",\n",
    "    batch_size = 8,\n",
    "    seed = 42,\n",
    "    shuffle = True,\n",
    "    class_mode = \"raw\",\n",
    "    target_size = (224,224)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dedicated-chapel",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-30T20:12:42.897491Z",
     "iopub.status.busy": "2021-05-30T20:12:42.895853Z",
     "iopub.status.idle": "2021-05-30T20:12:42.898202Z",
     "shell.execute_reply": "2021-05-30T20:12:42.898602Z",
     "shell.execute_reply.started": "2021-05-30T09:30:26.966513Z"
    },
    "id": "dedicated-chapel",
    "papermill": {
     "duration": 3.108025,
     "end_time": "2021-05-30T20:12:42.898739",
     "exception": false,
     "start_time": "2021-05-30T20:12:39.790714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "STEP_SIZE_TRAIN = train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID =valid_generator.n//valid_generator.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "sized-norfolk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-30T20:12:48.820785Z",
     "iopub.status.busy": "2021-05-30T20:12:48.819337Z",
     "iopub.status.idle": "2021-05-30T20:25:26.371458Z",
     "shell.execute_reply": "2021-05-30T20:25:26.371941Z",
     "shell.execute_reply.started": "2021-05-30T09:30:36.962285Z"
    },
    "id": "sized-norfolk",
    "outputId": "20071e30-0a6e-43e4-8acc-543ff61f89ec",
    "papermill": {
     "duration": 760.527609,
     "end_time": "2021-05-30T20:25:26.372105",
     "exception": false,
     "start_time": "2021-05-30T20:12:45.844496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1100/1100 [==============================] - 50s 40ms/step - loss: 1.1248 - acc: 0.5279 - val_loss: 1.0193 - val_acc: 0.5984\n",
      "\n",
      "Epoch 00001: val_acc improved from 0.55712 to 0.59836, saving model to xception_best.ckpt\n",
      "Epoch 2/50\n",
      "1100/1100 [==============================] - 43s 39ms/step - loss: 1.1150 - acc: 0.5376 - val_loss: 1.0079 - val_acc: 0.6025\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.59836 to 0.60246, saving model to xception_best.ckpt\n",
      "Epoch 3/50\n",
      "1100/1100 [==============================] - 43s 39ms/step - loss: 1.1019 - acc: 0.5370 - val_loss: 1.0002 - val_acc: 0.5871\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.60246\n",
      "Epoch 4/50\n",
      "1100/1100 [==============================] - 43s 39ms/step - loss: 1.1190 - acc: 0.5309 - val_loss: 1.0033 - val_acc: 0.5943\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.60246\n",
      "Epoch 5/50\n",
      "1100/1100 [==============================] - 43s 39ms/step - loss: 1.1004 - acc: 0.5401 - val_loss: 1.0009 - val_acc: 0.5912\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.60246\n",
      "Epoch 6/50\n",
      "1100/1100 [==============================] - 44s 40ms/step - loss: 1.1026 - acc: 0.5417 - val_loss: 1.0095 - val_acc: 0.5922\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.60246\n",
      "Epoch 7/50\n",
      "1100/1100 [==============================] - 43s 39ms/step - loss: 1.1036 - acc: 0.5378 - val_loss: 1.0084 - val_acc: 0.5902\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.60246\n",
      "Epoch 8/50\n",
      "1100/1100 [==============================] - 43s 39ms/step - loss: 1.0938 - acc: 0.5409 - val_loss: 1.0030 - val_acc: 0.5953\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.60246\n",
      "Epoch 9/50\n",
      "1100/1100 [==============================] - 43s 40ms/step - loss: 1.1140 - acc: 0.5338 - val_loss: 0.9999 - val_acc: 0.5748\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.60246\n",
      "Epoch 10/50\n",
      "1100/1100 [==============================] - 43s 39ms/step - loss: 1.0939 - acc: 0.5359 - val_loss: 1.0056 - val_acc: 0.5922\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.60246\n",
      "Epoch 11/50\n",
      "1100/1100 [==============================] - 43s 39ms/step - loss: 1.0923 - acc: 0.5362 - val_loss: 0.9998 - val_acc: 0.5799\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.60246\n",
      "Epoch 12/50\n",
      "1100/1100 [==============================] - 43s 39ms/step - loss: 1.1016 - acc: 0.5343 - val_loss: 1.0048 - val_acc: 0.5891\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.60246\n",
      "Epoch 13/50\n",
      "1100/1100 [==============================] - 43s 39ms/step - loss: 1.0886 - acc: 0.5451 - val_loss: 0.9994 - val_acc: 0.5707\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.60246\n",
      "Epoch 14/50\n",
      "1100/1100 [==============================] - 43s 39ms/step - loss: 1.0981 - acc: 0.5378 - val_loss: 1.0022 - val_acc: 0.5758\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.60246\n",
      "Epoch 15/50\n",
      "1100/1100 [==============================] - 43s 39ms/step - loss: 1.0938 - acc: 0.5378 - val_loss: 1.0033 - val_acc: 0.5922\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.60246\n",
      "Epoch 16/50\n",
      "1100/1100 [==============================] - 43s 39ms/step - loss: 1.0832 - acc: 0.5426 - val_loss: 1.0069 - val_acc: 0.5994\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.60246\n",
      "Epoch 17/50\n",
      "1100/1100 [==============================] - 43s 39ms/step - loss: 1.0909 - acc: 0.5438 - val_loss: 1.0023 - val_acc: 0.5830\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.60246\n",
      "Epoch 18/50\n",
      "1100/1100 [==============================] - 43s 39ms/step - loss: 1.0904 - acc: 0.5370 - val_loss: 1.0076 - val_acc: 0.5902\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.60246\n",
      "Epoch 19/50\n",
      "1100/1100 [==============================] - 43s 39ms/step - loss: 1.0918 - acc: 0.5438 - val_loss: 1.0045 - val_acc: 0.5738\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.60246\n",
      "Epoch 20/50\n",
      "1100/1100 [==============================] - 43s 39ms/step - loss: 1.1018 - acc: 0.5426 - val_loss: 1.0073 - val_acc: 0.5932\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.60246\n",
      "Epoch 21/50\n",
      "1100/1100 [==============================] - 44s 40ms/step - loss: 1.0782 - acc: 0.5468 - val_loss: 1.0050 - val_acc: 0.5850\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.60246\n",
      "Epoch 22/50\n",
      "1100/1100 [==============================] - 43s 39ms/step - loss: 1.0885 - acc: 0.5449 - val_loss: 0.9971 - val_acc: 0.5779\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.60246\n",
      "Epoch 23/50\n",
      "1100/1100 [==============================] - 43s 39ms/step - loss: 1.0904 - acc: 0.5480 - val_loss: 1.0051 - val_acc: 0.5738\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.60246\n",
      "Epoch 24/50\n",
      "1100/1100 [==============================] - 43s 39ms/step - loss: 1.0736 - acc: 0.5517 - val_loss: 1.0099 - val_acc: 0.5871\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.60246\n",
      "Epoch 25/50\n",
      "1100/1100 [==============================] - 43s 39ms/step - loss: 1.0799 - acc: 0.5437 - val_loss: 1.0035 - val_acc: 0.5779\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.60246\n",
      "Epoch 26/50\n",
      "1100/1100 [==============================] - 43s 39ms/step - loss: 1.0848 - acc: 0.5468 - val_loss: 1.0094 - val_acc: 0.5881\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.60246\n",
      "Epoch 27/50\n",
      "1100/1100 [==============================] - 43s 39ms/step - loss: 1.0756 - acc: 0.5509 - val_loss: 1.0151 - val_acc: 0.5912\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.60246\n",
      "Epoch 28/50\n",
      "1100/1100 [==============================] - 43s 39ms/step - loss: 1.0679 - acc: 0.5510 - val_loss: 1.0042 - val_acc: 0.5840\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.60246\n",
      "Epoch 29/50\n",
      "1100/1100 [==============================] - 43s 39ms/step - loss: 1.0799 - acc: 0.5517 - val_loss: 1.0077 - val_acc: 0.5912\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.60246\n",
      "Epoch 30/50\n",
      "1100/1100 [==============================] - 43s 39ms/step - loss: 1.0743 - acc: 0.5481 - val_loss: 1.0057 - val_acc: 0.5902\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.60246\n",
      "Epoch 31/50\n",
      "1100/1100 [==============================] - 43s 39ms/step - loss: 1.0713 - acc: 0.5513 - val_loss: 1.0113 - val_acc: 0.5871\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.60246\n",
      "Epoch 32/50\n",
      "1100/1100 [==============================] - 43s 39ms/step - loss: 1.0704 - acc: 0.5517 - val_loss: 1.0096 - val_acc: 0.5830\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.60246\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f5d881888d0>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xception_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "    epochs=50,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=STEP_SIZE_VALID,callbacks=[my_callbacks]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "complete-spelling",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-30T20:25:33.497837Z",
     "iopub.status.busy": "2021-05-30T20:25:33.496999Z",
     "iopub.status.idle": "2021-05-30T20:25:34.695800Z",
     "shell.execute_reply": "2021-05-30T20:25:34.695365Z",
     "shell.execute_reply.started": "2021-05-24T16:41:06.725854Z"
    },
    "id": "complete-spelling",
    "outputId": "7f0049d7-bedf-4255-c346-481788903f41",
    "papermill": {
     "duration": 4.566888,
     "end_time": "2021-05-30T20:25:34.695951",
     "exception": false,
     "start_time": "2021-05-30T20:25:30.129063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f5d7c8c7470>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xception_model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-cylinder",
   "metadata": {
    "id": "intended-cylinder",
    "papermill": {
     "duration": 3.633537,
     "end_time": "2021-05-30T20:25:41.743104",
     "exception": false,
     "start_time": "2021-05-30T20:25:38.109567",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 검증 데이터 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "equivalent-album",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-30T20:25:48.625234Z",
     "iopub.status.busy": "2021-05-30T20:25:48.624438Z",
     "iopub.status.idle": "2021-05-30T20:25:48.634216Z",
     "shell.execute_reply": "2021-05-30T20:25:48.633804Z",
     "shell.execute_reply.started": "2021-05-27T05:35:10.161826Z"
    },
    "id": "equivalent-album",
    "outputId": "388c9b93-9c21-4130-d864-0f4678592fdd",
    "papermill": {
     "duration": 3.430239,
     "end_time": "2021-05-30T20:25:48.634343",
     "exception": false,
     "start_time": "2021-05-30T20:25:45.204104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 979 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "target_shape = 224\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "compi_gen = valid_aug.flow_from_dataframe(\n",
    "    dataframe= X_test, \n",
    "    x_col= \"filepath\",\n",
    "    class_mode=None,\n",
    "    target_size= (target_shape, target_shape),\n",
    "    shuffle= False,\n",
    "    batch_size= BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "preceding-delight",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-30T20:25:56.117007Z",
     "iopub.status.busy": "2021-05-30T20:25:56.116264Z",
     "iopub.status.idle": "2021-05-30T20:26:05.659358Z",
     "shell.execute_reply": "2021-05-30T20:26:05.658569Z",
     "shell.execute_reply.started": "2021-05-27T05:35:17.324361Z"
    },
    "id": "preceding-delight",
    "outputId": "711df68f-54a2-471c-eb7d-7e14d128c91a",
    "papermill": {
     "duration": 12.914933,
     "end_time": "2021-05-30T20:26:05.659489",
     "exception": false,
     "start_time": "2021-05-30T20:25:52.744556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "979/979 [==============================] - 11s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "predicition_compi = xception_model.predict(compi_gen, steps= compi_gen.n/ BATCH_SIZE, verbose= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "distinguished-midwest",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "execution": {
     "iopub.execute_input": "2021-05-30T20:26:12.667940Z",
     "iopub.status.busy": "2021-05-30T20:26:12.649791Z",
     "iopub.status.idle": "2021-05-30T20:26:13.051891Z",
     "shell.execute_reply": "2021-05-30T20:26:13.051413Z",
     "shell.execute_reply.started": "2021-05-27T05:43:44.837052Z"
    },
    "id": "distinguished-midwest",
    "outputId": "ff29bab3-ae02-4987-c865-2a296b9c582b",
    "papermill": {
     "duration": 3.832152,
     "end_time": "2021-05-30T20:26:13.052010",
     "exception": false,
     "start_time": "2021-05-30T20:26:09.219858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f5d90ad09e8>"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATYAAAEECAYAAACiDhgPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAApGElEQVR4nO3deXxU1dnA8d+TfQ+EVTYJggiiguCG1VarrVsVlGotRW1foYpLq1aBqp++1hXbSq2KNvbtq0VKXWrFvi5UFBcQkACK7ELY95CE7NvM8/4xkxAgycyQmdw7w/P9fObjzJk75z5G55lz7rnnHFFVjDEmlsQ5HYAxxoSbJTZjTMyxxGaMiTmW2IwxMccSmzEm5iQ4HQBA55x47ds70ekwgvLNmmynQwiJNyU6/q4N4iqqnQ4haOr1Oh1CSMooLlTVLm2p4/sXpOv+Ik/A45auqJmjqpe05Vxt4YrE1rd3Il/M6e10GEG5/MzLnQ4hJNUndnc6hJAkLVnvdAhB85aVOR1CSObqG1vaWsf+Ig9fzOkT8Lj4477p3NZztYUrEpsxJjoo4MX9LVVLbMaYoClKnQbuijrNEpsxJiTWYjPGxBRF8UTBNExLbMaYkHixxGaMiSEKeCyxGWNijbXYjDExRYE6u8ZmjIklilpX1BgTYxQ87s9rltiMMcHzzTxwP0tsxpgQCB7E6SACssRmjAmab/DAEpsxJob47mOzxGaMiTFea7EZY2KJtdjayZ8m9yJOlLKSBM68qJTvXlPMm3ld2LgqldR0D2mZXm6atIu4OJj9P53ZuCoVrxdUhdsf205qujNjPBMnrUS9kJlVx5IFXZn3fk/Gjl9Pp67VJCQqleUJ5E0bhNfj/OrtLzzyFms3+hZe9XjieOZvZwPCNZespP/xRVRWJVJZnchfXx+OuuDXPC5OGXfnFvoPKefBm4c0lo++aQffvWovt48e5mB0rbtgdDHfvqoErwfWLE3n9eldnQ7pEIrgiYIdBSKW2ERkLHAd4AEWquqTkTjPnU9sB0AV7hndn4FDK9m8NoV7n94KwNKPM1n8QRbnfL+Uq/6rsPFzc/6Rw0dvduTycfsjEVZA06c2fOGUqXmLmPd+T2a+eGLj+2PHr2foGftZtqhNKzmHRWl5Mn/833MPKevV/QC5vYqZ+ufzARg+ZAdnD9vGwmWBV1eNtDMvKGLRR50YeNrBFW4HDStl+6ZUSkvc+1uemu7hojHF3D82FxDu/dNWeuTWsHNTstOhHSIauqIRSb0ikgmMA65S1dHAKSIyIBLnalBXI2R28JCU7KWiLJ6GWR8HiuJZszT9kGM9Hlj3ZRp9Bji/vn5ikpfyA4fuS5CU7KFPv3J270hzKKpDxccp/3VtPlNu/Zhzh/tWl66tiyc9rQ78d6FnZ1YzuP9eB6M8aNGHnVi3IvOQsjXLs1jySY5DEQVn8IgKln2aAf6u3sI5WZw2stzZoA6jCLUaH/DhtEj9fI0EPlBtnFQ2G7gA+CZC5+Olqcdx7cS9dO1VxwWji5l2T29SM7z06FtDTZUvf6vCH+/tzZqlaZx9cSmnnF0RqXCCdsMt63ljRj8AMjLrGH/3agadUszsWbns3JYe4NPt457HLgMgPt7Lb+74iM3bO7BjTzYfft6PX908n8rqRHbsySIlud7hSKNbVo6HsiYtyrKSeHrm1jgY0ZF8N+geu13RTkBRk9dFwCEtNhGZAEwA6NOzbWG8mdeF/qdUcfKZvkT1rcsO8K3LDgCweG4W9XXiPyfc9fttAPzjma4snOProjpl1PWb2LguizUrfC2J8rJEpj10GqDceu8q+g0opeCbLMfiO5zHE8fSlT3o26uEHXuymZ/fl/n5fQE4a+g2EhOi4Z509yotiuf4gQcTWWYHD6XF7us6R8PgQaRS736gY5PXOf6yRqqap6ojVHVEl05H33T990udSEnzcuHVxUe8V1sjzP5rZ87/QckR7yWlKJXlzjWZL79mC9VV8Xw8p2cz7wr19XGkpLmvBTR4wD42bDm0S5eY4GHU91bz8eJch6KKDWuXpzPsvDIauvfnfK+UlYvd0WpvoCp4NC7gw2mR+jlYDPxCRKb5u6NXAo+F+ySrlqTx6rPdOOO7pTw9qRcAN963i9n/04WK0ngOFCVw7W176dKjjtoa4Xd39iEj20NdrdCpex2j/mtfuEMKyqBTihlz40byP+/CbZO/BuDVv/bnZ3espbIigaQULwXrM1n9lTuuCU36+afU1MaTmlLPgvw+7Cn0Xb+6acxS0lPryM6s5tV/n0phkbu+hPX1R37Bmitzi4rSeD78Z0emPL8VTz0UrEpl24YUp8M6gjcKWmyiEVpbSUSuB64C6oEvVfX3LR074rQUtX1FI8P2FY2cKNxXdKmqjmhLHQNOSdOnZvcPeNyVJ3zd5nO1RcQ68Ko6C5gVqfqNMe3vWB88MMbEKE8U3Mdmic0YE7RjfuaBMSY2eV0w6hmIJTZjTNB8k+DDk9hE5Hl8C/LmAO+o6isiMhfY0OSwyapaIiKn4buzohyoBCaoal1LdVtiM8YETRHqwjRlSlVvBRARAT4FXvGX39LM4Y8B41S1SERuBm4CXmypbve3KY0xrqFKsDfodhaR/CaPCa1Um8zBmUrlIvKwiMwQkfEAIpIC1KtqwzFv4Zui2SJrsRljQiDB3qBbGMJ9bI8ATwKo6ihobMU9LyIbgbVASZPji/B1X1tkic0YEzSFsE6ZEpG7gOWquuCQ86iqiPwbOBVYwJFTNJvORT+CdUWNMSHxEBfwEQwRmQhUqOrMFg45H1iiqjVAoog0JLergE9aq9tabMaYoCkSloUmRWQkMBl4V0Re8Bc/6C/LAFKAxU1acpOAF0WkFN80zTtaq98SmzEmaL7t99qeNlT1c6C55ZbvaeH4FcCYYOu3xGaMCYFtmGyMiTGKzTwwxsQga7EZY2KKqliLzRgTW3yDB87vQhWIJTZjTAjEFXsaBOKKxLZ2WxfO/WVz817dJ+lk922w0prUTUducuNmnnJ37aNpDuUbPLBrbMaYGGMLTRpjYkq4Zh5EmiU2Y0xIbDMXY0xMUYU6ryU2Y0wM8XVFLbEZY2KMzTwwxsQUu93DGBODrCtqjIlBQe554ChLbMaYoPlGRW2uqDEmhtgNusaYmGRdUWNMTLFRUWNMTLJRUWNMTFEV6i2xGWNijXVFjTExxa6xtbM+XYu57ttfN74+ue8ennztfM4bspmcrCpqauPZXZTJ3+cNdS7IJnp0KeUnP/gSAI9XeOmt0xn3gy+Jj/eSmlzPtt3ZvDz7dGeDbGLiL5eTkKCkpNSzY3sGM18azNibVtO5SxUJCUpFRQJ5z52K1+Oubsodj2/D6xUyO9TzxYdZfPRmjtMhteqC0cV8+6oSvB5YszSd16d3dTqkIxzTiU1E4oHfAsNV9ZJInafB1r0d+d3r5wMQJ16euHkOGam11NQl8Pis7wDwg7PXcMJx+9m4q1OkwwlAGf/DJTz18rcoq0huLP3jjHMbn0+++RN6dy9h2+4ODsR3pOl/HNb4/O7J+fTsXcbMlwY3lo29aTVDT9/HsiXdnAivRc9M6e1/pvzhzQ2uTmyp6R4uGlPM/WNzAeHeP22lR24NOzclB/xse4mW+9gi+fN6BfA2DrQKv3PaJj77ui/VNQlkp1c3lnfMqGJI7p72DucIJ+UWsq8onZuvyef+CR9z2fnrDnk/I62G7Mxqig6kOhRhyzIyasnuUENJ8cEvW1KShz59y9i9M93ByFqXmKyUlbj7jvnBIypY9mkG+O8TWzgni9NGum8PCC8S8OG0iCUdVZ0NINL+/5KXnbmOX//1e9TWJ9C3ezGTf/QxVTWJFJWlkZLo/GYs3TuX0bdnMfc/fTF19Qn8ctwCtg3IZv+BNH46ahkn99/Dc7POoqLKPb/Ux/Us5yc3rWHQkP28+NypVJQnkZFRy4TbVzBoyH7eer0/O3dkOB1mi266bxevubBb11RWjoeykoNfybKSeHrm1jgY0ZFUod4WmmyZiEwAJgAkpXUMW73DB2xn1ZZu1Nb7/tXeXjiYtxf6ukxXf2sl+0vTwnauo1Vdm8DSVT2p88f4+Zd9OLFvIf/8YAiP5n2HuDgvD94yj9Ubu1LsgngBdu3I4HePnkFcvJdJDy5h7eociotSeOqJEYAy8Rdf0a9/CQUbOjgd6hFGj9/LhpWprM53b+IFKC2K5/iBBxNZZgcPpcXuuwx+rHdFW6Wqeao6QlVHJCaHrwtzzXmreHP+4CPK01NquHBoAYvX9m7mU+1r/ebOnNRvX+PrQf32UbD94LUfrzeOuDglMcHrRHit8np8sSUcEptQ7xFSUp1vDR/uihsLqa6MY96/3HttrcHa5ekMO68M39gjnPO9UlYudlf3vuEaW6BHMETkeRF5TkRmichP/GUXicg7IvKaiDzV5Nhmy1vivp+DNujfYz+FB9I5UNFwbUq5+5oFeFXITq/m6X+NpLo20dEYAYoOpJG/sicP/Hwe1bUJ7C7MoLwyiV9P+Jiq6kTSU2v5LL8ve4vc0cI4YUAxo6/dQHVVAmlpdSz4tAdehfse/ILKigSSk70UbMhm9dednQ71EINHVHDdbXtY8lEWA57YBsDLTx7HgSJ3/m9fURrPh//syJTnt+Kph4JVqWzbkOJ0WEfQMLXYVPVWAPFdr/pURGYCU4DLVLVGRB4RkYuBuc2Vq+oHLdUtqhqWIFs8gch7qnppa8dk5PTWU773y4jGES5JB9zXKmlN1G2Y/E2B0yEEL8LfnXCbq28sVdURbakjc2B3HTZ9XMDjPrvo91uAwiZFeaqa19yxIpICvArcB9yuqnf4y0cAVwMvN1euqr9u6fwR/+kKlNSMMdFDNehrbIUhJNFHgCeBTkBRk/Iif1lL5S1yZ5vcGONSgieMo6IichewXFUXiMhAoOlIYg6w3/9orrxFltiMMSEJ1zU2EZkIVKjqTH/RBmCIiCSrag1wFfBJK+UtssRmjAlauOaKishIYDLwroi84C9+EHgYmCki5cA+4D+qqiJyRHlr9VtiM8YET8MzZqKqnwN9mnlrnv9x+PHNlrfEEpsxJiRumDIViCU2Y0zQNMyDB5Fiic0YE5JouH3PEpsxJiThGhWNJEtsxpigqVpiM8bEoGhY3cMSmzEmJHaNzRgTUxTBa6OixphYEwUNNktsxpgQ2OCBMSYmRUGTzRKbMSYk1mILkniVxAqP02EEJe2bwsAHucg3Nx/ndAghGfBcldMhBK1+l/NbOYYkDF8xBbxeS2zGmFiiQDS32ETkOqC5HWY9qvpq5EIyxrhZtN/Hlkjzic39N7EYYyInmhObqr7S8FxEugKdVHVNu0RljHEpiYrBg4CtLxG5Ffg9cL//9W8iHZQxxsU0iIfDgulWnqyqNwDb/K+7RzAeY4ybKahXAj6cFsyoaMNW1A15ODtCsRhjooLziSuQYBLbHBF5E+gmIq8BcyIckzHGzVzQ1QwkYGJT1ddFZC7QH9isqvsiH5YxxrViIbGJSB98+//1AdaKyBOqGl233xtjwiNKbtANZvDgj8BfgCuBmcBTkQzIGONuqoEfTgvmGluBqi7zP18uIlE2Qc4YE1YuGPUMJJgWW43/Bl1EpC9QHdGIjDGuJhr44bTW5oq+638/Ffihv6XWHShop9iMMW7jkhtwA2ltStVl7RmIMSYaSFQMHgQzKtoTGAOk+4tUVR+PaFTGGPeK5hZbEw8AWcDfge8C5RGNyBjjbt7wVCMi8cBvgeGqeom/bC6woclhk1W1REROAx7Dl38qgQmqWtdS3cEktgqgVlXfAd4RkWeP8t8j4np0KeUnl30J+Fb5/N+3T+eCMwoYeHwhdfXxxMUp02aeS02t8+trTrznK1QhM6uOJZ93Y95/ejPq2o3kDjhAVWUClRUJzHhxkGMrKcSJl18MXcLJnQu5+YPLAbhz6BK6pVWQGOelrC6Jx74YiUfjSImv4+7hX5CVVEOtJ553N/Vn0e6ejsQNMHHSSlTF97dd0IV57/Vk3C3rycyuIznFw5aNGbz5Sj/H4mvN7Y9uJSFRSUnzsr0gmVee6uF0SIcK731sVwBvA2cdcgrVW5o59jFgnKoWicjNwE3Aiy1VHMw3vBYoFpFrgHeBnGAiFpHn8eX2HOCdpssgRYYy4eol/GHGuZRV+qa3ZqTWMHzQTqY8830Arr/kK0YM2sGCr46PbChBmP6H0/zPlKnPzWfdmo4c36+UaY+eDsCwM/Zy5sjdLF7gzNLeF/Tawofb+nJal72NZX/68ozG53cOXcI5x+1g/s7e3HX6Ev61YSBrijo7EeoRpk8d4n+mTM1bzLz3ejLjhRMb3//tn5bwzht9qKl2/gfucM/e36fx+a+mbaZXv2q2F6S08on2F+SoZ2cRyW/yOk9V85oeoKqzAUQOSZTlIvIw0Bf4VFVfFJEUoF5Vi/zHvAX8iTYmtueAInwZ8yZgWhCfQVVv9QctwKdARBPbSX33sbc4nfFX55OaUseXa4/jnfkDKTqQSk52JeWVSXTLKeedzwZGMoyQJSZ5KS9Noq42jrT0Onw/iUJWh1r69it1LLF9uC23xfeS4+s5oUMxszcOAJReGaV8//gCfjp4BXur0nh6+RnUeZtbo7R9+f62iYeVKqpQW+N8fK3JyK4nu1M9xYXuS75BXmMrVNURIVetOgoa88bzIrIRWAuUNDmsiAANrGDmiu7wP70r1CD9kv2BRFT3zuXk9ijm/mcvprY+gbvGzmfrng68t+BERn1nNaXlKazc2I3SCnf9+t0wfg1v/L0/+/ak8ckHvfjF5C+prExg1/Z0klPctcFNVlIN95+5gGFdd/PSqlPZUtaBnJQqTu2yl98u/hZ7KjO4PHcD44d8yfQVw50Olxtu/YY3/nZol/Oq6zfzwb97uXaxxB59qxl3zy4GD6/ghYd6UVHqwsTWDlRVReTfwKnAAqBjk7dzCJBTgrmP7XB1qnp5CDE+AjzZTP0TgAkAyakdQqiueTW1CeSv7kltvS/kBV8dz7CBO0lO9PDiv3xdqPOGbeLyb63lnfkntfl84TDq2o1sXJ/Nmq87AfD5pz34/FPfNZUzztlNQmKYrtKGSWltMpPmXwgovzlrPoNyCtlSms2a/Z3ZU5kBwNytfZn27bnOBgqMun4TG9dlsmbFwe/Dty7aRUKCMn+ue3fu2rk5hal35BIXr0x5bhNrl6VTvO/wVqez2vEG3POBt1W1RkQSRaSjqhYDVwGftPbBiN7HJiJ3ActVdUEz9ecBeQCZHXq1+U+1bktnvn/ON42vB+fuZVVBN4YN3NlYVlOXQPfO+9t6qrC4fPQmqqvj+fiD3ke8l5Do4QdjNvH0E0PbP7CgCHUaR1pCHZX1icTHeUlNqKOqPpHTuuxhbVEnR6O7fMwW39/2/YMDGGefv4c+ueX8/cUBDkYWPK9HiItXEhJddm+FEokpVY2jmyLyByAD3zqQi5vkjknAiyJSCtQDd7RWYcTauSIyEahQ1ZmROkdTRQfSWLK6Jw+O/4jqmgR2789kyaqenDpgF1N+9jF1dfEkJ9fzzKxz2iOcVg0aUsSYn3xD/sJu3ParrwCY8ZeTuHJMAenpdWR1qOWNV/qzf1+qw5FCvdc3665bWjmTRiyivC6RlHgPa4o6sXSvr+Xz9PIz+N15H7G/OpU4lMeXjHQs3kGnFjPmxgLyF3ThtskrAXjtpRO4/dcrWfhxt8ay2bP6sn1LhmNxNqf/kEqunrCH6op40jI9zH+3I/t2Jjkd1pHCnGtV9dImz+9p4ZgV+O6nDYpoBKbii8hI4B/4RlEbPNjSWm6ZHXrpsPPuDHsckZC+JrqWo4u+DZO3OB1C0KJtw+S5nleXHs0F/aaSe/fWXncFvtxecM89bT5XWwTVYvOPUHTw928DUtXP8a3fZoyJNS7rHTcnmF2qRgPv4L9nRERuj3RQxhgXi5Fdqi71DyQ0XJkfFMF4jDEuFsySRa5etqiJhpupGsLNjFAsxphoEAULTQaT2FaLyHSgr4g8BayLcEzGGBdzQ4sskGBmHjwjIoOBk4ENqro88mEZY1wrChJbMIMHZwEdgB1Aqv+1MeZYFEPX2C4G4v2Ps4CtwOJIBmWMcTEXJK5AgumKPtL0tYj8OXLhGGPcTtw1hblZwdzuEY7PGGNMuwlmz4MXOJjMegBrIhqRMcbdYqErim/ZoYZV+Q6oaknkwjHGuJpLBgcCCSaxTVXVsRGPxBgTHaIgsQVzvWyTiDi/1o8xxh2iYK5oayvoDlHVlcAA4BERaQi5RlWvaK8AjTHuIUTHqGhrXdE78e3dd117BWOMcbkYuMaWJiLNbWroUdXoWmHPGBM+UZ7YzsA3Inr4VP56YHzEIjLGuFuUJ7ZPVbVdElhcZS3py7a1x6nazFMU1CLCrjHg+XqnQwhJ6ZlHbm7jVmmzj82OS7R3RTe3VxDGmCgSzYlNVR9tz0CMMVFAo39U1BhjjhTNLTZjjGlOtF9jM8aYI1liM8bEFJdMmQrEEpsxJmiCdUWNMTHIEpsxJvZEQWKzZb6NMaEJ07JFIhIvIo+KyPtNyi4SkXdE5DX/PsatlrfEEpsxJnjh3X7vCuBt/D1HERFgCnC1ql4LVIrIxS2Vt1axJTZjTGjC1GJT1dmq2nQrzxOB1apa43/9FnBBK+UtsmtsxpiQBDmlqrOI5Dd5naeqeQE+0wkoavK6yF/WUnmLLLEZY0ISZFezUFVHhFj1fqBjk9c5/rKWyltkXVFjTPCC6YYe/ajpBmCIiCT7X18FfNJKeYusxWaMCU34b/eoA1BVj4g8DMwUkXJgH/AfVdXmylur0BKbMSZokZh5oKqXNnk+D5jXzDHNlrckphLbxCmrUS9kZtWxZH4X5r3n27Jh1NjNXHjZTu4cO9LhCJt3XJ9qfnTbTgC8XmHGtJ4U7U1yOKpDTbzva1TF97dd0IV57/fi0WcWsXN7euMxLz13EhXliQ5GCX26lfDDC75ufD0kdw9P/v181mzpSnycl/tvmEdlTSK/n3W+g1G27PZHt5KQqKSkedlekMwrTzW37YizxOv+O3QjlthEZDqQCKQD61X1vyN1rgbTHx/sf6ZM/csXzHuvB4NOLWbHlnTKDjj7hWuZ8rNJ23j617mUH3Dv78z0J0/xP1Om/nkh897vBcBzU09p+UMO2LqnA3/4x3kAxImXx3/+H9Zs6QLADZcs471FA7nw9I1OhtiqZ+/v0/j8V9M206tfNdsLUhyM6DDH+iR4VZ3Y8FxEXhaRgaq6LlLnayoxyUu5P5GtWeEbTBn1483tceqQnXhaBft2JvHTe7eRmu5lxaJM3n+1q9NhtSgxyUt5qa81WVWVwE9+vo5ux1WxcnkOc2b3CfDp9vXtYZuYv+J4QLhoxAbWbe3Ctr3ZTocVlIzserI71VNc6L4fO5srCohIR6AL0G47X9wwcQNv/C23vU7XJt161XD8wCr+++YTqauN4/aHN3NyQRmrlmQ6HVqzbrhlHW/M6AfAI/c1jOYrt01aya7thaxY2tm54A5z6dnreSDvYgb0KqRTViVz80+le06Z02G1qkffasbds4vBwyt44aFeVJS6L7FFQ4stYrd7iEh/EZkJLMN3c17JYe9PEJF8Ecmv9VaF7byjfryZjesyWfNVx8AHu0BNVRzL52dTV+v7T7FobgcGDKlwOKrmjfpRARvXZbNmRc5h7whfzO9K7gD3JI3hA3ewelNXausT+O7wjfTuVsI9P/qM8VcuYcgJexh13iqnQ2zWzs0pTL0jl5+edzIXji6iY5c6p0M6QhinVEVMJLuiG4CxIpIAzBKRRaq6u8n7eUAeQHZS17D8KS7/4Vaqq+L5+D33XXBtyYav07n4msLG1ycNLefrL7IcjKh5l1+zmerqBD6e07PZ94cMLWLRZ93aOaqWXX3+Kp78u2+A4IXZZzWWd88p48ZLlvHWZyc7FVpQvB4hLl5JSHRBljicC0M6XMTbuapaLyLxQESH+QadWsyYmzaRP78zt03x/RrPeH4ApSW+09bXu/Ne5KJ9SSz9LJvJT2+guiqePduT+GqhuxLboFOKGHPDRvI/78ptk3wjjjP+fCI/vGEjqWkeEpM8rF/VoZmWnDNO6LmffQfSOFBx5EV3r1eo97rz/4X+Qyq5esIeqiviScv0MP/djuzb6a7R8WjZpUpUw59+ReR04G6gHMgC3lHVmS0dn53UVUd2uS7scURCtG2YHN+1i9MhhKR0RPMtQjdKm50f+CAXmet5delRTHM6REan3jrk0rsCHrd45j1tPldbRKTFpqrLgJ9Eom5jjMMi0BgKNxcOuRhj3MwNgwOBWGIzxgTvWL9B1xgTm6Jh8MASmzEmJJbYjDGxRbHBA2NM7LHBA2NM7LHEZoyJJZFYaDISLLEZY4KnemwvNGmMiVHuz2uW2IwxobGuqDEmtihgXVFjTMxxf16zxGaMCY11RY0xMcdGRY0xscVW9wie1tVTv7vdNrFqmyiYJ9dU/bbtTocQkrQdu5wOIWgJPbo7HUJotrW9Ct8Nuu7/DrgisRljooit7mGMiTXWYjPGxBa7xmaMiT3hmSsqIsuBxf6X9cAdqqoichFwF1ABbFfVu4+mfktsxpjQhKcrul9Vb2laICICTAEuU9UaEXlERC5W1Q9CrdydO8caY9zJv2FyoEcQ4kXkcRGZKSKj/GUnAqtVtcb/+i3ggqMJ01psxpjQBNdi6ywiTXeUzlPVvINV6AUAIpIIvC4iq4BOQFGTzxT5y0Jmic0YE5rgeqKFwewEr6p1IvIBcDKwBujY5O0cYP/RhGhdUWNMSMTrDfgI0TnAl8AGYIiIJPvLrwI+OZoYrcVmjAmeEpYbdEXkZaAKyADeUtXN/vKHgZkiUg7sA/5zNPVbYjPGBE3QsNygq6o3tlA+D5jX1votsRljQmMzD4wxMccSmzEmpoTpGlukWWIzxoTkKEY9250lNmNMCNS6osaYGKNYYnPSHY9vw+sVMjvU88WHWXz0Zo7TIbXogtHFfPuqErweWLM0ndend3U6pBbFxSk33LubAadWcf/Yfk6HE9Dtj24lIVFJSfOyvSCZV57q4XRIR5g4aSXqhcysOpYs6Mq893sydvx6OnWtJiFRqSxPIG/aILwel9xP7/6eaOQSm4gkAH8DylT155E6T0uemdLb/0z5w5sbXJvYUtM9XDSmmPvH5gLCvX/aSo/cGnZuSg74WSecdXEpC/+TxUmnVzodSlCevb9P4/NfTdtMr37VbC9IcTCiI02fOsT/TJmat4h57/dk5osnNr4/dvx6hp6xn2WLujgT4GGiYaHJSP4EPAC8BMRH8BwBJSYrZSWOhtCqwSMqWPZpBr7V5GHhnCxOG1nubFCtWDgnm3XL050OI2QZ2fVkd6qnuNC9nZTEJC/lBxIPKUtK9tCnXzm7d6Q5FFUzVAM/HBaR/8oi8mMgH1gfifpDcdN9u3jNxV27rBwPZSUH/zOUlcTTM7emlU+YUPToW824e3YxeHgFLzzUi4pS9ya2G25ZzxszfN37jMw6xt+9mkGnFDN7Vi47t7nkx0QVPO7vi4a9xSYiw4Duqvp/AY6bICL5IpJfR2S+yKPH72XDylRW52dEpP5wKC2KJyPb0/g6s4OH0mL3fvmizc7NKUy9I5efnncyF44uomOXOqdDatao6zexcV0Wa1b4LpmUlyUy7aHTmDDm2/TpV0a/AaUOR9hEFLTYItEV/REwUEReAB4FzhWRiYcfpKp5qjpCVUckEv7rSVfcWEh1ZRzz/uXOa2sN1i5PZ9h5ZTSsBXPO90pZudglv84xxOsR4uKVhETnv3SHu/yaLVRXxfPxnJ7NvCvU18eRklbf7nG1KAoSW9ibBqo6qeG5iPQFHlDV6eE+T2sGj6jgutv2sOSjLAY84dtM8eUnj+NAkftaQhWl8Xz4z45MeX4rnnooWJXKtg3uurjdnPo6cTqEgPoPqeTqCXuorognLdPD/Hc7sm9nktNhHWLQKcWMuXEj+Z934bbJXwPw6l/787M71lJZkUBSipeC9Zms/solP9AKRMFO8KIRzK4i0hu4//C1zQ+XJTl6VtxFEYsjrFzwaxTT4tw70HO4aNsw+f1tTy8NZvHH1mQnd9ORPcYGPtfmaW0+V1tEtAmjqtuAVpOaMSaKKFExeOC+vpkxxt2ioNdiic0YExpLbMaY2OKOUc9ALLEZY4KngC1bZIyJOdZiM8bEluiYUmWJzRgTPAVVS2zGmFgTBTMPLLEZY0Jj19iMMTFF1UZFjTExyFpsxpjYoqjHE/gwh1liM8YEL0qWLbLEZowJjd3uYYyJJQpomFpsIjIWuA7wAAtV9cmwVIwlNmNMKFTD0mITkUxgHHCpqqqIzBCRAar6TZsrxxKbMSZEYRo8GAl8oAeX8J4NXACEJbFFdGnwoIMQ2QdsiUDVnYHCCNQbKdEUbzTFCtEVb6RiPV5V27Trsoi8jy++QFKA6iav81Q1r0k9PwaSVfV//a8vBM5S1cfbEl8DV7TY2vrHbomI5Du57nqooineaIoVoiteN8eqqpeEqar9wMlNXuf4y8IikjvBG2NMSxYDF4lIw3ZnVwKfhqtyV7TYjDHHFlUtEZEZwCwRqQe+VNW14ao/1hNbXuBDXCWa4o2mWCG64o2mWI+aqs4CZkWiblcMHhhjTDjZNTZjTMyxxGaMiTkxe40tktM1wk1E4oHfAsPDOJweMSLyPODFN0T/jqq+4nBILRKR6UAikA6sV9X/djai1olIAvA3oExVf+50PNEqJhNbpKdrRMAVwNvAWU4HEgxVvRXAP1T/KeDaxKaqExuei8jLIjJQVdc5GVMADwAvAdc6HEdUi8nERoSna4Sbqs4GOHhLT9RIBoqcDiIYItIR6ALscTqWlvjvxs8H1jsdS7SL1WtsnTj0C1fkLzPh9Qjg2i4+gIj0F5GZwDJ803pKHA6pWSIyDOiuqv/ndCyxIFYT236gY5PXYZ2uYUBE7gKWq+oCp2NpjapuUNWxwABgrIh0dzqmFvwIGCgiLwCPAueKyMQAnzEtiNWu6GLgFyIyzd8dvRJ4zOGYYob/C1ehqjOdjiVYqlrvH6RJcjqW5qjqpIbnItIXeEBVpzsXUXSLycQW6ekaEVTndACBiMhIYDLwrr91AfCgqu5zMKxmicjpwN1AOZAF/FNVtzobVVA8QL3TQUQzm3lgjIk5sXqNzRhzDLPEZoyJOZbYjDExxxKbMSbmWGIzxsQcS2xRSES2ishfRCRPRJ713591NPW85//nFSLyo1aOC+lueBGZIiLnHVY2TkSub+UzD4rIOSGc471QYjLHlpi8j+0YsF5VbwYQkcnAxcD7R1FPIkAQ03hSQqw33v84vKy1e4ua+0xrEkOMyRxDLLFFv1xgjog8CKQCJwGTgGHAJfi2QFulqs/5J4LnAbvx/bfvBr7WFFCvqrP8LbeLgQP4ZnD0BE4UkT8Cv8c3Ve0+fNvDxQN3qapHRJ7GtzRQFXAiML+lgEXkF0AfQIClTWYwjBORK4D+wNuq+jcR6Q08DJQAacA9qlrWtj+ZiXWW2KLTYBF5GV+r5V1VXS4iV+Fbw+tqEckBrlfV0QD+ZZv+CdwI/E1V/y0i6RzcyzUeUBHpD3xfVX/a9GQicpmq/tL/PA/4oapWiMgtwCgR2Y4vMTa0IgN1EzfhS8ClwC1AQ2L7RlV/LyJxwGci8ndgKnCfqm4XkUuACcAfjuqvZo4Zltii02pVvbGZ8oX+f/YHuonIE/7X8fhWN+kLvAbgT0yHr0s2tEkdLekPPOhfYikbWOCvd2WTY5a19GH/KhY34ku8tSKyqMnbX/pj84pIgT/mE4Db/edLAXYEiM8YS2wxpmF+4WZgp6pObvqmP5GdAmwSkWxg8GGfXwE8xJG7JHlFJF5VPUAB8JCqVjWpdyhwQ5PjzwY+aCHG/sCH/qQ2FN/KKw3OBeb6F7A8Dl93dyswTVVdu46acR9LbNGptpkyj/+Bqu4VkfdFZBa+a2V7VPU3wIvAsyJyEb4J9183/ayqrheR9/zrlxUCn6vqq8AnwMv+Se8PATNEpBBfV/gOVf1SREaLyJ+BGnytKk8L8c0BXhCRk/ANJixr8n5HEZkK9AKe9l+7e8AfcxG+UfyH/RPZXb9ggHGOTYI3xsQcu4/NGBNzLLEZY2KOJTZjTMyxxGaMiTmW2IwxMccSmzEm5lhiM8bEnP8HBZSK37Rxhs8AAAAASUVORK5CYII=\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "class_prediction_compi =  np.argmax(predicition_compi, axis= 1)\n",
    "cm = confusion_matrix(X_test.label, class_prediction_compi, labels=[0, 1, 2, 3, 4])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1, 2, 3, 4])\n",
    "\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83914daf",
   "metadata": {
    "id": "intended-cylinder",
    "papermill": {
     "duration": 3.633537,
     "end_time": "2021-05-30T20:25:41.743104",
     "exception": false,
     "start_time": "2021-05-30T20:25:38.109567",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 추가 테스트 데이터 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "11cf8fc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                              filepath  label\n0    /home/Lee_KneeData//0/5a21060fe4b0f1e9e5622e32...      0\n1    /home/Lee_KneeData//0/5a21060fe4b0f1e9e5622e32...      0\n2    /home/Lee_KneeData//0/5a21112ee4b09b69e603db16...      0\n3    /home/Lee_KneeData//0/5a21112ee4b09b69e603db16...      0\n4    /home/Lee_KneeData//0/5a28d27ae4b028f6846ae00a...      0\n..                                                 ...    ...\n526  /home/Lee_KneeData//4/5b0e3cede4b07f99d60ee44d...      4\n527  /home/Lee_KneeData//4/5b0f64cde4b0b80049bd1e8e...      4\n528  /home/Lee_KneeData//4/5b10dab8e4b07f99d61002a5...      4\n529  /home/Lee_KneeData//4/5b10dab8e4b07f99d61002a5...      4\n530  /home/Lee_KneeData//4/5b1f54b4e4b099a4cf79f0ae...      4\n\n[531 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filepath</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/home/Lee_KneeData//0/5a21060fe4b0f1e9e5622e32...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/home/Lee_KneeData//0/5a21060fe4b0f1e9e5622e32...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/home/Lee_KneeData//0/5a21112ee4b09b69e603db16...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/home/Lee_KneeData//0/5a21112ee4b09b69e603db16...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/home/Lee_KneeData//0/5a28d27ae4b028f6846ae00a...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>526</th>\n      <td>/home/Lee_KneeData//4/5b0e3cede4b07f99d60ee44d...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>527</th>\n      <td>/home/Lee_KneeData//4/5b0f64cde4b0b80049bd1e8e...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>528</th>\n      <td>/home/Lee_KneeData//4/5b10dab8e4b07f99d61002a5...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>529</th>\n      <td>/home/Lee_KneeData//4/5b10dab8e4b07f99d61002a5...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>530</th>\n      <td>/home/Lee_KneeData//4/5b1f54b4e4b099a4cf79f0ae...</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>531 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_class = 5\n",
    "test_root_path = \"/home/Lee_KneeData/\"\n",
    "test_image_path_list = []\n",
    "test_label_list = []\n",
    "\n",
    "for label in range(n_class):\n",
    "    test_image_list = os.listdir(f\"{test_root_path}/{label}\")\n",
    "    test_image_path_list += [ f\"{test_root_path}/{label}/\"+ path for path in test_image_list]\n",
    "    test_label_list += [label] * len(test_image_list)\n",
    "\n",
    "df_test = pd.DataFrame({\"filepath\" : test_image_path_list, \"label\": test_label_list})\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b8563088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(531, 2)"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6223be",
   "metadata": {
    "id": "intended-cylinder",
    "papermill": {
     "duration": 3.633537,
     "end_time": "2021-05-30T20:25:41.743104",
     "exception": false,
     "start_time": "2021-05-30T20:25:38.109567",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 추가 테스트 데이터 분포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e6f6a6d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Text(0, 0.5, 'count')"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_test.label.value_counts().plot.bar()\n",
    "plt.xlabel(\"label\")\n",
    "plt.ylabel(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "469679e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 42\n",
    "test_da = train_test_split(\n",
    "    df_test,\n",
    "    test_size = 0.1,\n",
    "    random_state = seed,\n",
    "    stratify = df_test.label\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ddf19c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 531 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "BS = 1\n",
    "image_size = 224\n",
    "seed = 42\n",
    "\n",
    "da_test_generator = valid_aug.flow_from_dataframe(\n",
    "    dataframe = df_test,\n",
    "    directory = test_root_path,\n",
    "    x_col = \"filepath\",\n",
    "    y_col = 'label',\n",
    "    batch_size = 1,\n",
    "    seed = seed,\n",
    "    shuffle = False,\n",
    "    class_mode = None,\n",
    "    target_size = (image_size, image_size)\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "499563c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "531/531 [==============================] - 5s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "predicition_test = xception_model.predict(da_test_generator, steps= da_test_generator.n/ BS, verbose= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e7d6147e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f5d7c494550>"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "class_prediction_test =  np.argmax(predicition_test, axis= 1)\n",
    "cm_test = confusion_matrix(df_test.label, class_prediction_test, labels=[0, 1, 2, 3, 4])\n",
    "disp_test = ConfusionMatrixDisplay(confusion_matrix = cm_test, display_labels=[0, 1, 2, 3, 4])\n",
    "\n",
    "disp_test.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-wilson",
   "metadata": {
    "id": "positive-wilson",
    "papermill": {
     "duration": 3.5306,
     "end_time": "2021-05-30T20:26:20.277590",
     "exception": false,
     "start_time": "2021-05-30T20:26:16.746990",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## DPhi에서 제공한 테스트 데이터 세트에 대한 예측 및 .csv파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "constitutional-catholic",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-30T20:26:28.031789Z",
     "iopub.status.busy": "2021-05-30T20:26:28.031111Z",
     "iopub.status.idle": "2021-05-30T20:26:28.058944Z",
     "shell.execute_reply": "2021-05-30T20:26:28.058161Z",
     "shell.execute_reply.started": "2021-05-27T09:08:20.550041Z"
    },
    "id": "constitutional-catholic",
    "outputId": "5f5f47f4-62db-4337-c3f4-043cc4d7d398",
    "papermill": {
     "duration": 4.034446,
     "end_time": "2021-05-30T20:26:28.059074",
     "exception": false,
     "start_time": "2021-05-30T20:26:24.024628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1958 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(compi_root_path + \"Test.csv\")\n",
    "\n",
    "test_generator = valid_aug.flow_from_dataframe(\n",
    "    dataframe= test,\n",
    "    directory= compi_root_path + \"test\",\n",
    "    x_col= \"filename\",\n",
    "    y_col= None,\n",
    "    batch_size= 1,\n",
    "    seed= 42,\n",
    "    shuffle= False,\n",
    "    class_mode= None,\n",
    "    target_size= (224,224)\n",
    ")\n",
    "\n",
    "STEP_SIZE_TEST=test_generator.n//test_generator.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "blocked-niagara",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-05-30T20:26:34.994605Z",
     "iopub.status.busy": "2021-05-30T20:26:34.994000Z",
     "iopub.status.idle": "2021-05-30T20:26:56.340759Z",
     "shell.execute_reply": "2021-05-30T20:26:56.341118Z",
     "shell.execute_reply.started": "2021-05-27T05:51:39.116529Z"
    },
    "id": "blocked-niagara",
    "outputId": "77c22b06-e93f-4228-8b21-57554d52e207",
    "papermill": {
     "duration": 24.791614,
     "end_time": "2021-05-30T20:26:56.341303",
     "exception": false,
     "start_time": "2021-05-30T20:26:31.549689",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1958/1958 [==============================] - 18s 9ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "0    967\n2    517\n1    239\n3    219\n4     16\nName: label, dtype: int64"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = xception_model.predict(test_generator,steps=STEP_SIZE_TEST,verbose=1)\n",
    "df_submit = pd.DataFrame({\"label\":np.argmax(pred, axis= 1)})\n",
    "df_submit[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "liquid-tsunami",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-30T20:27:26.290338Z",
     "iopub.status.busy": "2021-05-30T20:27:26.289493Z",
     "iopub.status.idle": "2021-05-30T20:27:26.809425Z",
     "shell.execute_reply": "2021-05-30T20:27:26.808879Z",
     "shell.execute_reply.started": "2021-05-27T05:51:46.57395Z"
    },
    "id": "liquid-tsunami",
    "papermill": {
     "duration": 4.073647,
     "end_time": "2021-05-30T20:27:26.809566",
     "exception": false,
     "start_time": "2021-05-30T20:27:22.735919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "xception_model.save_weights(\"knee_xray_Xceptionnet_GPA.h5\")\n",
    "df_submit.to_csv(\"submission.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "million-monaco",
    "rotary-prague",
    "hairy-greene",
    "dense-enzyme",
    "asian-chest",
    "marked-belarus",
    "pretty-morris",
    "intended-cylinder"
   ],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "[MDL]KLGrade.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6499.620651,
   "end_time": "2021-05-30T20:27:40.898205",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-05-30T18:39:21.277554",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}