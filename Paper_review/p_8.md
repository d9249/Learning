# Medical-Deep-Learning
Kyonggi Univ. 2021. 02. BE530_0058.

## Paper name

Accounting for Dependencies in Deep Learning Based Multiple Instance Learning for Whole Slide Imaging

> 전체 슬라이드 이미징에 대한 딥러닝 기반 다중 인스턴스 학습의 종속성 설명

## Presentation Date

2021.11.24

## Abstract

Multiple instance learning (MIL) is a key algorithm for classification of whole slide images (WSI).

> 다중 인스턴스 학습(MIL)은 전체 슬라이드 이미지 분류를 위한 핵심 알고리즘이다.

Histology whole slide images can have billions of pixels, which create enormous computational and annotation challenges.

> 조직학 전체 슬라이드 이미지는 수십억 개의 픽셀을 가질 수 있으며, 이는 엄청난 계산 및 주석 문제를 일으킨다.

Typically, such images are divided into a set of patches (a bag of instances), where only bag-level class labels are provided.

> 일반적으로 이러한 이미지는 패치 세트(인스턴스 가방)로 나뉘며, 여기서 백 레벨 클래스 레이블만 제공됩니다.

Deep learning based Multiple instance learning methods calculate instance features using convolutional neural network (CNN).

> 딥 러닝 기반 다중 인스턴스 학습 방법은 CNN(Convolutional Neural Network)을 사용하여 인스턴스 특징을 계산한다.

Our proposed approach is also deep learning based, with the following two contributions: 

> 우리가 제안한 접근 방식도 딥 러닝 기반이며, 다음과 같은 두 가지 기여가 있다.

Firstly, we propose to explicitly account for dependencies between instances during training by embedding self-attention Transformer blocks to capture dependencies between instances.

> 첫째, 인스턴스 간의 의존성을 포착하기 위해 자기 주의 트랜스포머 블록을 내장하여 훈련 중에 인스턴스 간의 의존성을 명시적으로 설명할 것을 제안한다.

For example, a tumor grade may depend on the presence of several particular patterns at different locations in whole slide images, which requires to account for dependencies between patches.

> 예를 들어, 종양 등급은 전체 슬라이드 이미지의 다른 위치에 있는 몇 가지 특정 패턴의 존재에 따라 달라질 수 있으며, 패치 간의 의존성을 고려해야 한다.

Secondly, we propose an instance-wise loss function based on instance pseudo-labels.

> 둘째, 인스턴스 유사 레이블을 기반으로 인스턴스별 손실 함수를 제안한다.

We compare the proposed algorithm to multiple baseline methods, evaluate it on the PANDA challenge dataset, the largest publicly available whole slide images dataset with over 11K images, and demonstrate state-of-the-art results.

> 제안된 알고리즘을 여러 기준 방법과 비교하고, 11K 이상의 이미지가 있는 공개적으로 사용 가능한 가장 큰 전체 슬라이드 이미지 데이터 세트인 PANDA 챌린지 데이터 세트에서 평가하고, 최첨단 결과를 시연한다.

**Keywords**: Multiple instance learning, Histopathology, Transformer, Whole slide imaging, Self-attention

> 키워드: 다중 인스턴스 학습, 조직병리학, 트랜스포머, 전체 슬라이드 이미징, 자기 주의

## 1. Introduction

Whole slide images (WSI) are digitizing histology slides often analysed for diagnosis of cancer [3]. 

WSI can contain several billions pixels, and are commonly tiled into smaller patches for processing to reduce the computational burden (Fig. 1). Another reason to use patches is because the area of interest (tumor cells) occupies only a tiny fraction of the image, which impedes the performance of conventional classifiers, most of which assume that the class object occupies a large central part of the image.

Unfortunately, patch-wise labels are usually not available, since the detailed annotations are too costly and time-consuming.

![image-20211117120937696](C:\Users\dodo9\AppData\Roaming\Typora\typora-user-images\image-20211117120937696.png)

Fig. 1. An example of patch extraction from WSI from the PANDA challenge dataset [2]. 
We tile the image and retain only the foreground patches, out of which we take a random subset to form a bag.

An alternative to supervised learning is weakly-supervised learning, where only a single label per WSI is available.

Multiple Instance Learning (MIL) is a weakly supervised learning algorithms, which aims to train a model using a set of weakly labeled data [5,13]. 

Usually a single class label is provided for a bag of many unlabeled instances, indicating that at least one instance has the provided class label. 

It has many applications in computer vision and language processing [4], however learning from bags raises important challenges that are unique to MIL. 

In context of histopathology, a WSI represents a bag, and the extracted patches (or their features) represent instances (we often use these notations interchangeably).
With the advent of convolutional neural networks (CNN), deep learning based MIL has become the mainstream methodological choice for WSI [10].

Campanella et al. [3] was one of the first works to conduct a large study on over 44K WSI, laying the foundation for MIL applications in clinical practise. 

Since the instance labels are not known, classical MIL algorithm usually selects only one (or a few) instances based on the maximum of the prediction probability at the current iteration. Such approach is very time consuming, as all patches need to be inferenced, but only a single patch contributes to the training of CNNs at each iteration. Ilse et al. [14] proposed to use an attention mechanism (a learnable weights per instance) to utilize all image patches, which we also adopt.

More recent MIL methods include works by Zhao et al. [18], who proposed to pre-train a feature extractor based on the variational auto-encoder, and use a graph convolutional network for final classification.

Hashimoto et al. [7] proposed to combine MIL with domain adversarial normalization and multiple scale learning. Lu et al. [11] precomputed patch-level features (using pretrained CNN) offline to speed up training, and proposed an additional clustering-based loss to improve generalization during MIL training. Maksoud et al. [12] proposed to use a hierarchical approach to process the down-scaled WSI first, followed by high resolution processing when necessary. Such approach demonstrated significant reduction in processing time, while maintaining the baseline accuracy.

We observed that most MIL methods assume no dependencies among instances, which is seldom true especially in histopathology [10]. 

Furthermore, a lack of instance-level loss supervision creates more opportunities for CNNs to overfit. 

In this work, we propose a deep learning based MIL algorithm for WSI classification with the following contributions:
– we propose to explicitly account for dependencies between instances during training. We embed transformer encoder [15] blocks into the classification CNN to capture the dependencies between instances.
– we propose an instance-wise loss supervision based on instance pseudo-labels.

The pseudo-labels are computed based on the ensemble of several models, by aggregating the attention weights and instance-level predictions.

We evaluate the proposed method on PANDA challenge [2] dataset, which is currently the largest publicly available WSI dataset with over 11000 images, against the baseline methods as well as against the Kaggle challenge leaderboard with over 1000 competing teams, and demonstrate state-of-the-art (SOTA) classification results.

## 2. Method

MIL aims to classify a bag of instances H = {h 1 , . . . , h K } as positive if at least one of the instances h k is positive. 

The number of instances K could vary between the bags. Individual instance labels are unknown, and only the bag level label Y = [0, 1] is provided:

![image-20211117121033774](C:\Users\dodo9\AppData\Roaming\Typora\typora-user-images\image-20211117121033774.png)

which is equivalent to Y = max k {y k } definition using a Max operator. 

Training a model whose loss is based on the maximum over instance labels is problematic due to vanishing gradients [14], and the training process becomes slow since only a single patch contributes to the optimization. Ilse et al. [14] proposed to use all image patches as linear combination weighted by attention weights. 

Consider H ∈ R M ×K to be instance embeddings, e.g. features of a CNN final layer after average pooling. Then a linear combination of patch embeddings is

![image-20211117121047033](C:\Users\dodo9\AppData\Roaming\Typora\typora-user-images\image-20211117121047033.png)

where the attention weights of patch embeddings are a = sof tmax(tanh(HV)w) where w ∈ R L×1 and V ∈ R M ×L are parameters. The attention weights are computed using a multilayer perceptron (MLP) network with a single hidden layer.

### 2.1 Dependency Between Instances

The assumption of no dependency between the bag instances often does not hold.
For example, for grading the severity of prostate cancer, pathologists need to find

![image-20211117121127882](C:\Users\dodo9\AppData\Roaming\Typora\typora-user-images\image-20211117121127882.png)

Fig. 2. Model architecture overview. The backbone CNN (blue) extracts features at different scales, which are spatially averaged-pooled before feeding into the transformer encoder layers (green), to account for dependencies between instances. 
The input to the network is B × N × 3 × W × H. Where B is the batch size, N is the number of instances (patches extracted from a single whole slide image), and 3 × W × H is the spatial patch size. (Color figure online)

two distinct tumor growth patterns in the image and assign Gleason scores to each [1]. 

Then the International Society of Urological Pathology (ISUP) grade is calculated, based on the combination of major and minor Gleason patterns.

ISUP grade indicates a severity of the tumor and plays a crucial role in treatment planning. 

Here, we propose to use the self-attention to account for dependencies between instances. 

In particular, we adopt the transformer, which was initially introduced to capture long range dependencies between words in sentences [15] and later applied to vision [6]. 

Whereas traditional convolutions are local operation, the self-attention block of Transformers computes attention between all combinations of tokens at a larger range directly.

A key component of transformer blocks is a scaled dot product self-attention which is defined as sof tmax(QK T / √ d)V , where queries Q, keys K, and values V matrices are all derived as linear transformations of the input (in our case the instance features space H). 

The self-attention is performed several times with different, learned linear projections in parallel (multi-head attention). 

In addition to self-attention, each of the transformer encoder layers also contains a fully connected feed-forward network and layer normalization (see Fig. 2) [6,15].

We propose two variants of utilizing transformers. 

In the simplest case we attach a transformer encoder block only to the end of the backbone classification CNN after avg pooling. 

The idea is similar to the approach proposed in Visual transformers, but before avg pooling [6]. The difference here is that in Visual transformers, the goal was to account for dependencies between the spatial regions (16px × 16px) of the same patch. 

Whereas we want to account for the dependencies among the patches. Another relevant work was proposed by Wang et al. [16] to utilize self-attention within MIL, but for text-based disease symptoms classification. 

We maintain the dimensionality of encoded data, so that the input, output and hidden dimensionality of the transformer encoder are the same. We call it Transformer MIL.

We also consider a variant of a deeper integration of the transformer with the backbone CNN. 

We attach separate transformer encoder blocks after each of the main ResNet blocks [8] to capture the patch encodings at different levels of its feature pyramid. 

The output of the first transformer encoder is concatenated with next feature scale space of ResNet (after average pooling), and is fed into the next level transformer encoder, up until the final encoder layer, followed by the attention layer. 

We want to capture dependencies between patches at multiple scales, since different level of CNN output features include different semantic information. 

Such a Pyramid Transformer MIL network is shown in Fig. 2 (Fig. 3).

### 2.2 Instance Level Semi-supervision and Pseudo-labeling

![image-20211117121230806](C:\Users\dodo9\AppData\Roaming\Typora\typora-user-images\image-20211117121230806.png)

Fig. 3. An example ISUP grade 5 prostate cancer WSI. 
(a) Green mask overlay shows ground truth location of cancer regions (provided in the PANDA dataset [2]). (b) an additional heat map overlay visualizes our pseudo-labels of ISUP 5 (weighted by attention), achieved from training on weak (bag-level) labels only. 
Notice the close agreement between the dense pseudo-labels and the ground truth. 
In practice, pseudolabels are computed per patch; here we used a sliding-window approach for dense visualization. (Color figure online)

One of the challenges of MIL training is the lack of instance labels to guide the optimization process. 

A somewhat similar issue is encountered in semi-supervised learning [17], where pseudo-labels are used either offline or on the fly based on some intermediate estimates or another network’s predictions. 

Here, we propose to generate pseudo-labels for each image patch and use the additional patch-wise loss to assist the optimization process.

![image-20211117121248983](C:\Users\dodo9\AppData\Roaming\Typora\typora-user-images\image-20211117121248983.png)

where the total loss L includes a bag-level loss L bag (based on the ground truth labels) and a patch level loss L patch (based on the pseudo-labels). We use crossentropy loss function for both bag-level and patch-level losses.

We opt for a simple approach to generate pseudo-labels based on ensembling of several identical models trained from random initialization. 

The final ensembled labels are hard label (rounded to the nearest classes). Consider a trained network, its bag-level prediction output is based on the final output vector z (see Eq. 2), followed by a linear projection onto the number of output classes:

![image-20211117121301150](C:\Users\dodo9\AppData\Roaming\Typora\typora-user-images\image-20211117121301150.png)

![image-20211117121312103](C:\Users\dodo9\AppData\Roaming\Typora\typora-user-images\image-20211117121312103.png)

here we assumed a final sigmoid function (but the same holds with softmax).
We approximate the individual instance level prediction as

![image-20211117121323126](C:\Users\dodo9\AppData\Roaming\Typora\typora-user-images\image-20211117121323126.png)

**Pseudocode1.** shows the algorithm to compute the pseudo-labels. 

For some patches, whose ensembled attention weights are neither small nor large (defined by 10% threshold), we do not assign any pseudo-labels, and mark then and unknown to exclude from the L patch loss. 

Given the pseudo-labels we re-optimize the model using the additional patch-wise loss. 

The 10% heuristic was chosen to retain only most confident patches, that contribute the most to the final baglevel classification.

 A relevant approach was recently proposed by Lerousseau et al. [9]. 

However the goal of their work is a dense segmentation map, and not the improvements to the global classification accuracy, and the pseudo-labels are calculated differently, through thresholding of current prediction probability estimates on the fly.



## 3. Experiments

We implemented our method in PyTorch 1 and trained it on 4 NVIDIA Tesla V100
16 GB GPUs, batch size of 16. For the classification backbone, we use ResNet50
pretrained on ImageNet [8]. 

For the transformer layers, we keep a similar configuration as in [15], with 4 stacked transformer encoder blocks.

The lower pyramid level transformer has dimensionality of 256 for both input and hidden. 

The final transformer encoder has input dimension of 2308 (a concatenation of ResNet50
output features and the previous transformer outputs). 

We use Adam optimizer with initial learning rate of α 0 = 3e−4 for CNN parameters, and 3e−5 for transformer parameters, then gradually decrease it using cosine learning rate scheduler for 50 epochs. 

We use 5-fold cross validations to tune the parameters. 

For transformer layers only, we use weight decay of 0.1 and no dropout.

**PANDA Dataset.** Prostate cANcer graDe Assessment (PANDA) challenge dataset consists of 11K whole-slide images from two centers [2]. 

Currently, this is the largest public WSI dataset available. 

The grading process consisted of finding and classifying cancer tissue into Gleason patterns based on the architectural growth patterns of the tumor [1]. 

Consequently, it is converted into an ISUP grade on a 1–5 scale, based on the presence of two distinct Gleason patterns.

The dataset was provided as part of the Panda kaggle challenge, which attracted more than 1000 teams, with the goal to predict the most accurate ISUP grades.

Each individual image on average is about 25,000px × 25,000px RGB. The challenge also includes a hidden dataset, whose images were graded by multiple pathologists. 

The private dataset labels are not publicly available, but can be used to asses your model blindly via Kaggle website (invisible to the public as the challenge is closed now). 

In our experiments, we use a medium resolution input images (4× smaller than the highest resolution).

**Patch Selection.** To extract patches from WSI, we tile the image into a grid of 224px × 224px patches. 

At each iteration, the grid has a random offset from the top left corner, to ensure randomness of the patches. We then retain only the foreground patches. From the remaining patches, we maintain only a random subset (K = 56), which is a trade-off between covering the tissue content and GPU memory limits (see Fig. 1). 

We use batch size 16, which makes the data input size 16 × K × 3 × 224 × 224 at each iteration. During testing, inference is done using all foreground patches.

### 3.1 Results

**Transformer MIL.** We evaluate and compare our method to the Attention MIL and its Gated Attention MIL [14], as well as to a classical MIL with Max operator [3]. 

For evaluation metrics we use Accuracy, Area Under Curve (AUC) and Quadratic Weighted Kappa (QWK) of ISUP grade prediction (see Table 1). 

QWK metric measures the similarity between the predictions and targets, with a maximum value of 1. 

QWK was chosen as the main metric during the PANDA challenge [2], since it is more appropriate for the tasks with predicted classes being severity grades/levels. All metrics are computed using our 5-fold (80%/20% training/validation) splits, except for the Leaderboard column results, which come from the evaluation on kaggle challenge hidden private test-set. 

Even though the challenge is closed now, it allows for blind submission of the code snippet, which runs on the PANDA hidden set and outputs the final QWK number. 

These results are not added to the kaggle leaderboard, and are allowed only for post-challenge evaluations. 

Table 1 shows that the proposed two transforms based approaches outperform other methods both in our validation sets, and on the challenge hidden set.

We have also inspected the self-attention matrices and found that for many cases, they have distinct off-diagonal high value elements. In particular, instances with WSI tumor cells of different Gleason scores have higher off-diagonal values, indicating that such a combination is valuable for the final classification, which was captured by the transformer self-attention.

**Patch-Wise Pseudo-labels.** We train 5 models and ensemble their patch-level predictions. 

We use λ = 100. We show the performance of adding the pseudolabels supervision in Table 2.

In all cases the performance has improved compared to the baselines shown in Table 1 by ∼1%. Table 2 also shows the QWK results of the winners (top 3 places) of the PANDA kaggle challenge. 

Notice that our single model results are on par with the winners of the challenge (who all use ensembling of several models). 

We also experimented with ensembling, and the ensemble of our 10 models, achieves the leaderboard QWK of 0.94136, which would have been the first place in the leaderboard.

We have also tried but found no benefit of repeating pseudo-labeling several rounds, because the pseudo-label values almost do not change after the 1st round.

Table 1. Evaluations results on PANDA dataset. 
The Leaderboard column shows the QWK results of the private leaderboard of Kaggle’s challenge, which allows direct comparison to more then 1000 participants.

![image-20211117121457177](C:\Users\dodo9\AppData\Roaming\Typora\typora-user-images\image-20211117121457177.png)

Table 2. Evaluation results of adding pseudo-labels to our baseline transformer MIL approaches. 
We also include the results of the top three places of this challenge a (who all use ensembling of several models). 
Our results indicate that pseudo-labeling further improves the performance, with our single model providing results on par with the top winning teams.

![image-20211117121512087](C:\Users\dodo9\AppData\Roaming\Typora\typora-user-images\image-20211117121512087.png)

## 4. Discussion and Conclusion

We proposed a new deep learning based MIL approach for WSI classification with the following two main contributions: the addition of the transformer module to account for dependencies among instances and the instance-level supervision loss using pseudo-labels. 

We evaluated the method on PANDA challenge prostate WSI dataset, which includes over 11000 images. 

To put in perspective, most recently published SOTA methods evaluated their performance on datasets with the order of only several hundred images [7,11,12,18]. 

Furthermore, we compared our results directly to the leaderboard of the PANDA kaggle challenge with over 1000 participating teams, and demonstrated that our single model performance is on par with the top three winning teams, as evaluated blindly on the same hidden private test-set. 

Finally, recently proposed visual transformers [6] have shown a capability to replace the classification CNN completely, allowing for the possibility to create deep learning based MIL model solely based on the transformer blocks; we leave these investigations for future research.



