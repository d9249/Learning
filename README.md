# DIYA 2021-01 Computer Vision 4th.
**Description: Repository for reviewing papers in DIYA.**

**Format: Year, Conference, Paper, Link(Korean translation(Notion), Paper Link(Link), Summary(Notion), Reference(Notion), Open Review(Link), Official code(Link))**

## ðŸ“• Paper List up.

|Year|Conference|Paper|Link|
|--|--|---|---|
|2012|NIPS, Spotlight|imageNet Classification with Deep Convolutional Neural Network|[Korean translation](https://www.notion.so/imageNet-Classification-with-Deep-Convolutional-Neural-Network-74b0fc38e9af4073b421d284b1f25f60), [Paper Link](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf), [Summary](https://www.notion.so/Complete-78f08c463f714ce8a41f1f3e252d6c92), [Reference](https://www.notion.so/Link-898dc4ba580347fa9044c4fd987c7e40), [Open review](https://openreview.net/forum?id=BJWjTIWOWr), [Official code](https://paperswithcode.com/paper/imagenet-classification-with-deep)|
|2014|ICLR|Network In Network|[Korean translation](https://www.notion.so/Network-In-Network-1204aa586bdc4e1eb091ccfa2516a959)|
|2014|ECCV|Visualizing and Understanding Convolutional Networks|[Korean translation](https://www.notion.so/Visualizing-and-Understanding-Convolutional-Networks-00b895b0ca9c49e08cea3689980fbf48)|
|2014|ICLR|OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks|[Korean translation](https://www.notion.so/OverFeat-Integrated-Recognition-Localization-and-Detection-using-Convolutional-Networks-566b4eac9804469c9e36c5395a7383a2)
|2015|TPAMI|Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition|[Korean translation](https://www.notion.so/Spatial-Pyramid-Pooling-in-Deep-Convolutional-Networks-for-Visual-Recognition-4a8895352109462f92ebc1e86f6dba5d)
|2015|ICLR, Oral|Very Deep Convolutional Networks for Large-Scale Image Recognition|[Korean translation](https://www.notion.so/Very-Deep-Convolutional-Networks-for-Large-Scale-Image-Recognition-1433b0fdc3ef40b9b2b33a2ee6bea891)
|2015|CVPR, Oral|Going Deeper with Convolutions|[Korean translation](https://www.notion.so/Going-Deeper-with-Convolutions-d6e3064831d340b9a62a6d51c19f2cec)
|2015|ICCV|Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification|[Korean translation](https://www.notion.so/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classification-5dc0462fb81747db949bc724a5b8afd0)
|2015|ICML|Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift|[Korean translation](https://www.notion.so/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift-df6613cbe9404f919af107c7d20a9f08)
|2015|NIPS|Spatial Transformer Networks|[Korean translation](https://www.notion.so/Spatial-Transformer-Networks-839b6a12e69f4cc0ae5c9829c6f03d47)
|2016|CVPR|Rethinking the Inception Architecture for Computer Vision|[Korean translation](https://www.notion.so/Rethinking-the-Inception-Architecture-for-Computer-Vision-d7b2da742e55418e8856a6c29fc29deb)
|2016|CVPR, Oral, Best Paper Award|Deep Residual Learning for Image Recognition|[Korean translation](https://www.notion.so/Deep-Residual-Learning-for-Image-Recognition-5ad0b39db4444e739d6f707707067093)
|2016|CVPR|Learning Deep Features for Discriminative Localization|[Korean translation](https://www.notion.so/Learning-Deep-Features-for-Discriminative-Localization-a5acb9db59f043caabf498b4dc94c691)
|2016|Arxiv|SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size|[Korean translation](https://www.notion.so/SqueezeNet-AlexNet-level-accuracy-with-50x-fewer-parameters-and-0-5MB-model-size-2aa832275f6b44feac64bd11d3171fd1)
|2016|ECCV, Spotlight|Identity Mappings in Deep Residual Networks|[Korean translation](https://www.notion.so/Identity-Mappings-in-Deep-Residual-Networks-599310352e994685968cc585f0cabd93)
|2016|BMVC|Wide Residual Networks|[Korean translation](https://www.notion.so/Wide-Residual-Networks-c2dcbd8dcc2a432cb2613d5cc265428b)
|2017|AAAI|Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning|[Korean translation](https://www.notion.so/Inception-v4-Inception-ResNet-and-the-Impact-of-Residual-Connections-on-Learning-622c720280cf472592a981f392bd9c03)
|2017|CVPR, Oral, Best Paper Award|Densely Connected Convolutional Networks|[Korean translation](https://www.notion.so/Densely-Connected-Convolutional-Networks-05e53d2598694bb99fff6d7e5cb4c9da)
|2017|ICCV|Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization|[Korean translation](https://www.notion.so/Grad-CAM-Visual-Explanations-from-Deep-Networks-via-Gradient-based-Localization-433a6764a8814dc9a49a26bac7c755cc)
|2017|CVPR|Deep Pyramidal Residual Networks|[Korean translation](https://www.notion.so/Deep-Pyramidal-Residual-Networks-a6001a1851d84292a528f6f18cb95001)
|2017|CVPR|Xception: Deep Learning with Depthwise Separable Convolutions|[Korean translation](https://www.notion.so/Xception-Deep-Learning-with-Depthwise-Separable-Convolutions-5818af0061c044f3b88bd4f71d9e765c)
|2017|CVPR|Aggregated Residual Transformations for Deep Neural Networks|[Korean translation](https://www.notion.so/Aggregated-Residual-Transformations-for-Deep-Neural-Networks-dbf9287a5edb440cb4b89944a9f54a66)
|2017|CVPR|PolyNet: A Pursuit of Structural Diversity in Very Deep Networks|[Korean translation](https://www.notion.so/PolyNet-A-Pursuit-of-Structural-Diversity-in-Very-Deep-Networks-03dbcccd1ab74e51b0dfda93a0a3114a)
|2017|CoRR|MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications|[Korean translation](https://www.notion.so/MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications-3622fa839f474e88ad20a1fd27800e32)
|2017|NIPS|Dynamic Routing Between Capsules|[Korean translation](https://www.notion.so/Dynamic-Routing-Between-Capsules-02be72ea401c47f390c3eac814fc83a8)
|2018|CVPR|ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices|[Korean translation](https://www.notion.so/ShuffleNet-An-Extremely-Efficient-Convolutional-Neural-Network-for-Mobile-Devices-64406878d4a04caa8e6eeda56b83c69f)
|2018|CVPR, Oral|Squeeze-and-Excitation Networks|[Korean translation](https://www.notion.so/Squeeze-and-Excitation-Networks-9a916233a6a945e68396ecdffca6a858)
|2018|CVPR|Non-local Neural Networks|[Korean translation](https://www.notion.so/Non-local-Neural-Networks-99fa2a9d393646af90246a37c0d03f05)
|2018|CVPR|MobileNetV2: Inverted Residuals and Linear Bottlenecks|[Korean translation](https://www.notion.so/MobileNetV2-Inverted-Residuals-and-Linear-Bottlenecks-9720a7607f024cb69eabb0ef54907164)
|2018|ECCV|Exploring the Limits of Weakly Supervised Pretraining|[Korean translation](https://www.notion.so/Exploring-the-Limits-of-Weakly-Supervised-Pretraining-8aa9db8d23024a8c861930b783b27625)
|2018|NIPS, Oral|How Does Batch Normalization Help Optimization?|[Korean translation](https://www.notion.so/How-Does-Batch-Normalization-Help-Optimization-e5ff18c092d94021832b192f350ab69a)
|2018|NIPS|Understanding Batch Normalization|[Korean translation](https://www.notion.so/Understanding-Batch-Normalization-38516d2f1a024d4699f8a878b6e3a0a8)
|2018|ECCV|ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design|[Korean translation](https://www.notion.so/ShuffleNet-V2-Practical-Guidelines-for-Efficient-CNN-Architecture-Design-4bb42454f3314d57ac227b72e631b3bd)
|2019|CVPR|Bag of Tricks for Image Classification with Convolutional Neural Networks|[Korean translation](https://www.notion.so/Bag-of-Tricks-for-Image-Classification-with-Convolutional-Neural-Networks-e81ae42863d5452a9e8d739c6d7f953b)
|2019|ICCV, Oral|Searching for MobileNetV3|[Korean translation](https://www.notion.so/Searching-for-MobileNetV3-03b1798e240d44a98da0050305c34387)
|2019|ICML, Oral|EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks|[Korean translation](https://www.notion.so/EfficientNet-Rethinking-Model-Scaling-for-Convolutional-Neural-Networks-dbad07b6dcc84f3480be869a7b186927)
|2019|NIPS, Spotlight|When Does Label Smoothing Help?|[Korean translation](https://www.notion.so/When-Does-Label-Smoothing-Help-499c58240fc94fa3813804dcfed3e81a)
|2019|NIPS|Stand-Alone Self-Attention in Vision Models|[Korean translation](https://www.notion.so/Stand-Alone-Self-Attention-in-Vision-Models-7943de8101644322b3a449bc7b8646ef)
|2019|NIPS|Fixing the train-test resolution discrepancy|[Korean translation](https://www.notion.so/Fixing-the-train-test-resolution-discrepancy-c5361686c90544d7b30fdb79b62ebbcd)
|2020|CVPR|Self-training with Noisy Student improves ImageNet classification|[Korean translation](https://www.notion.so/Self-training-with-Noisy-Student-improves-ImageNet-classification-8ae35f49144f46469c8ef67118d1db91)
|2020|CVPR|Adversarial Examples Improve Image Recognition|[Korean translation](https://www.notion.so/Adversarial-Examples-Improve-Image-Recognition-bf802d61fead4686a36d562a680f9655)
|2020|ECCV, Spotlight|Big Transfer (BiT): General Visual Representation Learning|[Korean translation](https://www.notion.so/Big-Transfer-BiT-General-Visual-Representation-Learning-d81a0b0bade141aa9ae6f06458a0d050)
|2020|Arxiv|Fixing the train-test resolution discrepancy: FixEfficientNet|[Korean translation](https://www.notion.so/Fixing-the-train-test-resolution-discrepancy-FixEfficientNet-1b752988a8254996a2bd4bc7fd3284fc)
|2021|ICLR, Spotlight|Sharpness-Aware Minimization for Efficiently Improving Generalization|[Korean translation](https://www.notion.so/Sharpness-Aware-Minimization-for-Efficiently-Improving-Generalization-564eeafb95274c3ca309ee9e10b2d1a6)
|2021|ICLR, Oral|An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale|[Korean translation](https://www.notion.so/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale-3f65fe916c0b4d52a86adde2a94b48b4)
|||Training data-efficient image transformers & distillation through attention|[Korean translation](https://www.notion.so/Training-data-efficient-image-transformers-distillation-through-attention-f3311d82d37548d7a651a1ad991ea3d9)
|||High-Performance Large-Scale Image Recognition Without Normalization|[Korean translation](https://www.notion.so/High-Performance-Large-Scale-Image-Recognition-Without-Normalization-da5ec8784e804ca88da63c077b221193)

<div align="center">
  Copyright 2021. d9249(Lee sangmin) all rights reserved.
</div>
