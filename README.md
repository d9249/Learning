# DIYA_2021_CV

## ðŸ“• Paper

### 2012

|:Year:|:Conference:|:Paper:|:Link:|
|---|---|---|---|
|:2012:|:NIPS, Spotlight:|:imageNet Classification with Deep Convolutional Neural Network:|:[Notion](https://www.notion.so/imageNet-Classification-with-Deep-Convolutional-Neural-Network-74b0fc38e9af4073b421d284b1f25f60):|

### 2014

[**Network In Network**](https://www.notion.so/Network-In-Network-1204aa586bdc4e1eb091ccfa2516a959)

[**Visualizing and Understanding Convolutional Networks**](https://www.notion.so/Visualizing-and-Understanding-Convolutional-Networks-00b895b0ca9c49e08cea3689980fbf48)

[**OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks**](https://www.notion.so/OverFeat-Integrated-Recognition-Localization-and-Detection-using-Convolutional-Networks-566b4eac9804469c9e36c5395a7383a2)

### 2015

[**Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition**](https://www.notion.so/Spatial-Pyramid-Pooling-in-Deep-Convolutional-Networks-for-Visual-Recognition-4a8895352109462f92ebc1e86f6dba5d)

[**Very Deep Convolutional Networks for Large-Scale Image Recognition**](https://www.notion.so/Very-Deep-Convolutional-Networks-for-Large-Scale-Image-Recognition-1433b0fdc3ef40b9b2b33a2ee6bea891)

[**Going Deeper with Convolutions**](https://www.notion.so/Going-Deeper-with-Convolutions-d6e3064831d340b9a62a6d51c19f2cec)

[**Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification**](https://www.notion.so/Delving-Deep-into-Rectifiers-Surpassing-Human-Level-Performance-on-ImageNet-Classification-5dc0462fb81747db949bc724a5b8afd0)

[**Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift**](https://www.notion.so/Batch-Normalization-Accelerating-Deep-Network-Training-by-Reducing-Internal-Covariate-Shift-df6613cbe9404f919af107c7d20a9f08)

[**Spatial Transformer Networks**](https://www.notion.so/Spatial-Transformer-Networks-839b6a12e69f4cc0ae5c9829c6f03d47)

### 2016

[**Rethinking the Inception Architecture for Computer Vision**](https://www.notion.so/Rethinking-the-Inception-Architecture-for-Computer-Vision-d7b2da742e55418e8856a6c29fc29deb)

[**Deep Residual Learning for Image Recognition**](https://www.notion.so/Deep-Residual-Learning-for-Image-Recognition-5ad0b39db4444e739d6f707707067093)

[**Learning Deep Features for Discriminative Localization**](https://www.notion.so/Learning-Deep-Features-for-Discriminative-Localization-a5acb9db59f043caabf498b4dc94c691)

[**SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size**](https://www.notion.so/SqueezeNet-AlexNet-level-accuracy-with-50x-fewer-parameters-and-0-5MB-model-size-2aa832275f6b44feac64bd11d3171fd1)

[**Identity Mappings in Deep Residual Networks**](https://www.notion.so/Identity-Mappings-in-Deep-Residual-Networks-599310352e994685968cc585f0cabd93)

[**Wide Residual Networks**](https://www.notion.so/Wide-Residual-Networks-c2dcbd8dcc2a432cb2613d5cc265428b)

### 2017

[**Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning**](https://www.notion.so/Inception-v4-Inception-ResNet-and-the-Impact-of-Residual-Connections-on-Learning-622c720280cf472592a981f392bd9c03)

[**Densely Connected Convolutional Networks**](https://www.notion.so/Densely-Connected-Convolutional-Networks-05e53d2598694bb99fff6d7e5cb4c9da)

[**Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization**](https://www.notion.so/Grad-CAM-Visual-Explanations-from-Deep-Networks-via-Gradient-based-Localization-433a6764a8814dc9a49a26bac7c755cc)

[**Deep Pyramidal Residual Networks**](https://www.notion.so/Deep-Pyramidal-Residual-Networks-a6001a1851d84292a528f6f18cb95001)

[**Xception: Deep Learning with Depthwise Separable Convolutions**](https://www.notion.so/Xception-Deep-Learning-with-Depthwise-Separable-Convolutions-5818af0061c044f3b88bd4f71d9e765c)

[**Aggregated Residual Transformations for Deep Neural Networks**](https://www.notion.so/Aggregated-Residual-Transformations-for-Deep-Neural-Networks-dbf9287a5edb440cb4b89944a9f54a66)

[**PolyNet: A Pursuit of Structural Diversity in Very Deep Networks**](https://www.notion.so/PolyNet-A-Pursuit-of-Structural-Diversity-in-Very-Deep-Networks-03dbcccd1ab74e51b0dfda93a0a3114a)

[**MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications**](https://www.notion.so/MobileNets-Efficient-Convolutional-Neural-Networks-for-Mobile-Vision-Applications-3622fa839f474e88ad20a1fd27800e32)

[**Dynamic Routing Between Capsules**](https://www.notion.so/Dynamic-Routing-Between-Capsules-02be72ea401c47f390c3eac814fc83a8)

### 2018

[**ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices**](https://www.notion.so/ShuffleNet-An-Extremely-Efficient-Convolutional-Neural-Network-for-Mobile-Devices-64406878d4a04caa8e6eeda56b83c69f)

[**Squeeze-and-Excitation Networks**](https://www.notion.so/Squeeze-and-Excitation-Networks-9a916233a6a945e68396ecdffca6a858)

[**Non-local Neural Networks**](https://www.notion.so/Non-local-Neural-Networks-99fa2a9d393646af90246a37c0d03f05)

[**MobileNetV2: Inverted Residuals and Linear Bottlenecks**](https://www.notion.so/MobileNetV2-Inverted-Residuals-and-Linear-Bottlenecks-9720a7607f024cb69eabb0ef54907164)

[**Exploring the Limits of Weakly Supervised Pretraining**](https://www.notion.so/Exploring-the-Limits-of-Weakly-Supervised-Pretraining-8aa9db8d23024a8c861930b783b27625)

[**How Does Batch Normalization Help Optimization?**](https://www.notion.so/How-Does-Batch-Normalization-Help-Optimization-e5ff18c092d94021832b192f350ab69a)

[**Understanding Batch Normalization**](https://www.notion.so/Understanding-Batch-Normalization-38516d2f1a024d4699f8a878b6e3a0a8)

[**ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design**](https://www.notion.so/ShuffleNet-V2-Practical-Guidelines-for-Efficient-CNN-Architecture-Design-4bb42454f3314d57ac227b72e631b3bd)

### 2019

[Bag of Tricks for Image Classification with Convolutional Neural Networks](https://www.notion.so/Bag-of-Tricks-for-Image-Classification-with-Convolutional-Neural-Networks-e81ae42863d5452a9e8d739c6d7f953b)

[Searching for MobileNetV3](https://www.notion.so/Searching-for-MobileNetV3-03b1798e240d44a98da0050305c34387)

[EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://www.notion.so/EfficientNet-Rethinking-Model-Scaling-for-Convolutional-Neural-Networks-dbad07b6dcc84f3480be869a7b186927)

[When Does Label Smoothing Help?](https://www.notion.so/When-Does-Label-Smoothing-Help-499c58240fc94fa3813804dcfed3e81a)

[Stand-Alone Self-Attention in Vision Models](https://www.notion.so/Stand-Alone-Self-Attention-in-Vision-Models-7943de8101644322b3a449bc7b8646ef)

[Fixing the train-test resolution discrepancy](https://www.notion.so/Fixing-the-train-test-resolution-discrepancy-c5361686c90544d7b30fdb79b62ebbcd)

### 2020

[Self-training with Noisy Student improves ImageNet classification](https://www.notion.so/Self-training-with-Noisy-Student-improves-ImageNet-classification-8ae35f49144f46469c8ef67118d1db91)

[Adversarial Examples Improve Image Recognition](https://www.notion.so/Adversarial-Examples-Improve-Image-Recognition-bf802d61fead4686a36d562a680f9655)

[Big Transfer (BiT): General Visual Representation Learning](https://www.notion.so/Big-Transfer-BiT-General-Visual-Representation-Learning-d81a0b0bade141aa9ae6f06458a0d050)

[Fixing the train-test resolution discrepancy: FixEfficientNet](https://www.notion.so/Fixing-the-train-test-resolution-discrepancy-FixEfficientNet-1b752988a8254996a2bd4bc7fd3284fc)

### 2021

[Sharpness-Aware Minimization for Efficiently Improving Generalization](https://www.notion.so/Sharpness-Aware-Minimization-for-Efficiently-Improving-Generalization-564eeafb95274c3ca309ee9e10b2d1a6)

[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://www.notion.so/An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale-3f65fe916c0b4d52a86adde2a94b48b4)

---

[Training data-efficient image transformers & distillation through attention](https://www.notion.so/Training-data-efficient-image-transformers-distillation-through-attention-f3311d82d37548d7a651a1ad991ea3d9)

[High-Performance Large-Scale Image Recognition Without Normalization](https://www.notion.so/High-Performance-Large-Scale-Image-Recognition-Without-Normalization-da5ec8784e804ca88da63c077b221193)
