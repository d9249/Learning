# DIYA Computer Vision 5th (2022)

> Format: Topic, Conference, Paper Link, Summary Link 
## üìï Paper list

|Topic|Paper|Link|
|--|---|---|
|Adversarially-Robust Deep Learning|Intriguing properties of neural networks|[paper](https://arxiv.org/abs/1312.6199), [summary]()|
|ÏûêÏú†Ï£ºÏ†ú|Show, Attend and Tell: Neural Image Caption Generation with Visual Attention|[paper](https://arxiv.org/abs/1502.03044), [summary]()|
|ÏûêÏú†Ï£ºÏ†ú|Deep Unsupervised Learning using Nonequilibrium Thermodynamics|[paper](https://arxiv.org/abs/1503.03585), [summary]()|
|Generative Adversarial Networks|Generative Adversarial Networks|[paper](https://arxiv.org/abs/1406.2661), [summary](https://ideal96.notion.site/Generative-Adversarial-Networks-88c9a85074054953b89b3658c27f205f)|
|VAEs, Autoregressive and Flow-Based Generative Models|MADE: Masked Autoencoder for Distribution Estimation|[paper](), [summary]()|
|Memory and Computation-Efficient Deep Learning|Learning both Weights and Connections for Efficient Neural Networks|[paper](https://arxiv.org/abs/1506.02626), [summary]()|
|Continual Learning|Progressive Neural Networks|[paper](https://arxiv.org/abs/1606.04671), [summary]()|
|Continual Learning|Overcoming catastrophic forgetting in neural networks|[paper](https://arxiv.org/abs/1612.00796), [summary]()|
|Interpretable Deep Learning|"Why Should I Trust You?": Explaining the Predictions of Any Classifier|[paper](https://arxiv.org/abs/1602.04938), [summary]()|
|Reliable Deep Learning|Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles|[paper](https://arxiv.org/abs/1612.01474), [summary]()|
|ÏûêÏú†Ï£ºÏ†ú|Texture Synthesis Using Convolutional Neural Networks|[paper](https://arxiv.org/abs/1505.07376), [summary](https://ideal96.notion.site/Texture-Synthesis-Using-Convolutional-Neural-Networks-707ca4babc8a42fe8b7804768d0ca2d2)|
|ÏûêÏú†Ï£ºÏ†ú|Dynamic Filter Networks|[paper](https://arxiv.org/abs/1605.09673), [summary]()|
|VAEs, Autoregressive and Flow-Based Generative Models|Variational Inference with Normalizing Flows|[paper](https://arxiv.org/abs/1505.05770), [summary]()|
|Memory and Computation-Efficient Deep Learning|Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding|[paper](https://arxiv.org/abs/1510.00149), [summary]()|
|ÏûêÏú†Ï£ºÏ†ú|On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima|[paper](https://arxiv.org/abs/1609.04836), [summary]()|
|Adversarially-Robust Deep Learning|Explaining and Harnessing Adversarial Examples|[paper](https://arxiv.org/abs/1412.6572), [summary]()|
|Interpretable Deep Learning|RETAIN: An Interpretable Predictive Model for Healthcare using Reverse Time Attention Mechanism|[paper](https://arxiv.org/abs/1608.05745), [summary]()|
|Reliable Deep Learning|Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks|[paper](https://arxiv.org/abs/1706.02690), [summary]()|
|Generative Adversarial Networks|Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks|[paper](https://arxiv.org/abs/1511.06434), [summary]()|
|ÏûêÏú†Ï£ºÏ†ú|Deep Networks with Stochastic Depth|[paper](https://arxiv.org/abs/1603.09382), [summary]()|
|VAEs, Autoregressive and Flow-Based Generative Models|Pixel Recurrent Neural Networks|[paper](https://arxiv.org/abs/1601.06759), [summary]()|
|Memory and Computation-Efficient Deep Learning|Learning Structured Sparsity in Deep Neural Networks|[paper](https://arxiv.org/abs/1608.03665), [summary]()|
|Continual Learning|Overcoming Catastrophic Forgetting by Incremental Moment Matching|[paper](https://arxiv.org/abs/1703.08475), [summary]()|
|Adversarially-Robust Deep Learning|Adversarial Machine Learning at Scale|[paper](https://arxiv.org/abs/1611.01236), [summary]()|
|Interpretable Deep Learning|Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization|[paper](https://arxiv.org/abs/1610.02391), [summary](https://ideal96.notion.site/Grad-CAM-Visual-Explanations-from-Deep-Networks-via-Gradient-based-Localization-026f13d8a1164e5dabed23b652816baf)|
|ÏûêÏú†Ï£ºÏ†ú|Prototypical Networks for Few-shot Learning|[paper](https://arxiv.org/abs/1703.05175), [summary]()|
|Generative Adversarial Networks|InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets|[paper](https://arxiv.org/abs/1606.03657), [summary]()|
|ÏûêÏú†Ï£ºÏ†ú|Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization|[paper](https://arxiv.org/abs/1703.06868), [summary]()|
|understanding & sota|Understanding deep learning requires rethinking generalization|[paper](https://arxiv.org/abs/1611.03530), [summary]()|
|Memory and Computation-Efficient Deep Learning|Variational Dropout Sparsifies Deep Neural Networks|[paper](https://arxiv.org/abs/1701.05369), [summary]()|
|Continual Learning|Continual Learning with Deep Generative Replay|[paper](https://arxiv.org/abs/1705.08690), [summary]()|
|Adversarially-Robust Deep Learning|Towards Deep Learning Models Resistant to Adversarial Attacks|[paper](https://arxiv.org/abs/1706.06083), [summary](https://ideal96.notion.site/Towards-Deep-Learning-Models-Resistant-to-Adversarial-Attacks-0b298fb6c6fb48698b98c6f92fe4dfc5)|
|Reliable Deep Learning|On Calibration of Modern Neural Networks|[paper](https://arxiv.org/abs/1706.04599), [summary]()|
|VAEs, Autoregressive and Flow-Based Generative Models|Density estimation using Real NVP|[paper](https://arxiv.org/abs/1605.08803), [summary]()|
|ÏûêÏú†Ï£ºÏ†ú|Image-to-Image Translation with Conditional Adversarial Networks|[paper](https://arxiv.org/abs/1611.07004), [summary]()|
|ÏûêÏú†Ï£ºÏ†ú|Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction|[paper](https://arxiv.org/abs/1611.09842), [summary]()|
|Understanding & SOTA|A Closer Look at Memorization in Deep Networks|[paper](https://arxiv.org/abs/1706.05394), [summary]()|
|ÏûêÏú†Ï£ºÏ†ú|Large Batch Training of Convolutional Networks|[paper](https://arxiv.org/abs/1708.03888), [summary]()|
|Memory and Computation-Efficient Deep Learning|MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications|[paper](https://arxiv.org/abs/1704.04861), [summary]()|
|Continual Learning|Gradient Episodic Memory for Continual Learning|[paper](https://arxiv.org/abs/1706.08840), [summary]()|
|Interpretable Deep Learning|Understanding Black-box Predictions via Influence Functions|[paper](https://arxiv.org/abs/1703.04730), [summary]()|
|ÏûêÏú†Ï£ºÏ†ú|Deep Learning for Case-Based Reasoning through Prototypes: A Neural Network that Explains Its Predictions|[paper](https://arxiv.org/abs/1710.04806), [summary]()|
|Reliable Deep Learning|Predict Responsibly: Improving Fairness and Accuracy by Learning to Defer|[paper](https://arxiv.org/abs/1711.06664), [summary]()|
|Generative Adversarial Networks|Wasserstein GAN|[paper](https://arxiv.org/abs/1701.07875), [summary]()|
|Understanding & SOTA|Train longer, generalize better: closing the generalization gap in large batch training of neural networks|[paper](https://arxiv.org/abs/1705.08741), [summary]()|
|VAEs, Autoregressive and Flow-Based Generative Models|Improving Variational Inference with Inverse Autoregressive Flow|[paper](https://arxiv.org/abs/1606.04934), [summary]()|
|ÏûêÏú†Ï£ºÏ†ú|Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)|[paper](https://arxiv.org/abs/1711.11279), [summary]()|
|ÏûêÏú†Ï£ºÏ†ú|Shake-Shake regularization|[paper](https://arxiv.org/abs/1705.07485), [summary](https://ideal96.notion.site/Shake-Shake-regularization-77d557aae73c4484bc70f93d6e5175b3)|
|Adversarially-Robust Deep Learning|Robust Physical-World Attacks on Deep Learning Models|[paper](https://arxiv.org/abs/1707.08945), [summary]()|
|Generative Adversarial Networks|Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks|[paper](https://arxiv.org/abs/1703.10593), [summary]()|
|Interpretable Deep Learning|Network Dissection: Quantifying Interpretability of Deep Visual Representations|[paper](https://arxiv.org/abs/1704.05796), [summary]()|
|Reliable Deep Learning|Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples|[paper](https://arxiv.org/abs/1711.09325), [summary]()|
|Understanding & SOTA|Do Better ImageNet Models Transfer Better?|[paper](https://arxiv.org/abs/1805.08974), [summary]()|
|Generative Adversarial Networks|Adversarial Feature Matching for Text Generation|[paper](https://arxiv.org/abs/1706.03850), [summary]()|
|VAEs, Autoregressive and Flow-Based Generative Models|PixelCNN++: Improving the PixelCNN with Discretized Logistic Mixture Likelihood and Other Modifications|[paper](https://arxiv.org/abs/1701.05517), [summary]()|
|Memory and Computation-Efficient Deep Learning|Bayesian Compression for Deep Learning|[paper](https://arxiv.org/abs/1705.08665), [summary]()|
|Continual Learning|Lifelong Learning with Dynamically Expandable Networks|[paper](https://arxiv.org/abs/1708.01547), [summary]()|
|ÏûêÏú†Ï£ºÏ†ú|Decoupled Weight Decay Regularization|[paper](https://arxiv.org/abs/1711.05101), [summary]()|
|Adversarially-Robust Deep Learning|Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples|[paper](https://arxiv.org/abs/1802.00420), [summary]()|
|ÏûêÏú†Ï£ºÏ†ú|Deep Image Prior|[paper](https://arxiv.org/abs/1711.10925), [summary]()|
|||[paper](), [summary]()|
|||[paper](), [summary]()|
|||[paper](), [summary]()|
|||[paper](), [summary]()|
|||[paper](), [summary]()|
|||[paper](), [summary]()|
|||[paper](), [summary]()|
|||[paper](), [summary]()|
|||[paper](), [summary]()|
|||[paper](), [summary]()|
|||[paper](), [summary]()|
|||[paper](), [summary]()|
|||[paper](), [summary]()|
|||[paper](), [summary]()|
|||[paper](), [summary]()|

<div align="center">
  Copyright 2021. d9249(Lee sangmin) all rights reserved.
</div>
