ppt1
안녕하세요 저는 경기대학교 스마트 아이오티 소속 이상민이라고 합니다.
발표를 시작하도록 하겠습니다.
제가 발표할 제목은 겹친 문자 이미지 분류를 위한 합성곱 신경망 모델의 정확도 분석입니다.
ppt2
우선, 문제에 대한 설명부터 진행하겠습니다.
DACON의 컴퓨터 비전 학습 경진 대회의 감추어진 숫자를 예측해내는 문제를 대상으로 진행하였으며,
여기서 말하는 디짓은 가려지는 대상이 되는 원본 숫자를 의미하며,
레터는 디짓을 가리기 위한 문자입니다 
디짓은 0부터 9까지 10가지가 사용되며, 레터는 알파벳 소대문자를 포함하여 52개가 사용됩니다.
ppt3
앞서 설명 드린 디짓과 레터를 사용하여서 다음과 같은 이미지를 학습데이터로 주어지며
학습 데이터를 만드는 과정에서 단순히 두개의 문자를 겹친 후 레터의 영역 나머지는 전부 지워버리는 것을 알 수 있습니다.
흰색, 회색, 검은색의 영역에 대해 설명 드리겠습니다.
흰색 부분은 디짓과 레터가 겹쳐진 영역을 의미하며
회색은 디짓과 레터가 겹쳐지지 않은 영역을 의미합니다.
나머지 검은색 영역은 레터의 영역 나머지를 의미하며, 해당 영역에 디짓의 가려진 부분이 존재합니다.
결과적으로 흰색 영역을 보고 감추어진 숫자가 무엇인지를 알아내는 것이 해당 문제의 요약이라고 볼 수 있습니다.
ppt4
다음은 해당 문제에 대한 데이터 분석 결과에 대해서 설명드리겠습니다.
해당 이미지들은 편히 보기위해 따로 시각화를 진행한 이미지들 입니다.
먼저 첫번쨰로 학습 데이터를 볼 수 있습니다
이러한 학습데이터에서 숫자를 나타내는 영역만을 추출한 사진이 가운데에 있는 사진이며,
전체 학습데이터에서 숫자를 나타내는 영역을 모두 추출하여 하나의 이미지로 합친 결과
제일 아래에 있는 이미지와 같이 대상이 되는 숫자는 각각 하나의 이미지만을 사용한 것을 알 수 있었습니다.
ppt5
다음으로는 전체 데이터에 대한 분포에 대한 분석을 진행하였습니다.
분석 결과 하늘색과 파란색으로 표시되어 있는 부분들 처럼 데이터 불균형 문제가 존재함을 알 수 있었습니다.
ppt6
지금까지가 해당 문제에 대한 설명, 분석이였으며,
이러한 문제를 통해 무엇을 하고자 하는가에 대해서 설명드리겠습니다.
MNIST 이미지 분류 대회의 필기체 분류 문제에서 사람의 분류는 약 95%임을 알 수 있고,
최근 분류 모델의 성능은 99.87%로 거의 완벽에 가까운 분류를 해내는 것을 알 수 있었습니다.
하지만 해당 문제는 단순히 필기체를 분류해내는 문제에만 그쳐 보다 정확한 성능 비교가 어렵다고 판단하여
그보다 더 어려운 문제에 기존의 저명한 합성곱 신경망 모델을 적용하여 성능 비교를 진행하였습니다. 
PPT7
다음과 같은 5가지의 주요 모델들을 활용하였는데
VGGNet은 2014 이미지넷 이미지 인식 대회에서 준우승을 한 모델이지만, CNN 기본 아키텍쳐만을 사용하였기 때문에
GoogleNet보다 더욱 각광을 받았습니다
GoogleNet은 같은 대회에서 가장 적은 에러율을 보여 우승을 차지한 모델입니다.
ResNet은 2015년 이미지넷 이미지 인식 대회에서 우승을 하였고,
GoogleNet에 ResNet의 아이디어를 적용하여 후속 모델들이 제안되었습니다.
DenseNet은 ResNet보다 더 적은 파라미터수로 더 높은 성능을 가진 모델이며, 
ResNet은 feature map 끼리 더하기를 해주는 방식이었다면 
DenseNet은 feature map끼리 병합(Concatenation)을 시키는 것이 가장 큰 차이점을 보이며, 
이를 통해서 Vanishing Gradient 개선, Feature Propagation 강화, Feature Reuse, Parameter 수 절약의 이점을 갖는 모델입니다.
EfficientNet은 AutoML을 통해 높이를 늘리거나, 채널 넓이를 넓히거나, 이미지의 해상도를 높이는 방법의 최적의 조합을 찾아 제안된 
모델입니다.
PPT8
모델간의 성능을 비교분석하기 위해서 다음과 같은 하이퍼파라미터를 고정하여 사용하였으며,
학습데이터의 불균형과 부족을 해결하기 위해서 오른쪽 사진과 같이 데이터 증강을 진행하여 학습에 사용하였습니다.
PPT9
전체적인 학습 결과에 대해 다음과 같이 정확도 추이를 나타내었습니다.
결과에 대해 요약하자면 각각 모델들의 최적이 이미지 사이즈와 학습에 사용된 이미지 사이즈가 같은 경우 DenseNet121 모델이
가장 높은 성능을 보였으며, 최적의 이미지 사이즈와 입력 이미지 사이즈가 다른 경우에는 Xception 모델이 가장 높은 성능을 보였습니다.
전체적인 학습을 통해 평균적으로 약 90%의 정확도를 보이는 것을 알 수 있었으며, 전체적인 성능이 비슷함을 알 수 있었습니다.
PPT10
앞서 개별적으로 학습된 모델들을 활용하여 앙상블을 진행하였으며, VGG 모델의 경우 추가 레이어가 필요하였기에 앙상블에서 제외하였습니다.
앙상블의 결과로는 약 93%의 성능을 보이는 것을 알 수 있었으며, 개별적인 모델의 결과 보다 약 3% 정도 높은 정확도를 보였습니다.
PPT11
추후 연구로는 앞서 설명드린 하나의 숫자 이미지가 학습데이터 생성에 사용되었기에 이 중 가장 유사도가 높은 이미지로 예측을 진행하는
방법을 통해 앞선 결과의 정확도보다 훨씬 높은 성능을 보일것으로 예상됩니다.
PPT12
결과적으로 전체적인 모델의 학습의 결과의 추이를 통해서 대부분 유사한 성능을 가짐을 알 수 있었고,
더욱이 정확도를 끌어올리기 위해서는 데이터 전처리를 통한 학습이 더 높은 정확도를 보일 것을 알 수 있었습니다.
이상으로 발표를 마치도록 하겠습니다.