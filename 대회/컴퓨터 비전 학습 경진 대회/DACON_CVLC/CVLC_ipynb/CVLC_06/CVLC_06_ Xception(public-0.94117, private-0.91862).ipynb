{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CVLC_06_ Xception(public-0.94117, private-0.91862).ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyNMQjUEoaHb8XVbqs2huhiJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"LmEaPJckuX-D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628942658433,"user_tz":-540,"elapsed":16394,"user":{"displayName":"이상민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwWA7JKqOrAuJreghl2Xa5oY8W9qpi7rlSayx_AsU=s64","userId":"07266262195408457313"}},"outputId":"9058b82f-22c0-4d63-a887-7f6430c3ca65"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"88GAtllsufPj","executionInfo":{"status":"ok","timestamp":1628942661840,"user_tz":-540,"elapsed":3002,"user":{"displayName":"이상민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwWA7JKqOrAuJreghl2Xa5oY8W9qpi7rlSayx_AsU=s64","userId":"07266262195408457313"}}},"source":["import pandas as pd\n","train = pd.read_csv('/content/drive/MyDrive/DACON_CVLC/data/train.csv')\n","test = pd.read_csv('/content/drive/MyDrive/DACON_CVLC/data/test.csv')"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"8qBWziyZrqBo","executionInfo":{"status":"ok","timestamp":1628942662917,"user_tz":-540,"elapsed":1087,"user":{"displayName":"이상민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwWA7JKqOrAuJreghl2Xa5oY8W9qpi7rlSayx_AsU=s64","userId":"07266262195408457313"}}},"source":["!mkdir images_train\n","!mkdir images_train/0\n","!mkdir images_train/1\n","!mkdir images_train/2\n","!mkdir images_train/3\n","!mkdir images_train/4\n","!mkdir images_train/5\n","!mkdir images_train/6\n","!mkdir images_train/7\n","!mkdir images_train/8\n","!mkdir images_train/9\n","!mkdir images_test"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"3fjN8mIDrazg","executionInfo":{"status":"ok","timestamp":1628942665562,"user_tz":-540,"elapsed":2649,"user":{"displayName":"이상민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwWA7JKqOrAuJreghl2Xa5oY8W9qpi7rlSayx_AsU=s64","userId":"07266262195408457313"}}},"source":["import cv2\n","\n","for idx in range(len(train)) :\n","    img = train.loc[idx, '0':].values.reshape(28, 28).astype(int)\n","    digit = train.loc[idx, 'digit']\n","    cv2.imwrite(f'./images_train/{digit}/{train[\"id\"][idx]}.png', img)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"k4P9AD1gyotc","executionInfo":{"status":"ok","timestamp":1628942684807,"user_tz":-540,"elapsed":19248,"user":{"displayName":"이상민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwWA7JKqOrAuJreghl2Xa5oY8W9qpi7rlSayx_AsU=s64","userId":"07266262195408457313"}}},"source":["import cv2\n","\n","for idx in range(len(test)) :\n","    img = test.loc[idx, '0':].values.reshape(28, 28).astype(int)\n","    cv2.imwrite(f'./images_test/{test[\"id\"][idx]}.png', img)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"HUJTlJ6GxNmK","executionInfo":{"status":"ok","timestamp":1628942691130,"user_tz":-540,"elapsed":6335,"user":{"displayName":"이상민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwWA7JKqOrAuJreghl2Xa5oY8W9qpi7rlSayx_AsU=s64","userId":"07266262195408457313"}}},"source":["import tensorflow as tf\n","Xception_model =  tf.keras.applications.Xception(weights=None, include_top=True, input_shape=(224, 224, 1), classes=10)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"KlVMd30ZxUMQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628942691131,"user_tz":-540,"elapsed":24,"user":{"displayName":"이상민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwWA7JKqOrAuJreghl2Xa5oY8W9qpi7rlSayx_AsU=s64","userId":"07266262195408457313"}},"outputId":"0ab065b8-b443-48dd-8534-fc428ac334ba"},"source":["from tensorflow.keras.optimizers import Adam\n","Xception_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.002,epsilon=None), metrics=['accuracy'])"],"execution_count":7,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"w1haI0Zjxa74","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628942691539,"user_tz":-540,"elapsed":425,"user":{"displayName":"이상민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwWA7JKqOrAuJreghl2Xa5oY8W9qpi7rlSayx_AsU=s64","userId":"07266262195408457313"}},"outputId":"a100660c-f687-4d7f-cb80-fc73a29452b9"},"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2,\n","                             rotation_range=10,\n","                             width_shift_range=0.1,\n","                             height_shift_range=0.1)\n","\n","train_generator = datagen.flow_from_directory('./images_train', target_size=(224,224), color_mode='grayscale', class_mode='categorical', subset='training')\n","val_generator = datagen.flow_from_directory('./images_train', target_size=(224,224), color_mode='grayscale', class_mode='categorical', subset='validation')"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Found 1642 images belonging to 10 classes.\n","Found 406 images belonging to 10 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SRP2R9hdxsyY","executionInfo":{"status":"ok","timestamp":1628942691540,"user_tz":-540,"elapsed":12,"user":{"displayName":"이상민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwWA7JKqOrAuJreghl2Xa5oY8W9qpi7rlSayx_AsU=s64","userId":"07266262195408457313"}}},"source":["checkpoint = tf.keras.callbacks.ModelCheckpoint(f'/content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_Xception.h5', monitor='val_accuracy', save_best_only=True, verbose=1)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZtBpC5-0dZ9q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628942691541,"user_tz":-540,"elapsed":12,"user":{"displayName":"이상민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwWA7JKqOrAuJreghl2Xa5oY8W9qpi7rlSayx_AsU=s64","userId":"07266262195408457313"}},"outputId":"80c51890-6a2c-4f7b-82d8-36f3948963b8"},"source":["Xception_model.summary()"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Model: \"xception\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 224, 224, 1) 0                                            \n","__________________________________________________________________________________________________\n","block1_conv1 (Conv2D)           (None, 111, 111, 32) 288         input_1[0][0]                    \n","__________________________________________________________________________________________________\n","block1_conv1_bn (BatchNormaliza (None, 111, 111, 32) 128         block1_conv1[0][0]               \n","__________________________________________________________________________________________________\n","block1_conv1_act (Activation)   (None, 111, 111, 32) 0           block1_conv1_bn[0][0]            \n","__________________________________________________________________________________________________\n","block1_conv2 (Conv2D)           (None, 109, 109, 64) 18432       block1_conv1_act[0][0]           \n","__________________________________________________________________________________________________\n","block1_conv2_bn (BatchNormaliza (None, 109, 109, 64) 256         block1_conv2[0][0]               \n","__________________________________________________________________________________________________\n","block1_conv2_act (Activation)   (None, 109, 109, 64) 0           block1_conv2_bn[0][0]            \n","__________________________________________________________________________________________________\n","block2_sepconv1 (SeparableConv2 (None, 109, 109, 128 8768        block1_conv2_act[0][0]           \n","__________________________________________________________________________________________________\n","block2_sepconv1_bn (BatchNormal (None, 109, 109, 128 512         block2_sepconv1[0][0]            \n","__________________________________________________________________________________________________\n","block2_sepconv2_act (Activation (None, 109, 109, 128 0           block2_sepconv1_bn[0][0]         \n","__________________________________________________________________________________________________\n","block2_sepconv2 (SeparableConv2 (None, 109, 109, 128 17536       block2_sepconv2_act[0][0]        \n","__________________________________________________________________________________________________\n","block2_sepconv2_bn (BatchNormal (None, 109, 109, 128 512         block2_sepconv2[0][0]            \n","__________________________________________________________________________________________________\n","conv2d (Conv2D)                 (None, 55, 55, 128)  8192        block1_conv2_act[0][0]           \n","__________________________________________________________________________________________________\n","block2_pool (MaxPooling2D)      (None, 55, 55, 128)  0           block2_sepconv2_bn[0][0]         \n","__________________________________________________________________________________________________\n","batch_normalization (BatchNorma (None, 55, 55, 128)  512         conv2d[0][0]                     \n","__________________________________________________________________________________________________\n","add (Add)                       (None, 55, 55, 128)  0           block2_pool[0][0]                \n","                                                                 batch_normalization[0][0]        \n","__________________________________________________________________________________________________\n","block3_sepconv1_act (Activation (None, 55, 55, 128)  0           add[0][0]                        \n","__________________________________________________________________________________________________\n","block3_sepconv1 (SeparableConv2 (None, 55, 55, 256)  33920       block3_sepconv1_act[0][0]        \n","__________________________________________________________________________________________________\n","block3_sepconv1_bn (BatchNormal (None, 55, 55, 256)  1024        block3_sepconv1[0][0]            \n","__________________________________________________________________________________________________\n","block3_sepconv2_act (Activation (None, 55, 55, 256)  0           block3_sepconv1_bn[0][0]         \n","__________________________________________________________________________________________________\n","block3_sepconv2 (SeparableConv2 (None, 55, 55, 256)  67840       block3_sepconv2_act[0][0]        \n","__________________________________________________________________________________________________\n","block3_sepconv2_bn (BatchNormal (None, 55, 55, 256)  1024        block3_sepconv2[0][0]            \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 28, 28, 256)  32768       add[0][0]                        \n","__________________________________________________________________________________________________\n","block3_pool (MaxPooling2D)      (None, 28, 28, 256)  0           block3_sepconv2_bn[0][0]         \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 28, 28, 256)  1024        conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","add_1 (Add)                     (None, 28, 28, 256)  0           block3_pool[0][0]                \n","                                                                 batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","block4_sepconv1_act (Activation (None, 28, 28, 256)  0           add_1[0][0]                      \n","__________________________________________________________________________________________________\n","block4_sepconv1 (SeparableConv2 (None, 28, 28, 728)  188672      block4_sepconv1_act[0][0]        \n","__________________________________________________________________________________________________\n","block4_sepconv1_bn (BatchNormal (None, 28, 28, 728)  2912        block4_sepconv1[0][0]            \n","__________________________________________________________________________________________________\n","block4_sepconv2_act (Activation (None, 28, 28, 728)  0           block4_sepconv1_bn[0][0]         \n","__________________________________________________________________________________________________\n","block4_sepconv2 (SeparableConv2 (None, 28, 28, 728)  536536      block4_sepconv2_act[0][0]        \n","__________________________________________________________________________________________________\n","block4_sepconv2_bn (BatchNormal (None, 28, 28, 728)  2912        block4_sepconv2[0][0]            \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 14, 14, 728)  186368      add_1[0][0]                      \n","__________________________________________________________________________________________________\n","block4_pool (MaxPooling2D)      (None, 14, 14, 728)  0           block4_sepconv2_bn[0][0]         \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 14, 14, 728)  2912        conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","add_2 (Add)                     (None, 14, 14, 728)  0           block4_pool[0][0]                \n","                                                                 batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","block5_sepconv1_act (Activation (None, 14, 14, 728)  0           add_2[0][0]                      \n","__________________________________________________________________________________________________\n","block5_sepconv1 (SeparableConv2 (None, 14, 14, 728)  536536      block5_sepconv1_act[0][0]        \n","__________________________________________________________________________________________________\n","block5_sepconv1_bn (BatchNormal (None, 14, 14, 728)  2912        block5_sepconv1[0][0]            \n","__________________________________________________________________________________________________\n","block5_sepconv2_act (Activation (None, 14, 14, 728)  0           block5_sepconv1_bn[0][0]         \n","__________________________________________________________________________________________________\n","block5_sepconv2 (SeparableConv2 (None, 14, 14, 728)  536536      block5_sepconv2_act[0][0]        \n","__________________________________________________________________________________________________\n","block5_sepconv2_bn (BatchNormal (None, 14, 14, 728)  2912        block5_sepconv2[0][0]            \n","__________________________________________________________________________________________________\n","block5_sepconv3_act (Activation (None, 14, 14, 728)  0           block5_sepconv2_bn[0][0]         \n","__________________________________________________________________________________________________\n","block5_sepconv3 (SeparableConv2 (None, 14, 14, 728)  536536      block5_sepconv3_act[0][0]        \n","__________________________________________________________________________________________________\n","block5_sepconv3_bn (BatchNormal (None, 14, 14, 728)  2912        block5_sepconv3[0][0]            \n","__________________________________________________________________________________________________\n","add_3 (Add)                     (None, 14, 14, 728)  0           block5_sepconv3_bn[0][0]         \n","                                                                 add_2[0][0]                      \n","__________________________________________________________________________________________________\n","block6_sepconv1_act (Activation (None, 14, 14, 728)  0           add_3[0][0]                      \n","__________________________________________________________________________________________________\n","block6_sepconv1 (SeparableConv2 (None, 14, 14, 728)  536536      block6_sepconv1_act[0][0]        \n","__________________________________________________________________________________________________\n","block6_sepconv1_bn (BatchNormal (None, 14, 14, 728)  2912        block6_sepconv1[0][0]            \n","__________________________________________________________________________________________________\n","block6_sepconv2_act (Activation (None, 14, 14, 728)  0           block6_sepconv1_bn[0][0]         \n","__________________________________________________________________________________________________\n","block6_sepconv2 (SeparableConv2 (None, 14, 14, 728)  536536      block6_sepconv2_act[0][0]        \n","__________________________________________________________________________________________________\n","block6_sepconv2_bn (BatchNormal (None, 14, 14, 728)  2912        block6_sepconv2[0][0]            \n","__________________________________________________________________________________________________\n","block6_sepconv3_act (Activation (None, 14, 14, 728)  0           block6_sepconv2_bn[0][0]         \n","__________________________________________________________________________________________________\n","block6_sepconv3 (SeparableConv2 (None, 14, 14, 728)  536536      block6_sepconv3_act[0][0]        \n","__________________________________________________________________________________________________\n","block6_sepconv3_bn (BatchNormal (None, 14, 14, 728)  2912        block6_sepconv3[0][0]            \n","__________________________________________________________________________________________________\n","add_4 (Add)                     (None, 14, 14, 728)  0           block6_sepconv3_bn[0][0]         \n","                                                                 add_3[0][0]                      \n","__________________________________________________________________________________________________\n","block7_sepconv1_act (Activation (None, 14, 14, 728)  0           add_4[0][0]                      \n","__________________________________________________________________________________________________\n","block7_sepconv1 (SeparableConv2 (None, 14, 14, 728)  536536      block7_sepconv1_act[0][0]        \n","__________________________________________________________________________________________________\n","block7_sepconv1_bn (BatchNormal (None, 14, 14, 728)  2912        block7_sepconv1[0][0]            \n","__________________________________________________________________________________________________\n","block7_sepconv2_act (Activation (None, 14, 14, 728)  0           block7_sepconv1_bn[0][0]         \n","__________________________________________________________________________________________________\n","block7_sepconv2 (SeparableConv2 (None, 14, 14, 728)  536536      block7_sepconv2_act[0][0]        \n","__________________________________________________________________________________________________\n","block7_sepconv2_bn (BatchNormal (None, 14, 14, 728)  2912        block7_sepconv2[0][0]            \n","__________________________________________________________________________________________________\n","block7_sepconv3_act (Activation (None, 14, 14, 728)  0           block7_sepconv2_bn[0][0]         \n","__________________________________________________________________________________________________\n","block7_sepconv3 (SeparableConv2 (None, 14, 14, 728)  536536      block7_sepconv3_act[0][0]        \n","__________________________________________________________________________________________________\n","block7_sepconv3_bn (BatchNormal (None, 14, 14, 728)  2912        block7_sepconv3[0][0]            \n","__________________________________________________________________________________________________\n","add_5 (Add)                     (None, 14, 14, 728)  0           block7_sepconv3_bn[0][0]         \n","                                                                 add_4[0][0]                      \n","__________________________________________________________________________________________________\n","block8_sepconv1_act (Activation (None, 14, 14, 728)  0           add_5[0][0]                      \n","__________________________________________________________________________________________________\n","block8_sepconv1 (SeparableConv2 (None, 14, 14, 728)  536536      block8_sepconv1_act[0][0]        \n","__________________________________________________________________________________________________\n","block8_sepconv1_bn (BatchNormal (None, 14, 14, 728)  2912        block8_sepconv1[0][0]            \n","__________________________________________________________________________________________________\n","block8_sepconv2_act (Activation (None, 14, 14, 728)  0           block8_sepconv1_bn[0][0]         \n","__________________________________________________________________________________________________\n","block8_sepconv2 (SeparableConv2 (None, 14, 14, 728)  536536      block8_sepconv2_act[0][0]        \n","__________________________________________________________________________________________________\n","block8_sepconv2_bn (BatchNormal (None, 14, 14, 728)  2912        block8_sepconv2[0][0]            \n","__________________________________________________________________________________________________\n","block8_sepconv3_act (Activation (None, 14, 14, 728)  0           block8_sepconv2_bn[0][0]         \n","__________________________________________________________________________________________________\n","block8_sepconv3 (SeparableConv2 (None, 14, 14, 728)  536536      block8_sepconv3_act[0][0]        \n","__________________________________________________________________________________________________\n","block8_sepconv3_bn (BatchNormal (None, 14, 14, 728)  2912        block8_sepconv3[0][0]            \n","__________________________________________________________________________________________________\n","add_6 (Add)                     (None, 14, 14, 728)  0           block8_sepconv3_bn[0][0]         \n","                                                                 add_5[0][0]                      \n","__________________________________________________________________________________________________\n","block9_sepconv1_act (Activation (None, 14, 14, 728)  0           add_6[0][0]                      \n","__________________________________________________________________________________________________\n","block9_sepconv1 (SeparableConv2 (None, 14, 14, 728)  536536      block9_sepconv1_act[0][0]        \n","__________________________________________________________________________________________________\n","block9_sepconv1_bn (BatchNormal (None, 14, 14, 728)  2912        block9_sepconv1[0][0]            \n","__________________________________________________________________________________________________\n","block9_sepconv2_act (Activation (None, 14, 14, 728)  0           block9_sepconv1_bn[0][0]         \n","__________________________________________________________________________________________________\n","block9_sepconv2 (SeparableConv2 (None, 14, 14, 728)  536536      block9_sepconv2_act[0][0]        \n","__________________________________________________________________________________________________\n","block9_sepconv2_bn (BatchNormal (None, 14, 14, 728)  2912        block9_sepconv2[0][0]            \n","__________________________________________________________________________________________________\n","block9_sepconv3_act (Activation (None, 14, 14, 728)  0           block9_sepconv2_bn[0][0]         \n","__________________________________________________________________________________________________\n","block9_sepconv3 (SeparableConv2 (None, 14, 14, 728)  536536      block9_sepconv3_act[0][0]        \n","__________________________________________________________________________________________________\n","block9_sepconv3_bn (BatchNormal (None, 14, 14, 728)  2912        block9_sepconv3[0][0]            \n","__________________________________________________________________________________________________\n","add_7 (Add)                     (None, 14, 14, 728)  0           block9_sepconv3_bn[0][0]         \n","                                                                 add_6[0][0]                      \n","__________________________________________________________________________________________________\n","block10_sepconv1_act (Activatio (None, 14, 14, 728)  0           add_7[0][0]                      \n","__________________________________________________________________________________________________\n","block10_sepconv1 (SeparableConv (None, 14, 14, 728)  536536      block10_sepconv1_act[0][0]       \n","__________________________________________________________________________________________________\n","block10_sepconv1_bn (BatchNorma (None, 14, 14, 728)  2912        block10_sepconv1[0][0]           \n","__________________________________________________________________________________________________\n","block10_sepconv2_act (Activatio (None, 14, 14, 728)  0           block10_sepconv1_bn[0][0]        \n","__________________________________________________________________________________________________\n","block10_sepconv2 (SeparableConv (None, 14, 14, 728)  536536      block10_sepconv2_act[0][0]       \n","__________________________________________________________________________________________________\n","block10_sepconv2_bn (BatchNorma (None, 14, 14, 728)  2912        block10_sepconv2[0][0]           \n","__________________________________________________________________________________________________\n","block10_sepconv3_act (Activatio (None, 14, 14, 728)  0           block10_sepconv2_bn[0][0]        \n","__________________________________________________________________________________________________\n","block10_sepconv3 (SeparableConv (None, 14, 14, 728)  536536      block10_sepconv3_act[0][0]       \n","__________________________________________________________________________________________________\n","block10_sepconv3_bn (BatchNorma (None, 14, 14, 728)  2912        block10_sepconv3[0][0]           \n","__________________________________________________________________________________________________\n","add_8 (Add)                     (None, 14, 14, 728)  0           block10_sepconv3_bn[0][0]        \n","                                                                 add_7[0][0]                      \n","__________________________________________________________________________________________________\n","block11_sepconv1_act (Activatio (None, 14, 14, 728)  0           add_8[0][0]                      \n","__________________________________________________________________________________________________\n","block11_sepconv1 (SeparableConv (None, 14, 14, 728)  536536      block11_sepconv1_act[0][0]       \n","__________________________________________________________________________________________________\n","block11_sepconv1_bn (BatchNorma (None, 14, 14, 728)  2912        block11_sepconv1[0][0]           \n","__________________________________________________________________________________________________\n","block11_sepconv2_act (Activatio (None, 14, 14, 728)  0           block11_sepconv1_bn[0][0]        \n","__________________________________________________________________________________________________\n","block11_sepconv2 (SeparableConv (None, 14, 14, 728)  536536      block11_sepconv2_act[0][0]       \n","__________________________________________________________________________________________________\n","block11_sepconv2_bn (BatchNorma (None, 14, 14, 728)  2912        block11_sepconv2[0][0]           \n","__________________________________________________________________________________________________\n","block11_sepconv3_act (Activatio (None, 14, 14, 728)  0           block11_sepconv2_bn[0][0]        \n","__________________________________________________________________________________________________\n","block11_sepconv3 (SeparableConv (None, 14, 14, 728)  536536      block11_sepconv3_act[0][0]       \n","__________________________________________________________________________________________________\n","block11_sepconv3_bn (BatchNorma (None, 14, 14, 728)  2912        block11_sepconv3[0][0]           \n","__________________________________________________________________________________________________\n","add_9 (Add)                     (None, 14, 14, 728)  0           block11_sepconv3_bn[0][0]        \n","                                                                 add_8[0][0]                      \n","__________________________________________________________________________________________________\n","block12_sepconv1_act (Activatio (None, 14, 14, 728)  0           add_9[0][0]                      \n","__________________________________________________________________________________________________\n","block12_sepconv1 (SeparableConv (None, 14, 14, 728)  536536      block12_sepconv1_act[0][0]       \n","__________________________________________________________________________________________________\n","block12_sepconv1_bn (BatchNorma (None, 14, 14, 728)  2912        block12_sepconv1[0][0]           \n","__________________________________________________________________________________________________\n","block12_sepconv2_act (Activatio (None, 14, 14, 728)  0           block12_sepconv1_bn[0][0]        \n","__________________________________________________________________________________________________\n","block12_sepconv2 (SeparableConv (None, 14, 14, 728)  536536      block12_sepconv2_act[0][0]       \n","__________________________________________________________________________________________________\n","block12_sepconv2_bn (BatchNorma (None, 14, 14, 728)  2912        block12_sepconv2[0][0]           \n","__________________________________________________________________________________________________\n","block12_sepconv3_act (Activatio (None, 14, 14, 728)  0           block12_sepconv2_bn[0][0]        \n","__________________________________________________________________________________________________\n","block12_sepconv3 (SeparableConv (None, 14, 14, 728)  536536      block12_sepconv3_act[0][0]       \n","__________________________________________________________________________________________________\n","block12_sepconv3_bn (BatchNorma (None, 14, 14, 728)  2912        block12_sepconv3[0][0]           \n","__________________________________________________________________________________________________\n","add_10 (Add)                    (None, 14, 14, 728)  0           block12_sepconv3_bn[0][0]        \n","                                                                 add_9[0][0]                      \n","__________________________________________________________________________________________________\n","block13_sepconv1_act (Activatio (None, 14, 14, 728)  0           add_10[0][0]                     \n","__________________________________________________________________________________________________\n","block13_sepconv1 (SeparableConv (None, 14, 14, 728)  536536      block13_sepconv1_act[0][0]       \n","__________________________________________________________________________________________________\n","block13_sepconv1_bn (BatchNorma (None, 14, 14, 728)  2912        block13_sepconv1[0][0]           \n","__________________________________________________________________________________________________\n","block13_sepconv2_act (Activatio (None, 14, 14, 728)  0           block13_sepconv1_bn[0][0]        \n","__________________________________________________________________________________________________\n","block13_sepconv2 (SeparableConv (None, 14, 14, 1024) 752024      block13_sepconv2_act[0][0]       \n","__________________________________________________________________________________________________\n","block13_sepconv2_bn (BatchNorma (None, 14, 14, 1024) 4096        block13_sepconv2[0][0]           \n","__________________________________________________________________________________________________\n","conv2d_3 (Conv2D)               (None, 7, 7, 1024)   745472      add_10[0][0]                     \n","__________________________________________________________________________________________________\n","block13_pool (MaxPooling2D)     (None, 7, 7, 1024)   0           block13_sepconv2_bn[0][0]        \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 7, 7, 1024)   4096        conv2d_3[0][0]                   \n","__________________________________________________________________________________________________\n","add_11 (Add)                    (None, 7, 7, 1024)   0           block13_pool[0][0]               \n","                                                                 batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","block14_sepconv1 (SeparableConv (None, 7, 7, 1536)   1582080     add_11[0][0]                     \n","__________________________________________________________________________________________________\n","block14_sepconv1_bn (BatchNorma (None, 7, 7, 1536)   6144        block14_sepconv1[0][0]           \n","__________________________________________________________________________________________________\n","block14_sepconv1_act (Activatio (None, 7, 7, 1536)   0           block14_sepconv1_bn[0][0]        \n","__________________________________________________________________________________________________\n","block14_sepconv2 (SeparableConv (None, 7, 7, 2048)   3159552     block14_sepconv1_act[0][0]       \n","__________________________________________________________________________________________________\n","block14_sepconv2_bn (BatchNorma (None, 7, 7, 2048)   8192        block14_sepconv2[0][0]           \n","__________________________________________________________________________________________________\n","block14_sepconv2_act (Activatio (None, 7, 7, 2048)   0           block14_sepconv2_bn[0][0]        \n","__________________________________________________________________________________________________\n","avg_pool (GlobalAveragePooling2 (None, 2048)         0           block14_sepconv2_act[0][0]       \n","__________________________________________________________________________________________________\n","predictions (Dense)             (None, 10)           20490       avg_pool[0][0]                   \n","==================================================================================================\n","Total params: 20,881,394\n","Trainable params: 20,826,866\n","Non-trainable params: 54,528\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DKMJhbFnxotA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628953407277,"user_tz":-540,"elapsed":608082,"user":{"displayName":"이상민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwWA7JKqOrAuJreghl2Xa5oY8W9qpi7rlSayx_AsU=s64","userId":"07266262195408457313"}},"outputId":"5083b8a0-f2e5-4b85-9663-4fd0d6ede88f"},"source":["Xception_model.fit_generator(train_generator, epochs=500, validation_data=val_generator, callbacks=[checkpoint])"],"execution_count":11,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  warnings.warn('`Model.fit_generator` is deprecated and '\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/500\n","52/52 [==============================] - 43s 436ms/step - loss: 1.7877 - accuracy: 0.3843 - val_loss: 2.3029 - val_accuracy: 0.0985\n","\n","Epoch 00001: val_accuracy improved from -inf to 0.09852, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_Xception.h5\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  category=CustomMaskWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 2/500\n","52/52 [==============================] - 21s 404ms/step - loss: 1.0805 - accuracy: 0.6322 - val_loss: 2.3066 - val_accuracy: 0.0985\n","\n","Epoch 00002: val_accuracy did not improve from 0.09852\n","Epoch 3/500\n","52/52 [==============================] - 21s 404ms/step - loss: 0.7949 - accuracy: 0.7272 - val_loss: 2.3276 - val_accuracy: 0.0985\n","\n","Epoch 00003: val_accuracy did not improve from 0.09852\n","Epoch 4/500\n","52/52 [==============================] - 21s 403ms/step - loss: 0.7383 - accuracy: 0.7558 - val_loss: 2.4601 - val_accuracy: 0.0985\n","\n","Epoch 00004: val_accuracy did not improve from 0.09852\n","Epoch 5/500\n","52/52 [==============================] - 21s 405ms/step - loss: 0.5638 - accuracy: 0.8009 - val_loss: 2.7523 - val_accuracy: 0.1305\n","\n","Epoch 00005: val_accuracy improved from 0.09852 to 0.13054, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_Xception.h5\n","Epoch 6/500\n","52/52 [==============================] - 21s 405ms/step - loss: 0.4910 - accuracy: 0.8386 - val_loss: 3.7066 - val_accuracy: 0.0985\n","\n","Epoch 00006: val_accuracy did not improve from 0.13054\n","Epoch 7/500\n","52/52 [==============================] - 21s 404ms/step - loss: 0.4734 - accuracy: 0.8356 - val_loss: 3.7036 - val_accuracy: 0.0936\n","\n","Epoch 00007: val_accuracy did not improve from 0.13054\n","Epoch 8/500\n","52/52 [==============================] - 21s 403ms/step - loss: 0.4140 - accuracy: 0.8666 - val_loss: 3.1633 - val_accuracy: 0.0985\n","\n","Epoch 00008: val_accuracy did not improve from 0.13054\n","Epoch 9/500\n","52/52 [==============================] - 21s 403ms/step - loss: 0.3881 - accuracy: 0.8733 - val_loss: 5.0752 - val_accuracy: 0.1355\n","\n","Epoch 00009: val_accuracy improved from 0.13054 to 0.13547, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_Xception.h5\n","Epoch 10/500\n","52/52 [==============================] - 21s 403ms/step - loss: 0.3992 - accuracy: 0.8697 - val_loss: 3.3878 - val_accuracy: 0.1897\n","\n","Epoch 00010: val_accuracy improved from 0.13547 to 0.18966, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_Xception.h5\n","Epoch 11/500\n","52/52 [==============================] - 21s 404ms/step - loss: 0.3186 - accuracy: 0.8928 - val_loss: 4.9756 - val_accuracy: 0.2389\n","\n","Epoch 00011: val_accuracy improved from 0.18966 to 0.23892, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_Xception.h5\n","Epoch 12/500\n","52/52 [==============================] - 21s 404ms/step - loss: 0.2876 - accuracy: 0.9026 - val_loss: 3.6997 - val_accuracy: 0.4532\n","\n","Epoch 00012: val_accuracy improved from 0.23892 to 0.45320, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_Xception.h5\n","Epoch 13/500\n","52/52 [==============================] - 21s 403ms/step - loss: 0.3195 - accuracy: 0.8886 - val_loss: 2.4237 - val_accuracy: 0.5985\n","\n","Epoch 00013: val_accuracy improved from 0.45320 to 0.59852, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_Xception.h5\n","Epoch 14/500\n","52/52 [==============================] - 21s 403ms/step - loss: 0.2766 - accuracy: 0.8989 - val_loss: 2.0422 - val_accuracy: 0.6182\n","\n","Epoch 00014: val_accuracy improved from 0.59852 to 0.61823, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_Xception.h5\n","Epoch 15/500\n","52/52 [==============================] - 21s 404ms/step - loss: 0.2702 - accuracy: 0.9062 - val_loss: 2.9584 - val_accuracy: 0.5320\n","\n","Epoch 00015: val_accuracy did not improve from 0.61823\n","Epoch 16/500\n","52/52 [==============================] - 21s 403ms/step - loss: 0.2620 - accuracy: 0.9147 - val_loss: 2.0421 - val_accuracy: 0.5936\n","\n","Epoch 00016: val_accuracy did not improve from 0.61823\n","Epoch 17/500\n","52/52 [==============================] - 21s 404ms/step - loss: 0.1909 - accuracy: 0.9373 - val_loss: 0.7357 - val_accuracy: 0.8054\n","\n","Epoch 00017: val_accuracy improved from 0.61823 to 0.80542, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_Xception.h5\n","Epoch 18/500\n","52/52 [==============================] - 21s 403ms/step - loss: 0.1639 - accuracy: 0.9464 - val_loss: 0.6826 - val_accuracy: 0.8325\n","\n","Epoch 00018: val_accuracy improved from 0.80542 to 0.83251, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_Xception.h5\n","Epoch 19/500\n","52/52 [==============================] - 21s 403ms/step - loss: 0.2327 - accuracy: 0.9263 - val_loss: 1.5471 - val_accuracy: 0.7020\n","\n","Epoch 00019: val_accuracy did not improve from 0.83251\n","Epoch 20/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.1774 - accuracy: 0.9440 - val_loss: 0.9966 - val_accuracy: 0.7685\n","\n","Epoch 00020: val_accuracy did not improve from 0.83251\n","Epoch 21/500\n","52/52 [==============================] - 21s 403ms/step - loss: 0.1962 - accuracy: 0.9287 - val_loss: 0.9864 - val_accuracy: 0.8030\n","\n","Epoch 00021: val_accuracy did not improve from 0.83251\n","Epoch 22/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.2530 - accuracy: 0.9196 - val_loss: 6.8013 - val_accuracy: 0.3522\n","\n","Epoch 00022: val_accuracy did not improve from 0.83251\n","Epoch 23/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.2374 - accuracy: 0.9147 - val_loss: 1.4273 - val_accuracy: 0.6995\n","\n","Epoch 00023: val_accuracy did not improve from 0.83251\n","Epoch 24/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.1578 - accuracy: 0.9495 - val_loss: 1.2309 - val_accuracy: 0.8079\n","\n","Epoch 00024: val_accuracy did not improve from 0.83251\n","Epoch 25/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.1123 - accuracy: 0.9622 - val_loss: 2.4119 - val_accuracy: 0.5985\n","\n","Epoch 00025: val_accuracy did not improve from 0.83251\n","Epoch 26/500\n","52/52 [==============================] - 21s 403ms/step - loss: 0.1480 - accuracy: 0.9555 - val_loss: 0.7067 - val_accuracy: 0.8424\n","\n","Epoch 00026: val_accuracy improved from 0.83251 to 0.84236, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_Xception.h5\n","Epoch 27/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.1106 - accuracy: 0.9647 - val_loss: 0.4787 - val_accuracy: 0.8695\n","\n","Epoch 00027: val_accuracy improved from 0.84236 to 0.86946, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_Xception.h5\n","Epoch 28/500\n","52/52 [==============================] - 21s 406ms/step - loss: 0.1044 - accuracy: 0.9641 - val_loss: 0.7950 - val_accuracy: 0.8030\n","\n","Epoch 00028: val_accuracy did not improve from 0.86946\n","Epoch 29/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0882 - accuracy: 0.9683 - val_loss: 0.9048 - val_accuracy: 0.7882\n","\n","Epoch 00029: val_accuracy did not improve from 0.86946\n","Epoch 30/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.1045 - accuracy: 0.9653 - val_loss: 0.5821 - val_accuracy: 0.8325\n","\n","Epoch 00030: val_accuracy did not improve from 0.86946\n","Epoch 31/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.1759 - accuracy: 0.9421 - val_loss: 2.4029 - val_accuracy: 0.5813\n","\n","Epoch 00031: val_accuracy did not improve from 0.86946\n","Epoch 32/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.1123 - accuracy: 0.9580 - val_loss: 0.8657 - val_accuracy: 0.8399\n","\n","Epoch 00032: val_accuracy did not improve from 0.86946\n","Epoch 33/500\n","52/52 [==============================] - 21s 403ms/step - loss: 0.0973 - accuracy: 0.9641 - val_loss: 0.5715 - val_accuracy: 0.8793\n","\n","Epoch 00033: val_accuracy improved from 0.86946 to 0.87931, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_Xception.h5\n","Epoch 34/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0854 - accuracy: 0.9714 - val_loss: 1.0854 - val_accuracy: 0.7709\n","\n","Epoch 00034: val_accuracy did not improve from 0.87931\n","Epoch 35/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.1175 - accuracy: 0.9647 - val_loss: 0.5469 - val_accuracy: 0.8768\n","\n","Epoch 00035: val_accuracy did not improve from 0.87931\n","Epoch 36/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.1723 - accuracy: 0.9464 - val_loss: 5.2415 - val_accuracy: 0.4236\n","\n","Epoch 00036: val_accuracy did not improve from 0.87931\n","Epoch 37/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.1191 - accuracy: 0.9574 - val_loss: 3.9406 - val_accuracy: 0.5369\n","\n","Epoch 00037: val_accuracy did not improve from 0.87931\n","Epoch 38/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.1199 - accuracy: 0.9616 - val_loss: 1.8106 - val_accuracy: 0.7167\n","\n","Epoch 00038: val_accuracy did not improve from 0.87931\n","Epoch 39/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.1145 - accuracy: 0.9616 - val_loss: 1.5542 - val_accuracy: 0.7586\n","\n","Epoch 00039: val_accuracy did not improve from 0.87931\n","Epoch 40/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0820 - accuracy: 0.9732 - val_loss: 1.6228 - val_accuracy: 0.7512\n","\n","Epoch 00040: val_accuracy did not improve from 0.87931\n","Epoch 41/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.1468 - accuracy: 0.9586 - val_loss: 1.5408 - val_accuracy: 0.7143\n","\n","Epoch 00041: val_accuracy did not improve from 0.87931\n","Epoch 42/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.1051 - accuracy: 0.9665 - val_loss: 0.6842 - val_accuracy: 0.8473\n","\n","Epoch 00042: val_accuracy did not improve from 0.87931\n","Epoch 43/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0664 - accuracy: 0.9769 - val_loss: 0.6779 - val_accuracy: 0.8621\n","\n","Epoch 00043: val_accuracy did not improve from 0.87931\n","Epoch 44/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0487 - accuracy: 0.9854 - val_loss: 0.5785 - val_accuracy: 0.8448\n","\n","Epoch 00044: val_accuracy did not improve from 0.87931\n","Epoch 45/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0587 - accuracy: 0.9787 - val_loss: 0.7337 - val_accuracy: 0.8424\n","\n","Epoch 00045: val_accuracy did not improve from 0.87931\n","Epoch 46/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0649 - accuracy: 0.9787 - val_loss: 0.4630 - val_accuracy: 0.8892\n","\n","Epoch 00046: val_accuracy improved from 0.87931 to 0.88916, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_Xception.h5\n","Epoch 47/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0634 - accuracy: 0.9817 - val_loss: 0.9062 - val_accuracy: 0.8325\n","\n","Epoch 00047: val_accuracy did not improve from 0.88916\n","Epoch 48/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0376 - accuracy: 0.9848 - val_loss: 0.4855 - val_accuracy: 0.8719\n","\n","Epoch 00048: val_accuracy did not improve from 0.88916\n","Epoch 49/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0677 - accuracy: 0.9775 - val_loss: 0.9234 - val_accuracy: 0.8153\n","\n","Epoch 00049: val_accuracy did not improve from 0.88916\n","Epoch 50/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0565 - accuracy: 0.9799 - val_loss: 0.9783 - val_accuracy: 0.8128\n","\n","Epoch 00050: val_accuracy did not improve from 0.88916\n","Epoch 51/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0586 - accuracy: 0.9811 - val_loss: 1.2849 - val_accuracy: 0.7906\n","\n","Epoch 00051: val_accuracy did not improve from 0.88916\n","Epoch 52/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0561 - accuracy: 0.9781 - val_loss: 0.5861 - val_accuracy: 0.8645\n","\n","Epoch 00052: val_accuracy did not improve from 0.88916\n","Epoch 53/500\n","52/52 [==============================] - 21s 406ms/step - loss: 0.0301 - accuracy: 0.9909 - val_loss: 0.5531 - val_accuracy: 0.8867\n","\n","Epoch 00053: val_accuracy did not improve from 0.88916\n","Epoch 54/500\n","52/52 [==============================] - 21s 405ms/step - loss: 0.0229 - accuracy: 0.9909 - val_loss: 0.4503 - val_accuracy: 0.8867\n","\n","Epoch 00054: val_accuracy did not improve from 0.88916\n","Epoch 55/500\n","52/52 [==============================] - 21s 403ms/step - loss: 0.0296 - accuracy: 0.9884 - val_loss: 1.0788 - val_accuracy: 0.8079\n","\n","Epoch 00055: val_accuracy did not improve from 0.88916\n","Epoch 56/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0823 - accuracy: 0.9726 - val_loss: 1.3632 - val_accuracy: 0.7266\n","\n","Epoch 00056: val_accuracy did not improve from 0.88916\n","Epoch 57/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.1876 - accuracy: 0.9446 - val_loss: 1.3999 - val_accuracy: 0.7586\n","\n","Epoch 00057: val_accuracy did not improve from 0.88916\n","Epoch 58/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.1228 - accuracy: 0.9641 - val_loss: 2.2245 - val_accuracy: 0.6601\n","\n","Epoch 00058: val_accuracy did not improve from 0.88916\n","Epoch 59/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0573 - accuracy: 0.9756 - val_loss: 0.9307 - val_accuracy: 0.8325\n","\n","Epoch 00059: val_accuracy did not improve from 0.88916\n","Epoch 60/500\n","52/52 [==============================] - 21s 407ms/step - loss: 0.0687 - accuracy: 0.9744 - val_loss: 0.6061 - val_accuracy: 0.8842\n","\n","Epoch 00060: val_accuracy did not improve from 0.88916\n","Epoch 61/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0355 - accuracy: 0.9884 - val_loss: 0.5636 - val_accuracy: 0.8892\n","\n","Epoch 00061: val_accuracy did not improve from 0.88916\n","Epoch 62/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0474 - accuracy: 0.9836 - val_loss: 0.6057 - val_accuracy: 0.8522\n","\n","Epoch 00062: val_accuracy did not improve from 0.88916\n","Epoch 63/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0331 - accuracy: 0.9872 - val_loss: 0.9937 - val_accuracy: 0.8300\n","\n","Epoch 00063: val_accuracy did not improve from 0.88916\n","Epoch 64/500\n","52/52 [==============================] - 21s 399ms/step - loss: 0.0396 - accuracy: 0.9848 - val_loss: 0.8466 - val_accuracy: 0.8350\n","\n","Epoch 00064: val_accuracy did not improve from 0.88916\n","Epoch 65/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0353 - accuracy: 0.9890 - val_loss: 0.8932 - val_accuracy: 0.8300\n","\n","Epoch 00065: val_accuracy did not improve from 0.88916\n","Epoch 66/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0329 - accuracy: 0.9890 - val_loss: 0.9304 - val_accuracy: 0.8325\n","\n","Epoch 00066: val_accuracy did not improve from 0.88916\n","Epoch 67/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.1140 - accuracy: 0.9641 - val_loss: 1.5167 - val_accuracy: 0.7512\n","\n","Epoch 00067: val_accuracy did not improve from 0.88916\n","Epoch 68/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.1312 - accuracy: 0.9598 - val_loss: 1.2789 - val_accuracy: 0.7906\n","\n","Epoch 00068: val_accuracy did not improve from 0.88916\n","Epoch 69/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0681 - accuracy: 0.9732 - val_loss: 2.9990 - val_accuracy: 0.6305\n","\n","Epoch 00069: val_accuracy did not improve from 0.88916\n","Epoch 70/500\n","52/52 [==============================] - 21s 403ms/step - loss: 0.0644 - accuracy: 0.9769 - val_loss: 0.7450 - val_accuracy: 0.8350\n","\n","Epoch 00070: val_accuracy did not improve from 0.88916\n","Epoch 71/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0626 - accuracy: 0.9775 - val_loss: 1.0203 - val_accuracy: 0.8276\n","\n","Epoch 00071: val_accuracy did not improve from 0.88916\n","Epoch 72/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0374 - accuracy: 0.9866 - val_loss: 0.7047 - val_accuracy: 0.8645\n","\n","Epoch 00072: val_accuracy did not improve from 0.88916\n","Epoch 73/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0197 - accuracy: 0.9939 - val_loss: 0.6267 - val_accuracy: 0.8695\n","\n","Epoch 00073: val_accuracy did not improve from 0.88916\n","Epoch 74/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0129 - accuracy: 0.9970 - val_loss: 0.5713 - val_accuracy: 0.8966\n","\n","Epoch 00074: val_accuracy improved from 0.88916 to 0.89655, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_Xception.h5\n","Epoch 75/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0171 - accuracy: 0.9945 - val_loss: 0.4941 - val_accuracy: 0.8916\n","\n","Epoch 00075: val_accuracy did not improve from 0.89655\n","Epoch 76/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0371 - accuracy: 0.9866 - val_loss: 0.6494 - val_accuracy: 0.8768\n","\n","Epoch 00076: val_accuracy did not improve from 0.89655\n","Epoch 77/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0481 - accuracy: 0.9799 - val_loss: 0.6061 - val_accuracy: 0.8818\n","\n","Epoch 00077: val_accuracy did not improve from 0.89655\n","Epoch 78/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0188 - accuracy: 0.9927 - val_loss: 0.5767 - val_accuracy: 0.8941\n","\n","Epoch 00078: val_accuracy did not improve from 0.89655\n","Epoch 79/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0186 - accuracy: 0.9933 - val_loss: 0.5252 - val_accuracy: 0.8818\n","\n","Epoch 00079: val_accuracy did not improve from 0.89655\n","Epoch 80/500\n","52/52 [==============================] - 21s 405ms/step - loss: 0.0271 - accuracy: 0.9909 - val_loss: 0.6386 - val_accuracy: 0.9089\n","\n","Epoch 00080: val_accuracy improved from 0.89655 to 0.90887, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_Xception.h5\n","Epoch 81/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0178 - accuracy: 0.9933 - val_loss: 0.5645 - val_accuracy: 0.8842\n","\n","Epoch 00081: val_accuracy did not improve from 0.90887\n","Epoch 82/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0764 - accuracy: 0.9762 - val_loss: 0.9848 - val_accuracy: 0.8424\n","\n","Epoch 00082: val_accuracy did not improve from 0.90887\n","Epoch 83/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0910 - accuracy: 0.9708 - val_loss: 1.3792 - val_accuracy: 0.7833\n","\n","Epoch 00083: val_accuracy did not improve from 0.90887\n","Epoch 84/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0777 - accuracy: 0.9750 - val_loss: 0.6289 - val_accuracy: 0.8842\n","\n","Epoch 00084: val_accuracy did not improve from 0.90887\n","Epoch 85/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0510 - accuracy: 0.9848 - val_loss: 3.8940 - val_accuracy: 0.5616\n","\n","Epoch 00085: val_accuracy did not improve from 0.90887\n","Epoch 86/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.1306 - accuracy: 0.9653 - val_loss: 8.1951 - val_accuracy: 0.3571\n","\n","Epoch 00086: val_accuracy did not improve from 0.90887\n","Epoch 87/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0906 - accuracy: 0.9695 - val_loss: 1.2368 - val_accuracy: 0.7833\n","\n","Epoch 00087: val_accuracy did not improve from 0.90887\n","Epoch 88/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0709 - accuracy: 0.9769 - val_loss: 0.9763 - val_accuracy: 0.8202\n","\n","Epoch 00088: val_accuracy did not improve from 0.90887\n","Epoch 89/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0453 - accuracy: 0.9836 - val_loss: 0.5734 - val_accuracy: 0.8670\n","\n","Epoch 00089: val_accuracy did not improve from 0.90887\n","Epoch 90/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0261 - accuracy: 0.9896 - val_loss: 0.7703 - val_accuracy: 0.8547\n","\n","Epoch 00090: val_accuracy did not improve from 0.90887\n","Epoch 91/500\n","52/52 [==============================] - 21s 404ms/step - loss: 0.0299 - accuracy: 0.9915 - val_loss: 0.8094 - val_accuracy: 0.8892\n","\n","Epoch 00091: val_accuracy did not improve from 0.90887\n","Epoch 92/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0085 - accuracy: 0.9982 - val_loss: 0.3873 - val_accuracy: 0.9089\n","\n","Epoch 00092: val_accuracy did not improve from 0.90887\n","Epoch 93/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0136 - accuracy: 0.9957 - val_loss: 0.5539 - val_accuracy: 0.8941\n","\n","Epoch 00093: val_accuracy did not improve from 0.90887\n","Epoch 94/500\n","52/52 [==============================] - 21s 399ms/step - loss: 0.0118 - accuracy: 0.9957 - val_loss: 0.3450 - val_accuracy: 0.9335\n","\n","Epoch 00094: val_accuracy improved from 0.90887 to 0.93350, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_Xception.h5\n","Epoch 95/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0041 - accuracy: 0.9994 - val_loss: 0.4793 - val_accuracy: 0.9064\n","\n","Epoch 00095: val_accuracy did not improve from 0.93350\n","Epoch 96/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0035 - accuracy: 0.9988 - val_loss: 0.4420 - val_accuracy: 0.9163\n","\n","Epoch 00096: val_accuracy did not improve from 0.93350\n","Epoch 97/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.4034 - val_accuracy: 0.9163\n","\n","Epoch 00097: val_accuracy did not improve from 0.93350\n","Epoch 98/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.3724 - val_accuracy: 0.9212\n","\n","Epoch 00098: val_accuracy did not improve from 0.93350\n","Epoch 99/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4084 - val_accuracy: 0.9236\n","\n","Epoch 00099: val_accuracy did not improve from 0.93350\n","Epoch 100/500\n","52/52 [==============================] - 21s 400ms/step - loss: 3.5577e-04 - accuracy: 1.0000 - val_loss: 0.3663 - val_accuracy: 0.9310\n","\n","Epoch 00100: val_accuracy did not improve from 0.93350\n","Epoch 101/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0016 - accuracy: 0.9994 - val_loss: 0.3727 - val_accuracy: 0.9360\n","\n","Epoch 00101: val_accuracy improved from 0.93350 to 0.93596, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_Xception.h5\n","Epoch 102/500\n","52/52 [==============================] - 21s 401ms/step - loss: 9.3711e-04 - accuracy: 1.0000 - val_loss: 0.4216 - val_accuracy: 0.9286\n","\n","Epoch 00102: val_accuracy did not improve from 0.93596\n","Epoch 103/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0016 - accuracy: 0.9994 - val_loss: 0.4833 - val_accuracy: 0.9187\n","\n","Epoch 00103: val_accuracy did not improve from 0.93596\n","Epoch 104/500\n","52/52 [==============================] - 21s 401ms/step - loss: 9.6515e-04 - accuracy: 1.0000 - val_loss: 0.4965 - val_accuracy: 0.9138\n","\n","Epoch 00104: val_accuracy did not improve from 0.93596\n","Epoch 105/500\n","52/52 [==============================] - 21s 400ms/step - loss: 7.2569e-04 - accuracy: 1.0000 - val_loss: 0.3934 - val_accuracy: 0.9261\n","\n","Epoch 00105: val_accuracy did not improve from 0.93596\n","Epoch 106/500\n","52/52 [==============================] - 21s 400ms/step - loss: 8.6410e-04 - accuracy: 1.0000 - val_loss: 0.4432 - val_accuracy: 0.9261\n","\n","Epoch 00106: val_accuracy did not improve from 0.93596\n","Epoch 107/500\n","52/52 [==============================] - 21s 401ms/step - loss: 7.0522e-04 - accuracy: 1.0000 - val_loss: 0.4033 - val_accuracy: 0.9261\n","\n","Epoch 00107: val_accuracy did not improve from 0.93596\n","Epoch 108/500\n","52/52 [==============================] - 21s 401ms/step - loss: 4.7752e-04 - accuracy: 1.0000 - val_loss: 0.4048 - val_accuracy: 0.9384\n","\n","Epoch 00108: val_accuracy improved from 0.93596 to 0.93842, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_Xception.h5\n","Epoch 109/500\n","52/52 [==============================] - 21s 401ms/step - loss: 2.1132e-04 - accuracy: 1.0000 - val_loss: 0.3766 - val_accuracy: 0.9236\n","\n","Epoch 00109: val_accuracy did not improve from 0.93842\n","Epoch 110/500\n","52/52 [==============================] - 21s 399ms/step - loss: 3.1176e-04 - accuracy: 1.0000 - val_loss: 0.3948 - val_accuracy: 0.9212\n","\n","Epoch 00110: val_accuracy did not improve from 0.93842\n","Epoch 111/500\n","52/52 [==============================] - 21s 400ms/step - loss: 2.8389e-04 - accuracy: 1.0000 - val_loss: 0.4048 - val_accuracy: 0.9261\n","\n","Epoch 00111: val_accuracy did not improve from 0.93842\n","Epoch 112/500\n","52/52 [==============================] - 21s 401ms/step - loss: 2.2766e-04 - accuracy: 1.0000 - val_loss: 0.3842 - val_accuracy: 0.9261\n","\n","Epoch 00112: val_accuracy did not improve from 0.93842\n","Epoch 113/500\n","52/52 [==============================] - 21s 401ms/step - loss: 6.1705e-04 - accuracy: 1.0000 - val_loss: 0.4680 - val_accuracy: 0.9236\n","\n","Epoch 00113: val_accuracy did not improve from 0.93842\n","Epoch 114/500\n","52/52 [==============================] - 21s 401ms/step - loss: 6.1849e-04 - accuracy: 1.0000 - val_loss: 0.4037 - val_accuracy: 0.9335\n","\n","Epoch 00114: val_accuracy did not improve from 0.93842\n","Epoch 115/500\n","52/52 [==============================] - 21s 399ms/step - loss: 1.1971e-04 - accuracy: 1.0000 - val_loss: 0.3884 - val_accuracy: 0.9335\n","\n","Epoch 00115: val_accuracy did not improve from 0.93842\n","Epoch 116/500\n","52/52 [==============================] - 21s 399ms/step - loss: 3.0450e-04 - accuracy: 1.0000 - val_loss: 0.4129 - val_accuracy: 0.9187\n","\n","Epoch 00116: val_accuracy did not improve from 0.93842\n","Epoch 117/500\n","52/52 [==============================] - 21s 400ms/step - loss: 1.8929e-04 - accuracy: 1.0000 - val_loss: 0.3571 - val_accuracy: 0.9236\n","\n","Epoch 00117: val_accuracy did not improve from 0.93842\n","Epoch 118/500\n","52/52 [==============================] - 21s 401ms/step - loss: 2.0019e-04 - accuracy: 1.0000 - val_loss: 0.3895 - val_accuracy: 0.9360\n","\n","Epoch 00118: val_accuracy did not improve from 0.93842\n","Epoch 119/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0131 - accuracy: 0.9976 - val_loss: 0.4967 - val_accuracy: 0.9064\n","\n","Epoch 00119: val_accuracy did not improve from 0.93842\n","Epoch 120/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0517 - accuracy: 0.9860 - val_loss: 1.8414 - val_accuracy: 0.7414\n","\n","Epoch 00120: val_accuracy did not improve from 0.93842\n","Epoch 121/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.3867 - accuracy: 0.9019 - val_loss: 52.5455 - val_accuracy: 0.1232\n","\n","Epoch 00121: val_accuracy did not improve from 0.93842\n","Epoch 122/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.2764 - accuracy: 0.9208 - val_loss: 9.7899 - val_accuracy: 0.3547\n","\n","Epoch 00122: val_accuracy did not improve from 0.93842\n","Epoch 123/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0900 - accuracy: 0.9756 - val_loss: 2.6973 - val_accuracy: 0.6675\n","\n","Epoch 00123: val_accuracy did not improve from 0.93842\n","Epoch 124/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0765 - accuracy: 0.9762 - val_loss: 2.1028 - val_accuracy: 0.7291\n","\n","Epoch 00124: val_accuracy did not improve from 0.93842\n","Epoch 125/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0354 - accuracy: 0.9921 - val_loss: 0.3846 - val_accuracy: 0.8990\n","\n","Epoch 00125: val_accuracy did not improve from 0.93842\n","Epoch 126/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0111 - accuracy: 0.9963 - val_loss: 0.5234 - val_accuracy: 0.8966\n","\n","Epoch 00126: val_accuracy did not improve from 0.93842\n","Epoch 127/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0114 - accuracy: 0.9988 - val_loss: 0.4174 - val_accuracy: 0.9089\n","\n","Epoch 00127: val_accuracy did not improve from 0.93842\n","Epoch 128/500\n","52/52 [==============================] - 21s 399ms/step - loss: 0.0183 - accuracy: 0.9945 - val_loss: 0.4779 - val_accuracy: 0.8916\n","\n","Epoch 00128: val_accuracy did not improve from 0.93842\n","Epoch 129/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0194 - accuracy: 0.9945 - val_loss: 0.5926 - val_accuracy: 0.8621\n","\n","Epoch 00129: val_accuracy did not improve from 0.93842\n","Epoch 130/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0288 - accuracy: 0.9896 - val_loss: 0.5562 - val_accuracy: 0.8892\n","\n","Epoch 00130: val_accuracy did not improve from 0.93842\n","Epoch 131/500\n","52/52 [==============================] - 21s 399ms/step - loss: 0.0329 - accuracy: 0.9915 - val_loss: 0.4484 - val_accuracy: 0.9212\n","\n","Epoch 00131: val_accuracy did not improve from 0.93842\n","Epoch 132/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0303 - accuracy: 0.9915 - val_loss: 0.4134 - val_accuracy: 0.9089\n","\n","Epoch 00132: val_accuracy did not improve from 0.93842\n","Epoch 133/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0080 - accuracy: 0.9976 - val_loss: 0.5181 - val_accuracy: 0.8916\n","\n","Epoch 00133: val_accuracy did not improve from 0.93842\n","Epoch 134/500\n","52/52 [==============================] - 21s 399ms/step - loss: 0.0192 - accuracy: 0.9927 - val_loss: 0.3884 - val_accuracy: 0.9064\n","\n","Epoch 00134: val_accuracy did not improve from 0.93842\n","Epoch 135/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0047 - accuracy: 0.9994 - val_loss: 0.3819 - val_accuracy: 0.9236\n","\n","Epoch 00135: val_accuracy did not improve from 0.93842\n","Epoch 136/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0068 - accuracy: 0.9988 - val_loss: 0.4393 - val_accuracy: 0.9015\n","\n","Epoch 00136: val_accuracy did not improve from 0.93842\n","Epoch 137/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.3874 - val_accuracy: 0.9187\n","\n","Epoch 00137: val_accuracy did not improve from 0.93842\n","Epoch 138/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0056 - accuracy: 0.9982 - val_loss: 0.7478 - val_accuracy: 0.8719\n","\n","Epoch 00138: val_accuracy did not improve from 0.93842\n","Epoch 139/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0154 - accuracy: 0.9957 - val_loss: 0.6381 - val_accuracy: 0.8744\n","\n","Epoch 00139: val_accuracy did not improve from 0.93842\n","Epoch 140/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0255 - accuracy: 0.9909 - val_loss: 0.6952 - val_accuracy: 0.8793\n","\n","Epoch 00140: val_accuracy did not improve from 0.93842\n","Epoch 141/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0571 - accuracy: 0.9823 - val_loss: 1.3763 - val_accuracy: 0.8522\n","\n","Epoch 00141: val_accuracy did not improve from 0.93842\n","Epoch 142/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0466 - accuracy: 0.9848 - val_loss: 1.2426 - val_accuracy: 0.7956\n","\n","Epoch 00142: val_accuracy did not improve from 0.93842\n","Epoch 143/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0575 - accuracy: 0.9842 - val_loss: 1.0021 - val_accuracy: 0.8350\n","\n","Epoch 00143: val_accuracy did not improve from 0.93842\n","Epoch 144/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0660 - accuracy: 0.9836 - val_loss: 0.9619 - val_accuracy: 0.8300\n","\n","Epoch 00144: val_accuracy did not improve from 0.93842\n","Epoch 145/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0613 - accuracy: 0.9787 - val_loss: 0.9620 - val_accuracy: 0.8424\n","\n","Epoch 00145: val_accuracy did not improve from 0.93842\n","Epoch 146/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0560 - accuracy: 0.9842 - val_loss: 2.5465 - val_accuracy: 0.6946\n","\n","Epoch 00146: val_accuracy did not improve from 0.93842\n","Epoch 147/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0992 - accuracy: 0.9683 - val_loss: 1.9840 - val_accuracy: 0.7389\n","\n","Epoch 00147: val_accuracy did not improve from 0.93842\n","Epoch 148/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0878 - accuracy: 0.9750 - val_loss: 4.1908 - val_accuracy: 0.5517\n","\n","Epoch 00148: val_accuracy did not improve from 0.93842\n","Epoch 149/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0537 - accuracy: 0.9799 - val_loss: 1.3461 - val_accuracy: 0.7882\n","\n","Epoch 00149: val_accuracy did not improve from 0.93842\n","Epoch 150/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0462 - accuracy: 0.9872 - val_loss: 1.1617 - val_accuracy: 0.7906\n","\n","Epoch 00150: val_accuracy did not improve from 0.93842\n","Epoch 151/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0168 - accuracy: 0.9945 - val_loss: 0.4013 - val_accuracy: 0.9089\n","\n","Epoch 00151: val_accuracy did not improve from 0.93842\n","Epoch 152/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0067 - accuracy: 0.9976 - val_loss: 0.4837 - val_accuracy: 0.9089\n","\n","Epoch 00152: val_accuracy did not improve from 0.93842\n","Epoch 153/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0057 - accuracy: 0.9988 - val_loss: 0.4516 - val_accuracy: 0.9113\n","\n","Epoch 00153: val_accuracy did not improve from 0.93842\n","Epoch 154/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.3550 - val_accuracy: 0.9236\n","\n","Epoch 00154: val_accuracy did not improve from 0.93842\n","Epoch 155/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.3639 - val_accuracy: 0.9212\n","\n","Epoch 00155: val_accuracy did not improve from 0.93842\n","Epoch 156/500\n","52/52 [==============================] - 21s 399ms/step - loss: 0.0042 - accuracy: 0.9982 - val_loss: 0.5226 - val_accuracy: 0.9138\n","\n","Epoch 00156: val_accuracy did not improve from 0.93842\n","Epoch 157/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0055 - accuracy: 0.9970 - val_loss: 0.4960 - val_accuracy: 0.9089\n","\n","Epoch 00157: val_accuracy did not improve from 0.93842\n","Epoch 158/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0113 - accuracy: 0.9976 - val_loss: 0.5542 - val_accuracy: 0.9064\n","\n","Epoch 00158: val_accuracy did not improve from 0.93842\n","Epoch 159/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0120 - accuracy: 0.9963 - val_loss: 1.8831 - val_accuracy: 0.6970\n","\n","Epoch 00159: val_accuracy did not improve from 0.93842\n","Epoch 160/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0238 - accuracy: 0.9927 - val_loss: 0.7587 - val_accuracy: 0.8695\n","\n","Epoch 00160: val_accuracy did not improve from 0.93842\n","Epoch 161/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.4913 - val_accuracy: 0.9113\n","\n","Epoch 00161: val_accuracy did not improve from 0.93842\n","Epoch 162/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4459 - val_accuracy: 0.9236\n","\n","Epoch 00162: val_accuracy did not improve from 0.93842\n","Epoch 163/500\n","52/52 [==============================] - 21s 402ms/step - loss: 5.2848e-04 - accuracy: 1.0000 - val_loss: 0.4248 - val_accuracy: 0.9261\n","\n","Epoch 00163: val_accuracy did not improve from 0.93842\n","Epoch 164/500\n","52/52 [==============================] - 21s 401ms/step - loss: 4.5729e-04 - accuracy: 1.0000 - val_loss: 0.4737 - val_accuracy: 0.9163\n","\n","Epoch 00164: val_accuracy did not improve from 0.93842\n","Epoch 165/500\n","52/52 [==============================] - 21s 401ms/step - loss: 6.9404e-04 - accuracy: 1.0000 - val_loss: 0.4029 - val_accuracy: 0.9310\n","\n","Epoch 00165: val_accuracy did not improve from 0.93842\n","Epoch 166/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0014 - accuracy: 0.9994 - val_loss: 0.4111 - val_accuracy: 0.9138\n","\n","Epoch 00166: val_accuracy did not improve from 0.93842\n","Epoch 167/500\n","52/52 [==============================] - 21s 400ms/step - loss: 6.4476e-04 - accuracy: 1.0000 - val_loss: 0.3643 - val_accuracy: 0.9286\n","\n","Epoch 00167: val_accuracy did not improve from 0.93842\n","Epoch 168/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0064 - accuracy: 0.9982 - val_loss: 0.5251 - val_accuracy: 0.8818\n","\n","Epoch 00168: val_accuracy did not improve from 0.93842\n","Epoch 169/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0325 - accuracy: 0.9927 - val_loss: 0.6268 - val_accuracy: 0.9064\n","\n","Epoch 00169: val_accuracy did not improve from 0.93842\n","Epoch 170/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0265 - accuracy: 0.9915 - val_loss: 0.8460 - val_accuracy: 0.8325\n","\n","Epoch 00170: val_accuracy did not improve from 0.93842\n","Epoch 171/500\n","52/52 [==============================] - 21s 403ms/step - loss: 0.0131 - accuracy: 0.9963 - val_loss: 0.5853 - val_accuracy: 0.8990\n","\n","Epoch 00171: val_accuracy did not improve from 0.93842\n","Epoch 172/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0629 - accuracy: 0.9799 - val_loss: 1.1173 - val_accuracy: 0.8054\n","\n","Epoch 00172: val_accuracy did not improve from 0.93842\n","Epoch 173/500\n","52/52 [==============================] - 21s 399ms/step - loss: 0.0874 - accuracy: 0.9744 - val_loss: 1.4509 - val_accuracy: 0.8276\n","\n","Epoch 00173: val_accuracy did not improve from 0.93842\n","Epoch 174/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0425 - accuracy: 0.9866 - val_loss: 0.6193 - val_accuracy: 0.8719\n","\n","Epoch 00174: val_accuracy did not improve from 0.93842\n","Epoch 175/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0136 - accuracy: 0.9957 - val_loss: 0.6754 - val_accuracy: 0.8842\n","\n","Epoch 00175: val_accuracy did not improve from 0.93842\n","Epoch 176/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.3964 - val_accuracy: 0.9261\n","\n","Epoch 00176: val_accuracy did not improve from 0.93842\n","Epoch 177/500\n","52/52 [==============================] - 21s 399ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.3589 - val_accuracy: 0.9409\n","\n","Epoch 00177: val_accuracy improved from 0.93842 to 0.94089, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_Xception.h5\n","Epoch 178/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0092 - accuracy: 0.9982 - val_loss: 0.3580 - val_accuracy: 0.9310\n","\n","Epoch 00178: val_accuracy did not improve from 0.94089\n","Epoch 179/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0204 - accuracy: 0.9933 - val_loss: 0.6399 - val_accuracy: 0.8916\n","\n","Epoch 00179: val_accuracy did not improve from 0.94089\n","Epoch 180/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0183 - accuracy: 0.9933 - val_loss: 1.2276 - val_accuracy: 0.8473\n","\n","Epoch 00180: val_accuracy did not improve from 0.94089\n","Epoch 181/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0417 - accuracy: 0.9909 - val_loss: 0.5492 - val_accuracy: 0.9015\n","\n","Epoch 00181: val_accuracy did not improve from 0.94089\n","Epoch 182/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0353 - accuracy: 0.9915 - val_loss: 0.8705 - val_accuracy: 0.8547\n","\n","Epoch 00182: val_accuracy did not improve from 0.94089\n","Epoch 183/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0107 - accuracy: 0.9963 - val_loss: 1.5098 - val_accuracy: 0.7882\n","\n","Epoch 00183: val_accuracy did not improve from 0.94089\n","Epoch 184/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0317 - accuracy: 0.9909 - val_loss: 0.6977 - val_accuracy: 0.8571\n","\n","Epoch 00184: val_accuracy did not improve from 0.94089\n","Epoch 185/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0266 - accuracy: 0.9909 - val_loss: 0.6136 - val_accuracy: 0.8867\n","\n","Epoch 00185: val_accuracy did not improve from 0.94089\n","Epoch 186/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0454 - accuracy: 0.9884 - val_loss: 0.8174 - val_accuracy: 0.8621\n","\n","Epoch 00186: val_accuracy did not improve from 0.94089\n","Epoch 187/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0374 - accuracy: 0.9866 - val_loss: 1.7875 - val_accuracy: 0.7635\n","\n","Epoch 00187: val_accuracy did not improve from 0.94089\n","Epoch 188/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0532 - accuracy: 0.9860 - val_loss: 0.7601 - val_accuracy: 0.8596\n","\n","Epoch 00188: val_accuracy did not improve from 0.94089\n","Epoch 189/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0185 - accuracy: 0.9933 - val_loss: 0.5368 - val_accuracy: 0.8842\n","\n","Epoch 00189: val_accuracy did not improve from 0.94089\n","Epoch 190/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0053 - accuracy: 0.9988 - val_loss: 0.4931 - val_accuracy: 0.9064\n","\n","Epoch 00190: val_accuracy did not improve from 0.94089\n","Epoch 191/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0023 - accuracy: 0.9988 - val_loss: 0.4615 - val_accuracy: 0.9187\n","\n","Epoch 00191: val_accuracy did not improve from 0.94089\n","Epoch 192/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3671 - val_accuracy: 0.9212\n","\n","Epoch 00192: val_accuracy did not improve from 0.94089\n","Epoch 193/500\n","52/52 [==============================] - 21s 401ms/step - loss: 8.7124e-04 - accuracy: 1.0000 - val_loss: 0.3205 - val_accuracy: 0.9310\n","\n","Epoch 00193: val_accuracy did not improve from 0.94089\n","Epoch 194/500\n","52/52 [==============================] - 21s 399ms/step - loss: 0.0097 - accuracy: 0.9976 - val_loss: 0.3644 - val_accuracy: 0.9261\n","\n","Epoch 00194: val_accuracy did not improve from 0.94089\n","Epoch 195/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.5072 - val_accuracy: 0.8966\n","\n","Epoch 00195: val_accuracy did not improve from 0.94089\n","Epoch 196/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0036 - accuracy: 0.9994 - val_loss: 0.5025 - val_accuracy: 0.9163\n","\n","Epoch 00196: val_accuracy did not improve from 0.94089\n","Epoch 197/500\n","52/52 [==============================] - 21s 399ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.3764 - val_accuracy: 0.9236\n","\n","Epoch 00197: val_accuracy did not improve from 0.94089\n","Epoch 198/500\n","52/52 [==============================] - 21s 401ms/step - loss: 5.6054e-04 - accuracy: 1.0000 - val_loss: 0.3818 - val_accuracy: 0.9335\n","\n","Epoch 00198: val_accuracy did not improve from 0.94089\n","Epoch 199/500\n","52/52 [==============================] - 21s 400ms/step - loss: 2.1738e-04 - accuracy: 1.0000 - val_loss: 0.3657 - val_accuracy: 0.9335\n","\n","Epoch 00199: val_accuracy did not improve from 0.94089\n","Epoch 200/500\n","52/52 [==============================] - 21s 400ms/step - loss: 2.3339e-04 - accuracy: 1.0000 - val_loss: 0.3642 - val_accuracy: 0.9212\n","\n","Epoch 00200: val_accuracy did not improve from 0.94089\n","Epoch 201/500\n","52/52 [==============================] - 21s 400ms/step - loss: 3.6408e-04 - accuracy: 1.0000 - val_loss: 0.3619 - val_accuracy: 0.9212\n","\n","Epoch 00201: val_accuracy did not improve from 0.94089\n","Epoch 202/500\n","52/52 [==============================] - 21s 398ms/step - loss: 4.3148e-04 - accuracy: 1.0000 - val_loss: 0.3854 - val_accuracy: 0.9286\n","\n","Epoch 00202: val_accuracy did not improve from 0.94089\n","Epoch 203/500\n","52/52 [==============================] - 21s 401ms/step - loss: 2.6444e-04 - accuracy: 1.0000 - val_loss: 0.3959 - val_accuracy: 0.9236\n","\n","Epoch 00203: val_accuracy did not improve from 0.94089\n","Epoch 204/500\n","52/52 [==============================] - 21s 400ms/step - loss: 3.6762e-04 - accuracy: 1.0000 - val_loss: 0.3916 - val_accuracy: 0.9261\n","\n","Epoch 00204: val_accuracy did not improve from 0.94089\n","Epoch 205/500\n","52/52 [==============================] - 21s 401ms/step - loss: 5.4044e-04 - accuracy: 1.0000 - val_loss: 0.4144 - val_accuracy: 0.9163\n","\n","Epoch 00205: val_accuracy did not improve from 0.94089\n","Epoch 206/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0013 - accuracy: 0.9994 - val_loss: 0.5319 - val_accuracy: 0.9310\n","\n","Epoch 00206: val_accuracy did not improve from 0.94089\n","Epoch 207/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0047 - accuracy: 0.9982 - val_loss: 0.6712 - val_accuracy: 0.8719\n","\n","Epoch 00207: val_accuracy did not improve from 0.94089\n","Epoch 208/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0194 - accuracy: 0.9957 - val_loss: 0.9970 - val_accuracy: 0.8596\n","\n","Epoch 00208: val_accuracy did not improve from 0.94089\n","Epoch 209/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.2229 - accuracy: 0.9458 - val_loss: 36.6051 - val_accuracy: 0.0887\n","\n","Epoch 00209: val_accuracy did not improve from 0.94089\n","Epoch 210/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.2599 - accuracy: 0.9300 - val_loss: 6.6533 - val_accuracy: 0.4212\n","\n","Epoch 00210: val_accuracy did not improve from 0.94089\n","Epoch 211/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0283 - accuracy: 0.9903 - val_loss: 1.0270 - val_accuracy: 0.8251\n","\n","Epoch 00211: val_accuracy did not improve from 0.94089\n","Epoch 212/500\n","52/52 [==============================] - 21s 399ms/step - loss: 0.0200 - accuracy: 0.9945 - val_loss: 0.6615 - val_accuracy: 0.8916\n","\n","Epoch 00212: val_accuracy did not improve from 0.94089\n","Epoch 213/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0186 - accuracy: 0.9976 - val_loss: 0.5257 - val_accuracy: 0.8990\n","\n","Epoch 00213: val_accuracy did not improve from 0.94089\n","Epoch 214/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.3617 - val_accuracy: 0.9187\n","\n","Epoch 00214: val_accuracy did not improve from 0.94089\n","Epoch 215/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.3687 - val_accuracy: 0.9187\n","\n","Epoch 00215: val_accuracy did not improve from 0.94089\n","Epoch 216/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0039 - accuracy: 0.9988 - val_loss: 0.4617 - val_accuracy: 0.9212\n","\n","Epoch 00216: val_accuracy did not improve from 0.94089\n","Epoch 217/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0044 - accuracy: 0.9988 - val_loss: 0.4223 - val_accuracy: 0.9187\n","\n","Epoch 00217: val_accuracy did not improve from 0.94089\n","Epoch 218/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.4409 - val_accuracy: 0.9163\n","\n","Epoch 00218: val_accuracy did not improve from 0.94089\n","Epoch 219/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.3507 - val_accuracy: 0.9261\n","\n","Epoch 00219: val_accuracy did not improve from 0.94089\n","Epoch 220/500\n","52/52 [==============================] - 21s 399ms/step - loss: 0.0015 - accuracy: 0.9994 - val_loss: 0.3546 - val_accuracy: 0.9286\n","\n","Epoch 00220: val_accuracy did not improve from 0.94089\n","Epoch 221/500\n","52/52 [==============================] - 21s 402ms/step - loss: 9.3691e-04 - accuracy: 1.0000 - val_loss: 0.3787 - val_accuracy: 0.9261\n","\n","Epoch 00221: val_accuracy did not improve from 0.94089\n","Epoch 222/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.3980 - val_accuracy: 0.9113\n","\n","Epoch 00222: val_accuracy did not improve from 0.94089\n","Epoch 223/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0017 - accuracy: 0.9994 - val_loss: 0.4066 - val_accuracy: 0.9286\n","\n","Epoch 00223: val_accuracy did not improve from 0.94089\n","Epoch 224/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.4494 - val_accuracy: 0.9113\n","\n","Epoch 00224: val_accuracy did not improve from 0.94089\n","Epoch 225/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4343 - val_accuracy: 0.9015\n","\n","Epoch 00225: val_accuracy did not improve from 0.94089\n","Epoch 226/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6464 - val_accuracy: 0.8867\n","\n","Epoch 00226: val_accuracy did not improve from 0.94089\n","Epoch 227/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.4533 - val_accuracy: 0.9089\n","\n","Epoch 00227: val_accuracy did not improve from 0.94089\n","Epoch 228/500\n","52/52 [==============================] - 21s 401ms/step - loss: 7.0237e-04 - accuracy: 1.0000 - val_loss: 0.4754 - val_accuracy: 0.9089\n","\n","Epoch 00228: val_accuracy did not improve from 0.94089\n","Epoch 229/500\n","52/52 [==============================] - 21s 401ms/step - loss: 9.5326e-04 - accuracy: 1.0000 - val_loss: 0.5087 - val_accuracy: 0.9187\n","\n","Epoch 00229: val_accuracy did not improve from 0.94089\n","Epoch 230/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0221 - accuracy: 0.9951 - val_loss: 0.8843 - val_accuracy: 0.8645\n","\n","Epoch 00230: val_accuracy did not improve from 0.94089\n","Epoch 231/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0244 - accuracy: 0.9909 - val_loss: 0.5705 - val_accuracy: 0.8744\n","\n","Epoch 00231: val_accuracy did not improve from 0.94089\n","Epoch 232/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0605 - accuracy: 0.9793 - val_loss: 1.2690 - val_accuracy: 0.8054\n","\n","Epoch 00232: val_accuracy did not improve from 0.94089\n","Epoch 233/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0223 - accuracy: 0.9909 - val_loss: 0.5588 - val_accuracy: 0.8966\n","\n","Epoch 00233: val_accuracy did not improve from 0.94089\n","Epoch 234/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0065 - accuracy: 0.9988 - val_loss: 0.5984 - val_accuracy: 0.8842\n","\n","Epoch 00234: val_accuracy did not improve from 0.94089\n","Epoch 235/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0513 - accuracy: 0.9848 - val_loss: 0.6119 - val_accuracy: 0.8916\n","\n","Epoch 00235: val_accuracy did not improve from 0.94089\n","Epoch 236/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0357 - accuracy: 0.9909 - val_loss: 0.7054 - val_accuracy: 0.8547\n","\n","Epoch 00236: val_accuracy did not improve from 0.94089\n","Epoch 237/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0247 - accuracy: 0.9915 - val_loss: 0.7067 - val_accuracy: 0.8990\n","\n","Epoch 00237: val_accuracy did not improve from 0.94089\n","Epoch 238/500\n","52/52 [==============================] - 21s 404ms/step - loss: 0.0339 - accuracy: 0.9915 - val_loss: 0.4739 - val_accuracy: 0.9113\n","\n","Epoch 00238: val_accuracy did not improve from 0.94089\n","Epoch 239/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0077 - accuracy: 0.9970 - val_loss: 0.4155 - val_accuracy: 0.9163\n","\n","Epoch 00239: val_accuracy did not improve from 0.94089\n","Epoch 240/500\n","52/52 [==============================] - 21s 399ms/step - loss: 0.0081 - accuracy: 0.9976 - val_loss: 0.5247 - val_accuracy: 0.8695\n","\n","Epoch 00240: val_accuracy did not improve from 0.94089\n","Epoch 241/500\n","52/52 [==============================] - 21s 406ms/step - loss: 0.0079 - accuracy: 0.9976 - val_loss: 0.4413 - val_accuracy: 0.8892\n","\n","Epoch 00241: val_accuracy did not improve from 0.94089\n","Epoch 242/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0038 - accuracy: 0.9982 - val_loss: 0.3526 - val_accuracy: 0.9163\n","\n","Epoch 00242: val_accuracy did not improve from 0.94089\n","Epoch 243/500\n","52/52 [==============================] - 21s 399ms/step - loss: 0.0050 - accuracy: 0.9988 - val_loss: 0.4109 - val_accuracy: 0.9261\n","\n","Epoch 00243: val_accuracy did not improve from 0.94089\n","Epoch 244/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3464 - val_accuracy: 0.9187\n","\n","Epoch 00244: val_accuracy did not improve from 0.94089\n","Epoch 245/500\n","52/52 [==============================] - 21s 404ms/step - loss: 6.8234e-04 - accuracy: 1.0000 - val_loss: 0.2886 - val_accuracy: 0.9483\n","\n","Epoch 00245: val_accuracy improved from 0.94089 to 0.94828, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_Xception.h5\n","Epoch 246/500\n","52/52 [==============================] - 21s 401ms/step - loss: 6.1560e-04 - accuracy: 1.0000 - val_loss: 0.2773 - val_accuracy: 0.9433\n","\n","Epoch 00246: val_accuracy did not improve from 0.94828\n","Epoch 247/500\n","52/52 [==============================] - 21s 401ms/step - loss: 4.5104e-04 - accuracy: 1.0000 - val_loss: 0.3407 - val_accuracy: 0.9286\n","\n","Epoch 00247: val_accuracy did not improve from 0.94828\n","Epoch 248/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0021 - accuracy: 0.9988 - val_loss: 0.5302 - val_accuracy: 0.8990\n","\n","Epoch 00248: val_accuracy did not improve from 0.94828\n","Epoch 249/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0130 - accuracy: 0.9963 - val_loss: 0.6630 - val_accuracy: 0.8719\n","\n","Epoch 00249: val_accuracy did not improve from 0.94828\n","Epoch 250/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0298 - accuracy: 0.9921 - val_loss: 1.1210 - val_accuracy: 0.8695\n","\n","Epoch 00250: val_accuracy did not improve from 0.94828\n","Epoch 251/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0161 - accuracy: 0.9939 - val_loss: 0.7389 - val_accuracy: 0.8941\n","\n","Epoch 00251: val_accuracy did not improve from 0.94828\n","Epoch 252/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0181 - accuracy: 0.9957 - val_loss: 0.5498 - val_accuracy: 0.8621\n","\n","Epoch 00252: val_accuracy did not improve from 0.94828\n","Epoch 253/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0239 - accuracy: 0.9909 - val_loss: 0.9091 - val_accuracy: 0.8448\n","\n","Epoch 00253: val_accuracy did not improve from 0.94828\n","Epoch 254/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0606 - accuracy: 0.9811 - val_loss: 1.8028 - val_accuracy: 0.8054\n","\n","Epoch 00254: val_accuracy did not improve from 0.94828\n","Epoch 255/500\n","52/52 [==============================] - 21s 403ms/step - loss: 0.0587 - accuracy: 0.9817 - val_loss: 0.6150 - val_accuracy: 0.8818\n","\n","Epoch 00255: val_accuracy did not improve from 0.94828\n","Epoch 256/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0760 - accuracy: 0.9762 - val_loss: 1.3965 - val_accuracy: 0.8251\n","\n","Epoch 00256: val_accuracy did not improve from 0.94828\n","Epoch 257/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0435 - accuracy: 0.9854 - val_loss: 0.7526 - val_accuracy: 0.8695\n","\n","Epoch 00257: val_accuracy did not improve from 0.94828\n","Epoch 258/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0272 - accuracy: 0.9915 - val_loss: 1.1240 - val_accuracy: 0.8177\n","\n","Epoch 00258: val_accuracy did not improve from 0.94828\n","Epoch 259/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0142 - accuracy: 0.9939 - val_loss: 0.5403 - val_accuracy: 0.9064\n","\n","Epoch 00259: val_accuracy did not improve from 0.94828\n","Epoch 260/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0264 - accuracy: 0.9939 - val_loss: 1.1268 - val_accuracy: 0.7931\n","\n","Epoch 00260: val_accuracy did not improve from 0.94828\n","Epoch 261/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0430 - accuracy: 0.9848 - val_loss: 0.6101 - val_accuracy: 0.8892\n","\n","Epoch 00261: val_accuracy did not improve from 0.94828\n","Epoch 262/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0136 - accuracy: 0.9957 - val_loss: 0.5510 - val_accuracy: 0.9163\n","\n","Epoch 00262: val_accuracy did not improve from 0.94828\n","Epoch 263/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0077 - accuracy: 0.9982 - val_loss: 0.4371 - val_accuracy: 0.8916\n","\n","Epoch 00263: val_accuracy did not improve from 0.94828\n","Epoch 264/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0035 - accuracy: 0.9988 - val_loss: 0.3612 - val_accuracy: 0.9138\n","\n","Epoch 00264: val_accuracy did not improve from 0.94828\n","Epoch 265/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.3769 - val_accuracy: 0.9163\n","\n","Epoch 00265: val_accuracy did not improve from 0.94828\n","Epoch 266/500\n","52/52 [==============================] - 21s 400ms/step - loss: 8.6266e-04 - accuracy: 1.0000 - val_loss: 0.3093 - val_accuracy: 0.9236\n","\n","Epoch 00266: val_accuracy did not improve from 0.94828\n","Epoch 267/500\n","52/52 [==============================] - 21s 401ms/step - loss: 6.3649e-04 - accuracy: 1.0000 - val_loss: 0.3096 - val_accuracy: 0.9335\n","\n","Epoch 00267: val_accuracy did not improve from 0.94828\n","Epoch 268/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0104 - accuracy: 0.9957 - val_loss: 0.5391 - val_accuracy: 0.8966\n","\n","Epoch 00268: val_accuracy did not improve from 0.94828\n","Epoch 269/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0070 - accuracy: 0.9976 - val_loss: 0.4705 - val_accuracy: 0.9064\n","\n","Epoch 00269: val_accuracy did not improve from 0.94828\n","Epoch 270/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.6057 - val_accuracy: 0.8916\n","\n","Epoch 00270: val_accuracy did not improve from 0.94828\n","Epoch 271/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0025 - accuracy: 0.9988 - val_loss: 0.5214 - val_accuracy: 0.9113\n","\n","Epoch 00271: val_accuracy did not improve from 0.94828\n","Epoch 272/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4715 - val_accuracy: 0.9261\n","\n","Epoch 00272: val_accuracy did not improve from 0.94828\n","Epoch 273/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0013 - accuracy: 0.9994 - val_loss: 0.4031 - val_accuracy: 0.9360\n","\n","Epoch 00273: val_accuracy did not improve from 0.94828\n","Epoch 274/500\n","52/52 [==============================] - 21s 401ms/step - loss: 5.0868e-04 - accuracy: 1.0000 - val_loss: 0.4316 - val_accuracy: 0.9236\n","\n","Epoch 00274: val_accuracy did not improve from 0.94828\n","Epoch 275/500\n","52/52 [==============================] - 21s 400ms/step - loss: 4.6987e-04 - accuracy: 1.0000 - val_loss: 0.3741 - val_accuracy: 0.9335\n","\n","Epoch 00275: val_accuracy did not improve from 0.94828\n","Epoch 276/500\n","52/52 [==============================] - 21s 401ms/step - loss: 8.8937e-04 - accuracy: 0.9994 - val_loss: 0.4340 - val_accuracy: 0.9187\n","\n","Epoch 00276: val_accuracy did not improve from 0.94828\n","Epoch 277/500\n","52/52 [==============================] - 21s 399ms/step - loss: 5.7407e-04 - accuracy: 1.0000 - val_loss: 0.4698 - val_accuracy: 0.9187\n","\n","Epoch 00277: val_accuracy did not improve from 0.94828\n","Epoch 278/500\n","52/52 [==============================] - 21s 400ms/step - loss: 1.7252e-04 - accuracy: 1.0000 - val_loss: 0.3269 - val_accuracy: 0.9360\n","\n","Epoch 00278: val_accuracy did not improve from 0.94828\n","Epoch 279/500\n","52/52 [==============================] - 21s 400ms/step - loss: 5.4066e-04 - accuracy: 1.0000 - val_loss: 0.4793 - val_accuracy: 0.9212\n","\n","Epoch 00279: val_accuracy did not improve from 0.94828\n","Epoch 280/500\n","52/52 [==============================] - 21s 400ms/step - loss: 5.8514e-04 - accuracy: 1.0000 - val_loss: 0.4341 - val_accuracy: 0.9286\n","\n","Epoch 00280: val_accuracy did not improve from 0.94828\n","Epoch 281/500\n","52/52 [==============================] - 21s 400ms/step - loss: 1.7810e-04 - accuracy: 1.0000 - val_loss: 0.3737 - val_accuracy: 0.9261\n","\n","Epoch 00281: val_accuracy did not improve from 0.94828\n","Epoch 282/500\n","52/52 [==============================] - 21s 401ms/step - loss: 4.1974e-04 - accuracy: 1.0000 - val_loss: 0.3973 - val_accuracy: 0.9335\n","\n","Epoch 00282: val_accuracy did not improve from 0.94828\n","Epoch 283/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0020 - accuracy: 0.9988 - val_loss: 0.3987 - val_accuracy: 0.9286\n","\n","Epoch 00283: val_accuracy did not improve from 0.94828\n","Epoch 284/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.5064 - val_accuracy: 0.9212\n","\n","Epoch 00284: val_accuracy did not improve from 0.94828\n","Epoch 285/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0371 - accuracy: 0.9896 - val_loss: 1.5000 - val_accuracy: 0.8030\n","\n","Epoch 00285: val_accuracy did not improve from 0.94828\n","Epoch 286/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0549 - accuracy: 0.9817 - val_loss: 1.2099 - val_accuracy: 0.8522\n","\n","Epoch 00286: val_accuracy did not improve from 0.94828\n","Epoch 287/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0368 - accuracy: 0.9896 - val_loss: 0.6914 - val_accuracy: 0.8941\n","\n","Epoch 00287: val_accuracy did not improve from 0.94828\n","Epoch 288/500\n","52/52 [==============================] - 21s 399ms/step - loss: 0.0072 - accuracy: 0.9970 - val_loss: 0.6430 - val_accuracy: 0.9015\n","\n","Epoch 00288: val_accuracy did not improve from 0.94828\n","Epoch 289/500\n","52/52 [==============================] - 21s 399ms/step - loss: 0.0059 - accuracy: 0.9982 - val_loss: 0.4913 - val_accuracy: 0.9039\n","\n","Epoch 00289: val_accuracy did not improve from 0.94828\n","Epoch 290/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0175 - accuracy: 0.9939 - val_loss: 0.5393 - val_accuracy: 0.8867\n","\n","Epoch 00290: val_accuracy did not improve from 0.94828\n","Epoch 291/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0162 - accuracy: 0.9957 - val_loss: 0.5516 - val_accuracy: 0.8818\n","\n","Epoch 00291: val_accuracy did not improve from 0.94828\n","Epoch 292/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0060 - accuracy: 0.9982 - val_loss: 0.5159 - val_accuracy: 0.8966\n","\n","Epoch 00292: val_accuracy did not improve from 0.94828\n","Epoch 293/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0173 - accuracy: 0.9945 - val_loss: 0.6249 - val_accuracy: 0.9015\n","\n","Epoch 00293: val_accuracy did not improve from 0.94828\n","Epoch 294/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0081 - accuracy: 0.9963 - val_loss: 0.6024 - val_accuracy: 0.8941\n","\n","Epoch 00294: val_accuracy did not improve from 0.94828\n","Epoch 295/500\n","52/52 [==============================] - 21s 403ms/step - loss: 0.0536 - accuracy: 0.9842 - val_loss: 0.9044 - val_accuracy: 0.8079\n","\n","Epoch 00295: val_accuracy did not improve from 0.94828\n","Epoch 296/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0460 - accuracy: 0.9878 - val_loss: 2.9737 - val_accuracy: 0.6232\n","\n","Epoch 00296: val_accuracy did not improve from 0.94828\n","Epoch 297/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0384 - accuracy: 0.9909 - val_loss: 1.1038 - val_accuracy: 0.8448\n","\n","Epoch 00297: val_accuracy did not improve from 0.94828\n","Epoch 298/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0356 - accuracy: 0.9896 - val_loss: 0.7812 - val_accuracy: 0.8793\n","\n","Epoch 00298: val_accuracy did not improve from 0.94828\n","Epoch 299/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0542 - accuracy: 0.9896 - val_loss: 0.6392 - val_accuracy: 0.8695\n","\n","Epoch 00299: val_accuracy did not improve from 0.94828\n","Epoch 300/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0137 - accuracy: 0.9939 - val_loss: 0.5644 - val_accuracy: 0.8842\n","\n","Epoch 00300: val_accuracy did not improve from 0.94828\n","Epoch 301/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0086 - accuracy: 0.9970 - val_loss: 0.6892 - val_accuracy: 0.8719\n","\n","Epoch 00301: val_accuracy did not improve from 0.94828\n","Epoch 302/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0166 - accuracy: 0.9957 - val_loss: 0.6177 - val_accuracy: 0.8990\n","\n","Epoch 00302: val_accuracy did not improve from 0.94828\n","Epoch 303/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0172 - accuracy: 0.9963 - val_loss: 0.5154 - val_accuracy: 0.8990\n","\n","Epoch 00303: val_accuracy did not improve from 0.94828\n","Epoch 304/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0039 - accuracy: 0.9988 - val_loss: 0.6062 - val_accuracy: 0.9015\n","\n","Epoch 00304: val_accuracy did not improve from 0.94828\n","Epoch 305/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0095 - accuracy: 0.9970 - val_loss: 0.5196 - val_accuracy: 0.8990\n","\n","Epoch 00305: val_accuracy did not improve from 0.94828\n","Epoch 306/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0135 - accuracy: 0.9970 - val_loss: 0.7572 - val_accuracy: 0.8695\n","\n","Epoch 00306: val_accuracy did not improve from 0.94828\n","Epoch 307/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0097 - accuracy: 0.9988 - val_loss: 0.4684 - val_accuracy: 0.9163\n","\n","Epoch 00307: val_accuracy did not improve from 0.94828\n","Epoch 308/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0018 - accuracy: 0.9994 - val_loss: 0.4301 - val_accuracy: 0.9163\n","\n","Epoch 00308: val_accuracy did not improve from 0.94828\n","Epoch 309/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0057 - accuracy: 0.9994 - val_loss: 0.5712 - val_accuracy: 0.9138\n","\n","Epoch 00309: val_accuracy did not improve from 0.94828\n","Epoch 310/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0020 - accuracy: 0.9988 - val_loss: 0.4572 - val_accuracy: 0.9138\n","\n","Epoch 00310: val_accuracy did not improve from 0.94828\n","Epoch 311/500\n","52/52 [==============================] - 21s 401ms/step - loss: 3.3260e-04 - accuracy: 1.0000 - val_loss: 0.4638 - val_accuracy: 0.9138\n","\n","Epoch 00311: val_accuracy did not improve from 0.94828\n","Epoch 312/500\n","52/52 [==============================] - 21s 400ms/step - loss: 2.4893e-04 - accuracy: 1.0000 - val_loss: 0.4506 - val_accuracy: 0.9113\n","\n","Epoch 00312: val_accuracy did not improve from 0.94828\n","Epoch 313/500\n","52/52 [==============================] - 21s 401ms/step - loss: 5.5566e-04 - accuracy: 1.0000 - val_loss: 0.4832 - val_accuracy: 0.9089\n","\n","Epoch 00313: val_accuracy did not improve from 0.94828\n","Epoch 314/500\n","52/52 [==============================] - 21s 400ms/step - loss: 5.7676e-04 - accuracy: 1.0000 - val_loss: 0.4584 - val_accuracy: 0.9113\n","\n","Epoch 00314: val_accuracy did not improve from 0.94828\n","Epoch 315/500\n","52/52 [==============================] - 21s 401ms/step - loss: 3.0180e-04 - accuracy: 1.0000 - val_loss: 0.4105 - val_accuracy: 0.9212\n","\n","Epoch 00315: val_accuracy did not improve from 0.94828\n","Epoch 316/500\n","52/52 [==============================] - 21s 400ms/step - loss: 2.7644e-04 - accuracy: 1.0000 - val_loss: 0.3978 - val_accuracy: 0.9384\n","\n","Epoch 00316: val_accuracy did not improve from 0.94828\n","Epoch 317/500\n","52/52 [==============================] - 21s 400ms/step - loss: 5.3824e-04 - accuracy: 1.0000 - val_loss: 0.4528 - val_accuracy: 0.9113\n","\n","Epoch 00317: val_accuracy did not improve from 0.94828\n","Epoch 318/500\n","52/52 [==============================] - 21s 401ms/step - loss: 9.1575e-04 - accuracy: 1.0000 - val_loss: 0.3755 - val_accuracy: 0.9212\n","\n","Epoch 00318: val_accuracy did not improve from 0.94828\n","Epoch 319/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.4162 - val_accuracy: 0.9335\n","\n","Epoch 00319: val_accuracy did not improve from 0.94828\n","Epoch 320/500\n","52/52 [==============================] - 21s 405ms/step - loss: 1.9317e-04 - accuracy: 1.0000 - val_loss: 0.4427 - val_accuracy: 0.9212\n","\n","Epoch 00320: val_accuracy did not improve from 0.94828\n","Epoch 321/500\n","52/52 [==============================] - 21s 400ms/step - loss: 1.6057e-04 - accuracy: 1.0000 - val_loss: 0.3923 - val_accuracy: 0.9212\n","\n","Epoch 00321: val_accuracy did not improve from 0.94828\n","Epoch 322/500\n","52/52 [==============================] - 21s 400ms/step - loss: 2.1036e-04 - accuracy: 1.0000 - val_loss: 0.4035 - val_accuracy: 0.9163\n","\n","Epoch 00322: val_accuracy did not improve from 0.94828\n","Epoch 323/500\n","52/52 [==============================] - 21s 401ms/step - loss: 2.9278e-04 - accuracy: 1.0000 - val_loss: 0.3823 - val_accuracy: 0.9138\n","\n","Epoch 00323: val_accuracy did not improve from 0.94828\n","Epoch 324/500\n","52/52 [==============================] - 21s 400ms/step - loss: 2.1434e-04 - accuracy: 1.0000 - val_loss: 0.3987 - val_accuracy: 0.9261\n","\n","Epoch 00324: val_accuracy did not improve from 0.94828\n","Epoch 325/500\n","52/52 [==============================] - 21s 401ms/step - loss: 8.7799e-05 - accuracy: 1.0000 - val_loss: 0.4040 - val_accuracy: 0.9286\n","\n","Epoch 00325: val_accuracy did not improve from 0.94828\n","Epoch 326/500\n","52/52 [==============================] - 21s 402ms/step - loss: 2.2978e-04 - accuracy: 1.0000 - val_loss: 0.3990 - val_accuracy: 0.9335\n","\n","Epoch 00326: val_accuracy did not improve from 0.94828\n","Epoch 327/500\n","52/52 [==============================] - 21s 400ms/step - loss: 5.7171e-05 - accuracy: 1.0000 - val_loss: 0.3964 - val_accuracy: 0.9360\n","\n","Epoch 00327: val_accuracy did not improve from 0.94828\n","Epoch 328/500\n","52/52 [==============================] - 21s 402ms/step - loss: 3.7633e-04 - accuracy: 1.0000 - val_loss: 0.4073 - val_accuracy: 0.9138\n","\n","Epoch 00328: val_accuracy did not improve from 0.94828\n","Epoch 329/500\n","52/52 [==============================] - 21s 401ms/step - loss: 1.2077e-04 - accuracy: 1.0000 - val_loss: 0.4065 - val_accuracy: 0.9187\n","\n","Epoch 00329: val_accuracy did not improve from 0.94828\n","Epoch 330/500\n","52/52 [==============================] - 21s 400ms/step - loss: 1.5324e-04 - accuracy: 1.0000 - val_loss: 0.4218 - val_accuracy: 0.9236\n","\n","Epoch 00330: val_accuracy did not improve from 0.94828\n","Epoch 331/500\n","52/52 [==============================] - 21s 400ms/step - loss: 1.9696e-04 - accuracy: 1.0000 - val_loss: 0.4893 - val_accuracy: 0.9187\n","\n","Epoch 00331: val_accuracy did not improve from 0.94828\n","Epoch 332/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0051 - accuracy: 0.9976 - val_loss: 0.8380 - val_accuracy: 0.8448\n","\n","Epoch 00332: val_accuracy did not improve from 0.94828\n","Epoch 333/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.1208 - accuracy: 0.9671 - val_loss: 3.3296 - val_accuracy: 0.6872\n","\n","Epoch 00333: val_accuracy did not improve from 0.94828\n","Epoch 334/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0635 - accuracy: 0.9805 - val_loss: 1.3664 - val_accuracy: 0.8350\n","\n","Epoch 00334: val_accuracy did not improve from 0.94828\n","Epoch 335/500\n","52/52 [==============================] - 21s 399ms/step - loss: 0.0259 - accuracy: 0.9927 - val_loss: 0.7007 - val_accuracy: 0.8768\n","\n","Epoch 00335: val_accuracy did not improve from 0.94828\n","Epoch 336/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0224 - accuracy: 0.9933 - val_loss: 0.8236 - val_accuracy: 0.8670\n","\n","Epoch 00336: val_accuracy did not improve from 0.94828\n","Epoch 337/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0490 - accuracy: 0.9860 - val_loss: 0.9393 - val_accuracy: 0.8325\n","\n","Epoch 00337: val_accuracy did not improve from 0.94828\n","Epoch 338/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0138 - accuracy: 0.9951 - val_loss: 0.5262 - val_accuracy: 0.9089\n","\n","Epoch 00338: val_accuracy did not improve from 0.94828\n","Epoch 339/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.4303 - val_accuracy: 0.9212\n","\n","Epoch 00339: val_accuracy did not improve from 0.94828\n","Epoch 340/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.3158 - val_accuracy: 0.9360\n","\n","Epoch 00340: val_accuracy did not improve from 0.94828\n","Epoch 341/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0041 - accuracy: 0.9988 - val_loss: 0.5472 - val_accuracy: 0.9113\n","\n","Epoch 00341: val_accuracy did not improve from 0.94828\n","Epoch 342/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0226 - accuracy: 0.9927 - val_loss: 0.8508 - val_accuracy: 0.8473\n","\n","Epoch 00342: val_accuracy did not improve from 0.94828\n","Epoch 343/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0082 - accuracy: 0.9970 - val_loss: 0.5937 - val_accuracy: 0.8892\n","\n","Epoch 00343: val_accuracy did not improve from 0.94828\n","Epoch 344/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.5595 - val_accuracy: 0.8867\n","\n","Epoch 00344: val_accuracy did not improve from 0.94828\n","Epoch 345/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0041 - accuracy: 0.9982 - val_loss: 0.4451 - val_accuracy: 0.9138\n","\n","Epoch 00345: val_accuracy did not improve from 0.94828\n","Epoch 346/500\n","52/52 [==============================] - 21s 399ms/step - loss: 0.0218 - accuracy: 0.9957 - val_loss: 0.6332 - val_accuracy: 0.8744\n","\n","Epoch 00346: val_accuracy did not improve from 0.94828\n","Epoch 347/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0242 - accuracy: 0.9921 - val_loss: 0.6340 - val_accuracy: 0.8793\n","\n","Epoch 00347: val_accuracy did not improve from 0.94828\n","Epoch 348/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0063 - accuracy: 0.9988 - val_loss: 0.4516 - val_accuracy: 0.9212\n","\n","Epoch 00348: val_accuracy did not improve from 0.94828\n","Epoch 349/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.3751 - val_accuracy: 0.9335\n","\n","Epoch 00349: val_accuracy did not improve from 0.94828\n","Epoch 350/500\n","52/52 [==============================] - 21s 405ms/step - loss: 0.0012 - accuracy: 0.9994 - val_loss: 0.3800 - val_accuracy: 0.9458\n","\n","Epoch 00350: val_accuracy did not improve from 0.94828\n","Epoch 351/500\n","52/52 [==============================] - 21s 406ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.3535 - val_accuracy: 0.9310\n","\n","Epoch 00351: val_accuracy did not improve from 0.94828\n","Epoch 352/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.4065 - val_accuracy: 0.9310\n","\n","Epoch 00352: val_accuracy did not improve from 0.94828\n","Epoch 353/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3165 - val_accuracy: 0.9335\n","\n","Epoch 00353: val_accuracy did not improve from 0.94828\n","Epoch 354/500\n","52/52 [==============================] - 21s 399ms/step - loss: 3.7973e-04 - accuracy: 1.0000 - val_loss: 0.3599 - val_accuracy: 0.9458\n","\n","Epoch 00354: val_accuracy did not improve from 0.94828\n","Epoch 355/500\n","52/52 [==============================] - 21s 400ms/step - loss: 5.7665e-04 - accuracy: 1.0000 - val_loss: 0.3558 - val_accuracy: 0.9310\n","\n","Epoch 00355: val_accuracy did not improve from 0.94828\n","Epoch 356/500\n","52/52 [==============================] - 21s 403ms/step - loss: 3.8146e-04 - accuracy: 1.0000 - val_loss: 0.3857 - val_accuracy: 0.9286\n","\n","Epoch 00356: val_accuracy did not improve from 0.94828\n","Epoch 357/500\n","52/52 [==============================] - 21s 401ms/step - loss: 3.7193e-04 - accuracy: 1.0000 - val_loss: 0.3999 - val_accuracy: 0.9261\n","\n","Epoch 00357: val_accuracy did not improve from 0.94828\n","Epoch 358/500\n","52/52 [==============================] - 21s 400ms/step - loss: 4.3710e-04 - accuracy: 1.0000 - val_loss: 0.3424 - val_accuracy: 0.9360\n","\n","Epoch 00358: val_accuracy did not improve from 0.94828\n","Epoch 359/500\n","52/52 [==============================] - 21s 402ms/step - loss: 1.9673e-04 - accuracy: 1.0000 - val_loss: 0.3561 - val_accuracy: 0.9286\n","\n","Epoch 00359: val_accuracy did not improve from 0.94828\n","Epoch 360/500\n","52/52 [==============================] - 21s 400ms/step - loss: 4.3774e-04 - accuracy: 1.0000 - val_loss: 0.3912 - val_accuracy: 0.9286\n","\n","Epoch 00360: val_accuracy did not improve from 0.94828\n","Epoch 361/500\n","52/52 [==============================] - 21s 400ms/step - loss: 2.9405e-04 - accuracy: 1.0000 - val_loss: 0.3568 - val_accuracy: 0.9286\n","\n","Epoch 00361: val_accuracy did not improve from 0.94828\n","Epoch 362/500\n","52/52 [==============================] - 21s 400ms/step - loss: 1.4045e-04 - accuracy: 1.0000 - val_loss: 0.3914 - val_accuracy: 0.9212\n","\n","Epoch 00362: val_accuracy did not improve from 0.94828\n","Epoch 363/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0045 - accuracy: 0.9982 - val_loss: 0.8202 - val_accuracy: 0.8522\n","\n","Epoch 00363: val_accuracy did not improve from 0.94828\n","Epoch 364/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0196 - accuracy: 0.9927 - val_loss: 0.7394 - val_accuracy: 0.9089\n","\n","Epoch 00364: val_accuracy did not improve from 0.94828\n","Epoch 365/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0035 - accuracy: 0.9994 - val_loss: 0.7934 - val_accuracy: 0.8941\n","\n","Epoch 00365: val_accuracy did not improve from 0.94828\n","Epoch 366/500\n","52/52 [==============================] - 21s 399ms/step - loss: 0.0041 - accuracy: 0.9988 - val_loss: 0.5864 - val_accuracy: 0.8990\n","\n","Epoch 00366: val_accuracy did not improve from 0.94828\n","Epoch 367/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0043 - accuracy: 0.9988 - val_loss: 0.6950 - val_accuracy: 0.8941\n","\n","Epoch 00367: val_accuracy did not improve from 0.94828\n","Epoch 368/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0147 - accuracy: 0.9963 - val_loss: 0.8150 - val_accuracy: 0.8399\n","\n","Epoch 00368: val_accuracy did not improve from 0.94828\n","Epoch 369/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0150 - accuracy: 0.9951 - val_loss: 1.0051 - val_accuracy: 0.8153\n","\n","Epoch 00369: val_accuracy did not improve from 0.94828\n","Epoch 370/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0441 - accuracy: 0.9903 - val_loss: 1.5582 - val_accuracy: 0.8030\n","\n","Epoch 00370: val_accuracy did not improve from 0.94828\n","Epoch 371/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0808 - accuracy: 0.9842 - val_loss: 0.8308 - val_accuracy: 0.8448\n","\n","Epoch 00371: val_accuracy did not improve from 0.94828\n","Epoch 372/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0164 - accuracy: 0.9945 - val_loss: 0.5563 - val_accuracy: 0.9113\n","\n","Epoch 00372: val_accuracy did not improve from 0.94828\n","Epoch 373/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0081 - accuracy: 0.9970 - val_loss: 0.5363 - val_accuracy: 0.9236\n","\n","Epoch 00373: val_accuracy did not improve from 0.94828\n","Epoch 374/500\n","52/52 [==============================] - 21s 399ms/step - loss: 0.0085 - accuracy: 0.9976 - val_loss: 0.3929 - val_accuracy: 0.9187\n","\n","Epoch 00374: val_accuracy did not improve from 0.94828\n","Epoch 375/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0037 - accuracy: 0.9982 - val_loss: 0.4258 - val_accuracy: 0.9335\n","\n","Epoch 00375: val_accuracy did not improve from 0.94828\n","Epoch 376/500\n","52/52 [==============================] - 21s 401ms/step - loss: 6.4434e-04 - accuracy: 1.0000 - val_loss: 0.4705 - val_accuracy: 0.9335\n","\n","Epoch 00376: val_accuracy did not improve from 0.94828\n","Epoch 377/500\n","52/52 [==============================] - 21s 401ms/step - loss: 5.4913e-04 - accuracy: 1.0000 - val_loss: 0.4605 - val_accuracy: 0.9187\n","\n","Epoch 00377: val_accuracy did not improve from 0.94828\n","Epoch 378/500\n","52/52 [==============================] - 21s 399ms/step - loss: 0.0014 - accuracy: 0.9994 - val_loss: 0.4617 - val_accuracy: 0.9335\n","\n","Epoch 00378: val_accuracy did not improve from 0.94828\n","Epoch 379/500\n","52/52 [==============================] - 21s 402ms/step - loss: 7.3120e-04 - accuracy: 1.0000 - val_loss: 0.4348 - val_accuracy: 0.9236\n","\n","Epoch 00379: val_accuracy did not improve from 0.94828\n","Epoch 380/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.5210 - val_accuracy: 0.9236\n","\n","Epoch 00380: val_accuracy did not improve from 0.94828\n","Epoch 381/500\n","52/52 [==============================] - 21s 400ms/step - loss: 9.7510e-04 - accuracy: 1.0000 - val_loss: 0.4426 - val_accuracy: 0.9261\n","\n","Epoch 00381: val_accuracy did not improve from 0.94828\n","Epoch 382/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0079 - accuracy: 0.9970 - val_loss: 0.5968 - val_accuracy: 0.9236\n","\n","Epoch 00382: val_accuracy did not improve from 0.94828\n","Epoch 383/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0109 - accuracy: 0.9982 - val_loss: 0.5162 - val_accuracy: 0.8966\n","\n","Epoch 00383: val_accuracy did not improve from 0.94828\n","Epoch 384/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0227 - accuracy: 0.9939 - val_loss: 1.0548 - val_accuracy: 0.8670\n","\n","Epoch 00384: val_accuracy did not improve from 0.94828\n","Epoch 385/500\n","52/52 [==============================] - 21s 403ms/step - loss: 0.0128 - accuracy: 0.9951 - val_loss: 0.6649 - val_accuracy: 0.8941\n","\n","Epoch 00385: val_accuracy did not improve from 0.94828\n","Epoch 386/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0086 - accuracy: 0.9976 - val_loss: 0.5408 - val_accuracy: 0.9187\n","\n","Epoch 00386: val_accuracy did not improve from 0.94828\n","Epoch 387/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4781 - val_accuracy: 0.9212\n","\n","Epoch 00387: val_accuracy did not improve from 0.94828\n","Epoch 388/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0013 - accuracy: 0.9994 - val_loss: 0.4534 - val_accuracy: 0.9384\n","\n","Epoch 00388: val_accuracy did not improve from 0.94828\n","Epoch 389/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0045 - accuracy: 0.9982 - val_loss: 0.5225 - val_accuracy: 0.9187\n","\n","Epoch 00389: val_accuracy did not improve from 0.94828\n","Epoch 390/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0130 - accuracy: 0.9970 - val_loss: 0.5822 - val_accuracy: 0.9089\n","\n","Epoch 00390: val_accuracy did not improve from 0.94828\n","Epoch 391/500\n","52/52 [==============================] - 21s 404ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4860 - val_accuracy: 0.9212\n","\n","Epoch 00391: val_accuracy did not improve from 0.94828\n","Epoch 392/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0027 - accuracy: 0.9988 - val_loss: 0.5127 - val_accuracy: 0.9113\n","\n","Epoch 00392: val_accuracy did not improve from 0.94828\n","Epoch 393/500\n","52/52 [==============================] - 21s 404ms/step - loss: 0.0052 - accuracy: 0.9982 - val_loss: 0.4554 - val_accuracy: 0.9212\n","\n","Epoch 00393: val_accuracy did not improve from 0.94828\n","Epoch 394/500\n","52/52 [==============================] - 21s 403ms/step - loss: 9.7412e-04 - accuracy: 1.0000 - val_loss: 0.4205 - val_accuracy: 0.9138\n","\n","Epoch 00394: val_accuracy did not improve from 0.94828\n","Epoch 395/500\n","52/52 [==============================] - 21s 404ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4684 - val_accuracy: 0.9212\n","\n","Epoch 00395: val_accuracy did not improve from 0.94828\n","Epoch 396/500\n","52/52 [==============================] - 21s 402ms/step - loss: 9.5120e-04 - accuracy: 0.9994 - val_loss: 0.4813 - val_accuracy: 0.9113\n","\n","Epoch 00396: val_accuracy did not improve from 0.94828\n","Epoch 397/500\n","52/52 [==============================] - 21s 403ms/step - loss: 0.0018 - accuracy: 0.9994 - val_loss: 0.5372 - val_accuracy: 0.9064\n","\n","Epoch 00397: val_accuracy did not improve from 0.94828\n","Epoch 398/500\n","52/52 [==============================] - 21s 403ms/step - loss: 0.0021 - accuracy: 0.9988 - val_loss: 0.8358 - val_accuracy: 0.8695\n","\n","Epoch 00398: val_accuracy did not improve from 0.94828\n","Epoch 399/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0040 - accuracy: 0.9988 - val_loss: 0.6754 - val_accuracy: 0.8966\n","\n","Epoch 00399: val_accuracy did not improve from 0.94828\n","Epoch 400/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0093 - accuracy: 0.9970 - val_loss: 0.7139 - val_accuracy: 0.8990\n","\n","Epoch 00400: val_accuracy did not improve from 0.94828\n","Epoch 401/500\n","52/52 [==============================] - 21s 403ms/step - loss: 0.0431 - accuracy: 0.9890 - val_loss: 1.5286 - val_accuracy: 0.7709\n","\n","Epoch 00401: val_accuracy did not improve from 0.94828\n","Epoch 402/500\n","52/52 [==============================] - 21s 403ms/step - loss: 0.0628 - accuracy: 0.9829 - val_loss: 0.9901 - val_accuracy: 0.8128\n","\n","Epoch 00402: val_accuracy did not improve from 0.94828\n","Epoch 403/500\n","52/52 [==============================] - 21s 403ms/step - loss: 0.0450 - accuracy: 0.9896 - val_loss: 0.8103 - val_accuracy: 0.8818\n","\n","Epoch 00403: val_accuracy did not improve from 0.94828\n","Epoch 404/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0136 - accuracy: 0.9957 - val_loss: 0.5515 - val_accuracy: 0.9187\n","\n","Epoch 00404: val_accuracy did not improve from 0.94828\n","Epoch 405/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0134 - accuracy: 0.9945 - val_loss: 0.6387 - val_accuracy: 0.8941\n","\n","Epoch 00405: val_accuracy did not improve from 0.94828\n","Epoch 406/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0040 - accuracy: 0.9988 - val_loss: 0.5354 - val_accuracy: 0.9015\n","\n","Epoch 00406: val_accuracy did not improve from 0.94828\n","Epoch 407/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0040 - accuracy: 0.9982 - val_loss: 0.4715 - val_accuracy: 0.9187\n","\n","Epoch 00407: val_accuracy did not improve from 0.94828\n","Epoch 408/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.5803 - val_accuracy: 0.9163\n","\n","Epoch 00408: val_accuracy did not improve from 0.94828\n","Epoch 409/500\n","52/52 [==============================] - 21s 403ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4137 - val_accuracy: 0.9163\n","\n","Epoch 00409: val_accuracy did not improve from 0.94828\n","Epoch 410/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0025 - accuracy: 0.9988 - val_loss: 0.6011 - val_accuracy: 0.8990\n","\n","Epoch 00410: val_accuracy did not improve from 0.94828\n","Epoch 411/500\n","52/52 [==============================] - 21s 402ms/step - loss: 6.6670e-04 - accuracy: 1.0000 - val_loss: 0.4508 - val_accuracy: 0.9187\n","\n","Epoch 00411: val_accuracy did not improve from 0.94828\n","Epoch 412/500\n","52/52 [==============================] - 21s 402ms/step - loss: 4.4518e-04 - accuracy: 1.0000 - val_loss: 0.4925 - val_accuracy: 0.9212\n","\n","Epoch 00412: val_accuracy did not improve from 0.94828\n","Epoch 413/500\n","52/52 [==============================] - 21s 402ms/step - loss: 3.8274e-04 - accuracy: 1.0000 - val_loss: 0.4424 - val_accuracy: 0.9261\n","\n","Epoch 00413: val_accuracy did not improve from 0.94828\n","Epoch 414/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0011 - accuracy: 0.9994 - val_loss: 0.4585 - val_accuracy: 0.9113\n","\n","Epoch 00414: val_accuracy did not improve from 0.94828\n","Epoch 415/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0022 - accuracy: 0.9988 - val_loss: 0.4988 - val_accuracy: 0.9064\n","\n","Epoch 00415: val_accuracy did not improve from 0.94828\n","Epoch 416/500\n","52/52 [==============================] - 21s 403ms/step - loss: 0.0062 - accuracy: 0.9982 - val_loss: 0.4537 - val_accuracy: 0.9163\n","\n","Epoch 00416: val_accuracy did not improve from 0.94828\n","Epoch 417/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0011 - accuracy: 0.9994 - val_loss: 0.4464 - val_accuracy: 0.9212\n","\n","Epoch 00417: val_accuracy did not improve from 0.94828\n","Epoch 418/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.6388 - val_accuracy: 0.8867\n","\n","Epoch 00418: val_accuracy did not improve from 0.94828\n","Epoch 419/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0138 - accuracy: 0.9963 - val_loss: 0.9360 - val_accuracy: 0.8768\n","\n","Epoch 00419: val_accuracy did not improve from 0.94828\n","Epoch 420/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0205 - accuracy: 0.9933 - val_loss: 1.0815 - val_accuracy: 0.8079\n","\n","Epoch 00420: val_accuracy did not improve from 0.94828\n","Epoch 421/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.7839 - val_accuracy: 0.8350\n","\n","Epoch 00421: val_accuracy did not improve from 0.94828\n","Epoch 422/500\n","52/52 [==============================] - 21s 404ms/step - loss: 0.0028 - accuracy: 0.9988 - val_loss: 0.5357 - val_accuracy: 0.8966\n","\n","Epoch 00422: val_accuracy did not improve from 0.94828\n","Epoch 423/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.5127 - val_accuracy: 0.9113\n","\n","Epoch 00423: val_accuracy did not improve from 0.94828\n","Epoch 424/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0018 - accuracy: 0.9994 - val_loss: 0.4688 - val_accuracy: 0.9236\n","\n","Epoch 00424: val_accuracy did not improve from 0.94828\n","Epoch 425/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0059 - accuracy: 0.9994 - val_loss: 0.5548 - val_accuracy: 0.9187\n","\n","Epoch 00425: val_accuracy did not improve from 0.94828\n","Epoch 426/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0023 - accuracy: 0.9988 - val_loss: 0.4372 - val_accuracy: 0.9286\n","\n","Epoch 00426: val_accuracy did not improve from 0.94828\n","Epoch 427/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0031 - accuracy: 0.9988 - val_loss: 0.4195 - val_accuracy: 0.9433\n","\n","Epoch 00427: val_accuracy did not improve from 0.94828\n","Epoch 428/500\n","52/52 [==============================] - 21s 401ms/step - loss: 6.7883e-04 - accuracy: 1.0000 - val_loss: 0.4407 - val_accuracy: 0.9310\n","\n","Epoch 00428: val_accuracy did not improve from 0.94828\n","Epoch 429/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0071 - accuracy: 0.9982 - val_loss: 0.7106 - val_accuracy: 0.8719\n","\n","Epoch 00429: val_accuracy did not improve from 0.94828\n","Epoch 430/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0018 - accuracy: 0.9994 - val_loss: 0.5042 - val_accuracy: 0.9212\n","\n","Epoch 00430: val_accuracy did not improve from 0.94828\n","Epoch 431/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.3945 - val_accuracy: 0.9310\n","\n","Epoch 00431: val_accuracy did not improve from 0.94828\n","Epoch 432/500\n","52/52 [==============================] - 21s 402ms/step - loss: 8.3737e-04 - accuracy: 1.0000 - val_loss: 0.5283 - val_accuracy: 0.9286\n","\n","Epoch 00432: val_accuracy did not improve from 0.94828\n","Epoch 433/500\n","52/52 [==============================] - 21s 401ms/step - loss: 3.2837e-04 - accuracy: 1.0000 - val_loss: 0.4730 - val_accuracy: 0.9384\n","\n","Epoch 00433: val_accuracy did not improve from 0.94828\n","Epoch 434/500\n","52/52 [==============================] - 21s 401ms/step - loss: 4.3571e-04 - accuracy: 1.0000 - val_loss: 0.5002 - val_accuracy: 0.9310\n","\n","Epoch 00434: val_accuracy did not improve from 0.94828\n","Epoch 435/500\n","52/52 [==============================] - 21s 404ms/step - loss: 1.6678e-04 - accuracy: 1.0000 - val_loss: 0.5043 - val_accuracy: 0.9212\n","\n","Epoch 00435: val_accuracy did not improve from 0.94828\n","Epoch 436/500\n","52/52 [==============================] - 21s 402ms/step - loss: 5.0467e-05 - accuracy: 1.0000 - val_loss: 0.4597 - val_accuracy: 0.9335\n","\n","Epoch 00436: val_accuracy did not improve from 0.94828\n","Epoch 437/500\n","52/52 [==============================] - 21s 400ms/step - loss: 5.2918e-04 - accuracy: 1.0000 - val_loss: 0.5107 - val_accuracy: 0.9261\n","\n","Epoch 00437: val_accuracy did not improve from 0.94828\n","Epoch 438/500\n","52/52 [==============================] - 21s 403ms/step - loss: 1.4688e-04 - accuracy: 1.0000 - val_loss: 0.4149 - val_accuracy: 0.9409\n","\n","Epoch 00438: val_accuracy did not improve from 0.94828\n","Epoch 439/500\n","52/52 [==============================] - 21s 402ms/step - loss: 5.8390e-05 - accuracy: 1.0000 - val_loss: 0.4631 - val_accuracy: 0.9384\n","\n","Epoch 00439: val_accuracy did not improve from 0.94828\n","Epoch 440/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0200 - accuracy: 0.9939 - val_loss: 0.7497 - val_accuracy: 0.8596\n","\n","Epoch 00440: val_accuracy did not improve from 0.94828\n","Epoch 441/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0760 - accuracy: 0.9811 - val_loss: 2.1915 - val_accuracy: 0.7857\n","\n","Epoch 00441: val_accuracy did not improve from 0.94828\n","Epoch 442/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0873 - accuracy: 0.9787 - val_loss: 1.4788 - val_accuracy: 0.7611\n","\n","Epoch 00442: val_accuracy did not improve from 0.94828\n","Epoch 443/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0243 - accuracy: 0.9939 - val_loss: 0.6587 - val_accuracy: 0.8695\n","\n","Epoch 00443: val_accuracy did not improve from 0.94828\n","Epoch 444/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0182 - accuracy: 0.9939 - val_loss: 0.6280 - val_accuracy: 0.8941\n","\n","Epoch 00444: val_accuracy did not improve from 0.94828\n","Epoch 445/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0073 - accuracy: 0.9982 - val_loss: 0.4957 - val_accuracy: 0.8818\n","\n","Epoch 00445: val_accuracy did not improve from 0.94828\n","Epoch 446/500\n","52/52 [==============================] - 21s 403ms/step - loss: 0.0023 - accuracy: 0.9988 - val_loss: 0.5657 - val_accuracy: 0.8990\n","\n","Epoch 00446: val_accuracy did not improve from 0.94828\n","Epoch 447/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4166 - val_accuracy: 0.9138\n","\n","Epoch 00447: val_accuracy did not improve from 0.94828\n","Epoch 448/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.3904 - val_accuracy: 0.9335\n","\n","Epoch 00448: val_accuracy did not improve from 0.94828\n","Epoch 449/500\n","52/52 [==============================] - 21s 401ms/step - loss: 4.5644e-04 - accuracy: 1.0000 - val_loss: 0.3956 - val_accuracy: 0.9286\n","\n","Epoch 00449: val_accuracy did not improve from 0.94828\n","Epoch 450/500\n","52/52 [==============================] - 21s 400ms/step - loss: 5.4792e-04 - accuracy: 1.0000 - val_loss: 0.3607 - val_accuracy: 0.9310\n","\n","Epoch 00450: val_accuracy did not improve from 0.94828\n","Epoch 451/500\n","52/52 [==============================] - 21s 400ms/step - loss: 3.7546e-04 - accuracy: 1.0000 - val_loss: 0.4281 - val_accuracy: 0.9236\n","\n","Epoch 00451: val_accuracy did not improve from 0.94828\n","Epoch 452/500\n","52/52 [==============================] - 21s 402ms/step - loss: 1.6705e-04 - accuracy: 1.0000 - val_loss: 0.3633 - val_accuracy: 0.9458\n","\n","Epoch 00452: val_accuracy did not improve from 0.94828\n","Epoch 453/500\n","52/52 [==============================] - 21s 400ms/step - loss: 1.5326e-04 - accuracy: 1.0000 - val_loss: 0.3740 - val_accuracy: 0.9409\n","\n","Epoch 00453: val_accuracy did not improve from 0.94828\n","Epoch 454/500\n","52/52 [==============================] - 21s 402ms/step - loss: 1.7906e-04 - accuracy: 1.0000 - val_loss: 0.4224 - val_accuracy: 0.9261\n","\n","Epoch 00454: val_accuracy did not improve from 0.94828\n","Epoch 455/500\n","52/52 [==============================] - 21s 401ms/step - loss: 6.3455e-04 - accuracy: 0.9994 - val_loss: 0.3967 - val_accuracy: 0.9360\n","\n","Epoch 00455: val_accuracy did not improve from 0.94828\n","Epoch 456/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0038 - accuracy: 0.9982 - val_loss: 0.6850 - val_accuracy: 0.9015\n","\n","Epoch 00456: val_accuracy did not improve from 0.94828\n","Epoch 457/500\n","52/52 [==============================] - 21s 403ms/step - loss: 0.0046 - accuracy: 0.9982 - val_loss: 0.5275 - val_accuracy: 0.9286\n","\n","Epoch 00457: val_accuracy did not improve from 0.94828\n","Epoch 458/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0028 - accuracy: 0.9988 - val_loss: 0.3727 - val_accuracy: 0.9310\n","\n","Epoch 00458: val_accuracy did not improve from 0.94828\n","Epoch 459/500\n","52/52 [==============================] - 21s 403ms/step - loss: 5.0502e-04 - accuracy: 1.0000 - val_loss: 0.3663 - val_accuracy: 0.9360\n","\n","Epoch 00459: val_accuracy did not improve from 0.94828\n","Epoch 460/500\n","52/52 [==============================] - 21s 403ms/step - loss: 2.5514e-04 - accuracy: 1.0000 - val_loss: 0.3993 - val_accuracy: 0.9310\n","\n","Epoch 00460: val_accuracy did not improve from 0.94828\n","Epoch 461/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0016 - accuracy: 0.9988 - val_loss: 0.4951 - val_accuracy: 0.9335\n","\n","Epoch 00461: val_accuracy did not improve from 0.94828\n","Epoch 462/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0015 - accuracy: 0.9994 - val_loss: 0.4657 - val_accuracy: 0.9212\n","\n","Epoch 00462: val_accuracy did not improve from 0.94828\n","Epoch 463/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4771 - val_accuracy: 0.9138\n","\n","Epoch 00463: val_accuracy did not improve from 0.94828\n","Epoch 464/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0052 - accuracy: 0.9988 - val_loss: 0.5876 - val_accuracy: 0.8941\n","\n","Epoch 00464: val_accuracy did not improve from 0.94828\n","Epoch 465/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0084 - accuracy: 0.9970 - val_loss: 0.7059 - val_accuracy: 0.8695\n","\n","Epoch 00465: val_accuracy did not improve from 0.94828\n","Epoch 466/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0072 - accuracy: 0.9970 - val_loss: 0.4420 - val_accuracy: 0.9039\n","\n","Epoch 00466: val_accuracy did not improve from 0.94828\n","Epoch 467/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0143 - accuracy: 0.9939 - val_loss: 0.4786 - val_accuracy: 0.9335\n","\n","Epoch 00467: val_accuracy did not improve from 0.94828\n","Epoch 468/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0038 - accuracy: 0.9976 - val_loss: 0.9589 - val_accuracy: 0.8892\n","\n","Epoch 00468: val_accuracy did not improve from 0.94828\n","Epoch 469/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0680 - accuracy: 0.9817 - val_loss: 3.4462 - val_accuracy: 0.6305\n","\n","Epoch 00469: val_accuracy did not improve from 0.94828\n","Epoch 470/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0498 - accuracy: 0.9878 - val_loss: 1.0971 - val_accuracy: 0.8448\n","\n","Epoch 00470: val_accuracy did not improve from 0.94828\n","Epoch 471/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0334 - accuracy: 0.9915 - val_loss: 0.6607 - val_accuracy: 0.8719\n","\n","Epoch 00471: val_accuracy did not improve from 0.94828\n","Epoch 472/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0183 - accuracy: 0.9957 - val_loss: 0.6438 - val_accuracy: 0.9039\n","\n","Epoch 00472: val_accuracy did not improve from 0.94828\n","Epoch 473/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0088 - accuracy: 0.9976 - val_loss: 0.5495 - val_accuracy: 0.9064\n","\n","Epoch 00473: val_accuracy did not improve from 0.94828\n","Epoch 474/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.5585 - val_accuracy: 0.9064\n","\n","Epoch 00474: val_accuracy did not improve from 0.94828\n","Epoch 475/500\n","52/52 [==============================] - 21s 403ms/step - loss: 5.9287e-04 - accuracy: 1.0000 - val_loss: 0.5081 - val_accuracy: 0.9113\n","\n","Epoch 00475: val_accuracy did not improve from 0.94828\n","Epoch 476/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0018 - accuracy: 0.9994 - val_loss: 0.6389 - val_accuracy: 0.9064\n","\n","Epoch 00476: val_accuracy did not improve from 0.94828\n","Epoch 477/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.4371 - val_accuracy: 0.9212\n","\n","Epoch 00477: val_accuracy did not improve from 0.94828\n","Epoch 478/500\n","52/52 [==============================] - 21s 401ms/step - loss: 6.5108e-04 - accuracy: 1.0000 - val_loss: 0.5458 - val_accuracy: 0.9163\n","\n","Epoch 00478: val_accuracy did not improve from 0.94828\n","Epoch 479/500\n","52/52 [==============================] - 21s 400ms/step - loss: 4.0554e-04 - accuracy: 1.0000 - val_loss: 0.5033 - val_accuracy: 0.9163\n","\n","Epoch 00479: val_accuracy did not improve from 0.94828\n","Epoch 480/500\n","52/52 [==============================] - 21s 402ms/step - loss: 3.1171e-04 - accuracy: 1.0000 - val_loss: 0.4613 - val_accuracy: 0.9236\n","\n","Epoch 00480: val_accuracy did not improve from 0.94828\n","Epoch 481/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0015 - accuracy: 0.9994 - val_loss: 0.4732 - val_accuracy: 0.9261\n","\n","Epoch 00481: val_accuracy did not improve from 0.94828\n","Epoch 482/500\n","52/52 [==============================] - 21s 402ms/step - loss: 0.0041 - accuracy: 0.9988 - val_loss: 0.6474 - val_accuracy: 0.8941\n","\n","Epoch 00482: val_accuracy did not improve from 0.94828\n","Epoch 483/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0019 - accuracy: 0.9994 - val_loss: 0.4523 - val_accuracy: 0.9187\n","\n","Epoch 00483: val_accuracy did not improve from 0.94828\n","Epoch 484/500\n","52/52 [==============================] - 21s 400ms/step - loss: 7.7317e-04 - accuracy: 1.0000 - val_loss: 0.4802 - val_accuracy: 0.9335\n","\n","Epoch 00484: val_accuracy did not improve from 0.94828\n","Epoch 485/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0017 - accuracy: 0.9988 - val_loss: 0.3766 - val_accuracy: 0.9360\n","\n","Epoch 00485: val_accuracy did not improve from 0.94828\n","Epoch 486/500\n","52/52 [==============================] - 21s 401ms/step - loss: 6.7390e-04 - accuracy: 1.0000 - val_loss: 0.4863 - val_accuracy: 0.9113\n","\n","Epoch 00486: val_accuracy did not improve from 0.94828\n","Epoch 487/500\n","52/52 [==============================] - 21s 400ms/step - loss: 4.3390e-04 - accuracy: 1.0000 - val_loss: 0.4909 - val_accuracy: 0.9138\n","\n","Epoch 00487: val_accuracy did not improve from 0.94828\n","Epoch 488/500\n","52/52 [==============================] - 21s 401ms/step - loss: 3.0796e-04 - accuracy: 1.0000 - val_loss: 0.4339 - val_accuracy: 0.9261\n","\n","Epoch 00488: val_accuracy did not improve from 0.94828\n","Epoch 489/500\n","52/52 [==============================] - 21s 400ms/step - loss: 9.2899e-04 - accuracy: 0.9994 - val_loss: 0.6169 - val_accuracy: 0.8892\n","\n","Epoch 00489: val_accuracy did not improve from 0.94828\n","Epoch 490/500\n","52/52 [==============================] - 21s 401ms/step - loss: 7.3799e-04 - accuracy: 1.0000 - val_loss: 0.4069 - val_accuracy: 0.9384\n","\n","Epoch 00490: val_accuracy did not improve from 0.94828\n","Epoch 491/500\n","52/52 [==============================] - 21s 401ms/step - loss: 8.7214e-04 - accuracy: 1.0000 - val_loss: 0.5987 - val_accuracy: 0.9212\n","\n","Epoch 00491: val_accuracy did not improve from 0.94828\n","Epoch 492/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0016 - accuracy: 0.9994 - val_loss: 0.5062 - val_accuracy: 0.9187\n","\n","Epoch 00492: val_accuracy did not improve from 0.94828\n","Epoch 493/500\n","52/52 [==============================] - 21s 399ms/step - loss: 4.2578e-04 - accuracy: 1.0000 - val_loss: 0.4886 - val_accuracy: 0.9163\n","\n","Epoch 00493: val_accuracy did not improve from 0.94828\n","Epoch 494/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.5600 - val_accuracy: 0.9089\n","\n","Epoch 00494: val_accuracy did not improve from 0.94828\n","Epoch 495/500\n","52/52 [==============================] - 21s 400ms/step - loss: 3.4619e-04 - accuracy: 1.0000 - val_loss: 0.4747 - val_accuracy: 0.9113\n","\n","Epoch 00495: val_accuracy did not improve from 0.94828\n","Epoch 496/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0482 - accuracy: 0.9878 - val_loss: 1.2833 - val_accuracy: 0.8522\n","\n","Epoch 00496: val_accuracy did not improve from 0.94828\n","Epoch 497/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0296 - accuracy: 0.9927 - val_loss: 1.7741 - val_accuracy: 0.7685\n","\n","Epoch 00497: val_accuracy did not improve from 0.94828\n","Epoch 498/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0405 - accuracy: 0.9890 - val_loss: 0.9559 - val_accuracy: 0.8522\n","\n","Epoch 00498: val_accuracy did not improve from 0.94828\n","Epoch 499/500\n","52/52 [==============================] - 21s 400ms/step - loss: 0.0121 - accuracy: 0.9963 - val_loss: 0.9348 - val_accuracy: 0.8842\n","\n","Epoch 00499: val_accuracy did not improve from 0.94828\n","Epoch 500/500\n","52/52 [==============================] - 21s 401ms/step - loss: 0.0180 - accuracy: 0.9927 - val_loss: 0.6447 - val_accuracy: 0.8990\n","\n","Epoch 00500: val_accuracy did not improve from 0.94828\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f5710072410>"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"kHmpkzRJyCrf","colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"status":"ok","timestamp":1628953407355,"user_tz":-540,"elapsed":348,"user":{"displayName":"이상민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwWA7JKqOrAuJreghl2Xa5oY8W9qpi7rlSayx_AsU=s64","userId":"07266262195408457313"}},"outputId":"85a16bc3-a6bd-4abd-b69f-1ea616eb4ed5"},"source":["import matplotlib.pyplot as plt\n","\n","plt.plot(Xception_model.history.history[\"accuracy\"], label='Xception_acc')\n","plt.plot(Xception_model.history.history[\"val_accuracy\"], label='Xception_val')\n","\n","plt.legend()\n","plt.show()"],"execution_count":12,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"qcElIu93yIQU","executionInfo":{"status":"ok","timestamp":1628953421345,"user_tz":-540,"elapsed":12642,"user":{"displayName":"이상민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwWA7JKqOrAuJreghl2Xa5oY8W9qpi7rlSayx_AsU=s64","userId":"07266262195408457313"}}},"source":["Xception_model = tf.keras.models.load_model('/content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_Xception.h5', compile=False)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"hR4N2pAZyiR-"},"source":["!mkdir images_test/none\n","!mv images_test/*.png images_test/none"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rxH98QOgyu1z"},"source":["datagen = ImageDataGenerator(rescale=1./255)\n","test_generator = datagen.flow_from_directory('./images_test', target_size=(224,224), color_mode='grayscale', class_mode='categorical', shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nFEcoCR-3DNH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628953467072,"user_tz":-540,"elapsed":34087,"user":{"displayName":"이상민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwWA7JKqOrAuJreghl2Xa5oY8W9qpi7rlSayx_AsU=s64","userId":"07266262195408457313"}},"outputId":"5f91d1d7-31e7-4adc-f289-d9be0d9da8c7"},"source":["Xception_predict = Xception_model.predict_generator(test_generator).argmax(axis=1)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2001: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n","  warnings.warn('`Model.predict_generator` is deprecated and '\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"qYhGZuzr1AjD"},"source":["submission = pd.read_csv('/content/drive/MyDrive/DACON_CVLC/data/submission.csv')\n","submission.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VWALVGA1shFz"},"source":["import numpy as np\n","mylist = []\n","\n","for i in range(len(submission)):\n","    name =  test_generator.filenames\n","    id = name[i].split('/')[1].rstrip('.').split('.')[0]\n","    mylist.append(id)\n","print(mylist)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7xjLSWZJvuVK"},"source":["for i in range(len(submission)):\n","    submission[\"id\"][i] = mylist[i]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WNg9gk9z3Noq","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1628953469916,"user_tz":-540,"elapsed":64,"user":{"displayName":"이상민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwWA7JKqOrAuJreghl2Xa5oY8W9qpi7rlSayx_AsU=s64","userId":"07266262195408457313"}},"outputId":"fe7ebf01-e0b6-4908-e48b-5210b9b93c7d"},"source":["submission[\"Xception_predict\"] = Xception_predict\n","submission.head()"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>digit</th>\n","      <th>Xception_predict</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>10000</td>\n","      <td>0</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>10001</td>\n","      <td>0</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>10002</td>\n","      <td>0</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>10003</td>\n","      <td>0</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>10004</td>\n","      <td>0</td>\n","      <td>5</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      id  digit  Xception_predict\n","0  10000      0                 4\n","1  10001      0                 4\n","2  10002      0                 6\n","3  10003      0                 9\n","4  10004      0                 5"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"-Smd-xg6deOK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628953485743,"user_tz":-540,"elapsed":15845,"user":{"displayName":"이상민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwWA7JKqOrAuJreghl2Xa5oY8W9qpi7rlSayx_AsU=s64","userId":"07266262195408457313"}},"outputId":"356789cd-adfe-43de-9d72-d98183672eff"},"source":["from collections import Counter\n","\n","for i in range(len(submission)) :\n","    predicts = submission.loc[i, ['Xception_predict']]\n","    submission.at[i, \"digit\"] = Counter(predicts).most_common(n=1)[0][0]\n","\n","submission.head()"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>digit</th>\n","      <th>Xception_predict</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>10000</td>\n","      <td>4</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>10001</td>\n","      <td>4</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>10002</td>\n","      <td>6</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>10003</td>\n","      <td>9</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>10004</td>\n","      <td>5</td>\n","      <td>5</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      id  digit  Xception_predict\n","0  10000      4                 4\n","1  10001      4                 4\n","2  10002      6                 6\n","3  10003      9                 9\n","4  10004      5                 5"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"Pg9m6Zgk4foS"},"source":["submission = submission[['id', 'digit']]\n","submission.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"flAHWrtH4flu","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1628954584145,"user_tz":-540,"elapsed":416,"user":{"displayName":"이상민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwWA7JKqOrAuJreghl2Xa5oY8W9qpi7rlSayx_AsU=s64","userId":"07266262195408457313"}},"outputId":"a718a7c1-62e8-4764-fc50-4bf7685da06e"},"source":["from google.colab import files\n","\n","submission.to_csv('/content/drive/MyDrive/DACON_CVLC/Submission/Xception_model.csv', index=False)\n","files.download('/content/drive/MyDrive/DACON_CVLC/Submission/Xception_model.csv')"],"execution_count":24,"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/javascript":["download(\"download_80fb500f-c438-4f42-bcf9-68142bc12a95\", \"Xception_model.csv\", 155898)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}}]}]}