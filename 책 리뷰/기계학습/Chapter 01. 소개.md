# 기계학습 Chapter 1. 소개

# Chapter 1. 소개

## 1. 기계학습이란

### 1. 기계 학습의 정의

학습

> "경험의 결과로 나타나는, 비교적 지속적이 행동의 변화나 그 잠재력의 변화. 또는 지식을 습득하는 과정"

기계 학습의 중요성

> 컴퓨터가 경험을 통해 학습할 수 있도록 프로그래밍할 수 있다면, 세세하게 프로그래밍해야 하는 번거로움에서 벗어날 수 있다.

기계 학습이란, 특정한 응용 영역에서 발생하는 **데이터(경험)를 이용하여 높은 성능으로 문제를 해결하는 컴퓨터 프로그램을 만드는 작업**을 뜻한다.

### 2. 지식 기반 방식에서 기계 학습으로의 대전환

컴퓨터는 사람이 어려워하는 일들 엄청난 크기의 수 곱셈, 복잡한 함수의 미분이나 적분을 아주 쉽게 하지만, 사람이 어려움 없이 순식간에 수행하는 얼굴 인식이나 개와 고양이 구별은 무척 서툴다.

1950년대에는 컴퓨터라는 기계에 지능을 부여할 수 있다는 기대감이 한껏 부풀어 오르고 있었으며, 해당 연구를 진행하는 연구원이나 학자들은 "인공지능(AI)"이라는 거창한 단어를 만든다.

이들은 사람이 어려워하는 체커 게임, 의료진단, 광물 탐사 등에서 상당한 성과를 거두기도 하였다.

그리고 사람이 쉽게 수행하는 문자 인식과 같은 패턴 인식 문제에 도전하였는데, 당시에는 사람이 인식할 때 사용할 것으로 보이는 **지식을 추려 프로그램에 심는 접근방식**을 채택하였으며,
예를 들면, "구멍이 2개이고 중간 부분이 홀쭉하며, 맨 위와 아래가 둥근 모양이라면 8이다"라는 규칙을 만들어 사용하는 방식이며, 이를 지식 기반(knowledge-based) 또는 규칙 기반(rule-based) 방식이라고 한다.

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled.png)

하지만 지식 기반의 방식을 아무리 세세하게 표현한다 할지라도 벗어나는 샘플이 발생할 수 있다. 
숫자는 10개의 부류밖에 없지만, 한글이나 한자처럼 수천 또는 수만 부류를 가진 언어의 일일이 규칙을 만드는 것은 매우 힘든 일이다. 위에 그림 처럼 단추라는 물체를 인식하는 경우라면 복잡도는 더 높아진다. 단추의 모양, 색깔, 놓여 있는 형태, 그림자, 일부 가림 등 다양한 변화를 감당할 수 없다.

초창기에는 지식 기반의 방식을 신봉하는 사람들이 주도권을 잡았다. 하지만 오래 지나지 않아 지식기반 방식에 분명한 한계가 있다는 사실을 깨닫게 된다. "사람은 변화가 심한 장면을 아주 쉽게 인식하지만, 왜 그렇게 인식하는지 서술하지는 못한다" 라는 것이다.

사람은 태어나면서부터 아주 많은 단추를 보면서 단추라는 개념을 학습하고, 아주 많은 개와 고양이를 보면서 이들을 구분하는 기준을 학습한다. 그리고 그냥 인식한다. 하지만 단추를 말로 서술하지는 못한다. '가운데 구명이 몇 개 있는 둥근 물체' 라고 서술하는 순간 단추가 아닌 것이 되고 단추가 아닌것이 단추가 된다.

이러한 큰 깨달음 이후에 인공지능의 주도권은 **지식기반 방식에서 기계 학습으로 넘어갔다.**

기계학습은 데이터를 중심으로 하는 접근 방식을 채택한다. 인식할 대상을 컴퓨터에 일일이 설명하려는 시도 대신, **데이터를 중심으로 하는 접근 방식**을 채택한다. 숫자나 단추를 인식하려면 그림 1-2와 같은 영상을 수집하고 데이터로 간주하여 기계 학습 알고리즘에 입력한다. 소리를 인식하려는 프로그램을 만들려면, 여러 사람이 발음한 음성 신호를 수집하여 데이터로 사용한다.

이렇게 기계 학습 알고리즘은 데이터를 이용하여 스스로 학습해 높은 성능을 보장하는 인식 프로그램을 자동으로 만든다.

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%201.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%201.png)

### 3. 기계 학습 개념

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%202.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%202.png)

그림 1-4에서 가로축은 시간이고 세로축은 이동체의 위치라 가정하자.
예를 들어, 가로축은 투입한 돈이고 세로축은 수익이거나, 가로축은 약물의 농도이고 세로축은 몸의 반응 정도 일 수도 있다. 가로축에서 2.0 4.0 6.0 8.0인 점을 샘플링하여 이동체의 위치를 측정한 결과, 3.0 4.0 5.0 6.0인 값을 4개를 얻었다고 가정하자. 이때 임의의 시간에 대한 이동체의 위치를 예측하는 문제를 풀어야한다.

기계 학습은 이러한 형태의 **예측(Prediction)문제를 풀며, 예측에는 회귀와 분류가 있다.**

그림 1-4와 같이 **실숫값을 예측하는 문제를 회귀(Regression)**라고 한다. 그림 1-2와 같이 숫자 인식 문제에서는 10가지 부류 중 하나를 예측해야 하는데, 이렇게 **부류를 예측하는 문제를 분류(Classification)**라고 한다.

가로축을 입력, 세로축을 출력으로 볼 수도 있다. 가로축은 입력 패턴에 해당하고, 세로축은 패턴이 가져야 할 목푯값에 해당한다. 그림 1-4는 시간이 패턴이고 이동체 위치가 목푯값인 단순한 상황이다. 그림 1-2의 숫자 인식은 숫자를 나타내는 비트맴 영상이 패턴이고 8이라는 부류 정보가 목푯값이다.

기계 학습에서는 가로축에 해당하는 패턴을 특징이라고 한다. 앞으로 특징을 X, 목표값을 y로 표기한다. 그림 1-4에서는 특징이 하나뿐이지만, 대부분 2개 이상의 특징으로 구성되는 특징 벡터 형태를 띠므로 스칼라 표기인 x가 아니라 벡터 표기인 X를 사용한다. 또한 특징 벡터와 목푯값의 쌍이 여러개 이므로 이들을 구분하기 위해서 X1, X2, X3, X4와 y1, y2, y3, y4로 표기하자.

예를 들어 그림 1-4에서는 X1= (2.0)이고 y1=3.0이다. 그림 1-4의 4개의 값 쌍은 기계 학습에 주어지는 데이터로서, 학습 집합 또는 훈련 집합 이라고 하며, X = {x1, x2, x3, x4}, Y = {y1, y2, y3, y4}로 표기한다.

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%203.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%203.png)

그림 1-4의 훈련 집합의 네 점이 직선을 이루므로 이들의 분포를 직선으로 표현하자.
기계 학습에서는 이러한 의사결정을 "모델로 직선을 선택했다." 라고 한다. 점들이 좀 더 복잡한 비선형 분포를 이룬다면 2차 곡선 또는 더 높은 차수의 곡선을 모델로 선택해야 한다. 직선이라는 모델은 (1.2)로 표현할 수 있다. 이 모델은 w와 b라는 2개의 매개변수를 가진다.

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%204.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%204.png)

기계 학습이란 가장 정확하게 예측할 수 있는, 즉 **최적의 매개변숫값을 찾는 작업**이다.
그림 1-4에서 w=0.5, b=2.0이 최적의 매개변수이고 그림으로 표현하면 f3에 해당한다.

처음에 최적의 매개변숫값을 알 수 없다고 가정하자. 임의의 값으로 설정하여 f1이 되었는데 f2를 거쳐 최적인 f3에 도달하였다면, 모델은 성능이 점점 개선되는 과정을 거쳐 최적의 상태에 도달했다고 말할 수 있다. 이처럼 성능을 개선하면서 최적의 상태에 도달하는 작업을 학습 또는 훈련이라고 한다.

학습 과정을 마치면 그때부터 학습된 모델을 이용하여 예측할 수 있다. 예를 들어 10.0이라는 시간에서 이동체의 위치를 알고 싶으면 f3(10.0) = 0.5 * 10.0 + 2.0을 계산하여 7.0을 출력할 수 있다.

이처럼 훈련집합에 없는 **'새로운' 샘플에 대한 목푯값을 예측하는 과정을 테스트**라고 한다.
새로운 샘플을 가진 데이터 집합을 테스트 집합이라 하는데, 테스트집합에 대해 높은 성능을 가진 성질을 일반화 능력이라 한다. 그리고 훈련집합과 테스트집합을 합쳐 데이터베이스라 한다.

### 4. 사람의 학습과 기계학습

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%205.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%205.png)

사람은 자신의 주위에서 발생하는 신호 중 관심 있는 것만 **선별하여 능동적으로 학습한다.**
반면, 기계는 사람이 준비한 데이터를 입력받아, 수학을 이용하여 최적의 매개변숫값을 찾아가는 수동적인 학습을 한다. 사람은 수영을 학습하면서, 강사와 동료의 얼굴과 목소리, 수영장의 구조와 특성, 나이나 성별에 따른 능력 차이 등 여러 과업을 동시에 학습한다. 반면, 기계는 특정한 응용 영역의 특정한 과업 하나만을 학습할 수 있다.

예를 들어, 필기 숫자를 인식하거나 개와 고양이를 구별하거나 카메라로 찍은 영상을 수채화풍의 영상으로 변환하는 등 **특정한 과업 하나만 학습한다.**

## 2. 특징 공간에 대한 이해

### 1. 1차원과 2차원 특징 공간

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%206.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%206.png)

예를 들어, 특징 벡터는 야구 선수의 (몸무게, 키)이고 목표값은 장타율일 수도 있고, 특징 벡터는 환자의 (체온, 두통), 목푯값은 감기여부일 수도 있다.

### 2. 다차원 특징 공간

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%207.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%207.png)

Haberman survival 데이터베이스는 유방암 환자의 데이터이다. 환자의 나이, 수술 연도, 양상 림프샘 개수가 3차원 특징 공간을 형성하며, 목표값은 수술 후 생존 연수이다.
(목푯값은 1 또는 2의 값을 가지며 1은 5년 이상 생존, 2는 5년 이내 사망을 뜻한다.)

x = (30, 1964, 1)^T, y = 1 이라는 샘플은 양성 림프샘을 1개 가진 30세 환자가 1964년 수술한 후 5년 이상 살았다는 사실을 표현한다.

MNIST 데이터베이스에는 필기 숫자를 제공하며, 28*28 크기의 비트맵으로 표현한다.

따라서 784개의 화소가 784차원의 특징 공간을 형성하며, 목푯값은 0 ~ 9의 10개의 부류 중 하나이다.

Farm ads는 가축 관련 웹사이트 문서에서 단어의 발생 빈도를 조사해 만든 데이터베이스이며, 총 54,877개의 단어에 대한 빈도를 조사했으므로, 54877차원의 특징 공간이 된다.

$x = (x_1,x_2,,,x_d)^T$로 표기한다.

차원이 증가하면 모델의 매개변수 개수가 함께 증가한다.

1차원에서는 2개의 매개변수로 표현이 가능하지만, 
d차원에서는 $y = w_1x_1+w_2x_2+w_dx_d+b$로 표현된다.

만일 2차 곡선을 모델로 선택한다면 매개변수의 개수가 더 증가한다.

1차원 특징 공간

$y = w_1x_1^2+w_2x_1+b$

d차원 특징 공간

$y = w_1x_1^2+w_2x_2^2+,,,+w_dx_d^2+w_{d+1}x_1x_2+,,,+w_{d^2}x_{d-1}x_d+w_{d^2+1}x_1+,,,+w_{d^2+d}x_d+b$

차원에 매개변수의 수에 따른 수식 $d^2+d+1$

Iris d=4, 21

MNIST d=784, 615,441개

차원이 늘어날수록 그리고 모델의 차수가 늘어날수록 추정해야하는 매개변수가 많아져 더 어려운 문제가 된다. 

### 3. 특징 공간 변환과 표현 학습

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%208.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%208.png)

a와 d가 하나의 부류에 속하고 b, c가 나머지 부류에 속한다고 가정.

직선 모델로는 두 부류를 100% 정확하게 분류할 수 없다. 어떤 직선을 사용한다고 하더라고 75%의 정확도에 불과하다.

이를 구분하기 위해서 **특징 벡터를 변환하여 사용한다.**

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%209.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%209.png)

위의 식을 사용하여서 a, b, c, d를 새로운 특징 벡터로 변환하여

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2010.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2010.png)

이렇게 변환된 특징 벡터는 직선 모델을 이용해서 100% 분류가 가능하다.

이처럼 현대의 기계 학습은 좋은 특징 공간을 찾아내는 작업을 매우 중요하게 생각하며,
최근에는 **좋은 특징 공간을 자동으로 찾아낸다는 뜻에서 '표현 학습(Representation Learning)'**이라는 용어를 자주 사용한다.

그림 1-6처럼 실제 세계에서 발생하는 고차원 데이터 분포에서는 표현 학습 알고리즘을 사용해야 한다. 딥러닝은 신경망 구조에 여러 은닉층을 두고, 왼쪽 은닉층에서는 저급 특징을 추출하고 오른쪽은 갈수록 고급 특징을 추출한다.

예를 들면 영상을 인식할 때에 저급 특징은 에지나 구석점 등이 해당되고, 고급 특징은 결합한 얼굴이나 바퀴 등이 해당된다.

신경망 학습 알고리즘이 특징 공간 변환 공식을 자동으로 찾아내 이러한 계층적인 특징 추출 기능을 부여한다.

차원이 증가한다고 수식이나 알고리즘이 바뀌어야 하는 것은 아니다.

예를 들어, 두 점 사이의 거리를 계산하는 경우에

$a=(a_1,a_2,a_3)^T, b=(b_1,b_2,b_3)^T$가 있다면,

$dist(a,b) = \sqrt{(a_1-b_1)^2+(a_2-b_2)^2+(a_3-b_3)^2}$ 과 같이 계산할 수 있다.

이 수식을 d차원으로 확대하면,

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2011.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2011.png)

(1.7)처럼 나타낼 수 있으며, 이처럼 기계 학습에서는 저차원에서 고안한 식 또는 알고리즘을 그대로 고차원에 적용할 수 있다.

하지만 고차원이 되면서 현실적인 문제가 발생하는데, 예를 들어 4차원인 Iris 데이터에서 네 축을 100개 구간으로 나눈다면 전체 공간이 $100^4$ 즉, 1억 개의 칸으로 나뉜다.

Iris 데이터베이스는 150개의 샘플을 가지고 있는데 이는 1억개의 칸에 150개 샘플이 분포되는 셈이기 때문에 이처럼 차원이 높아지면서 거대한 특징 공간이 형성되고, 이 때문에 발생하는 현실적인 문제를 **차원의 저주(curse of dimensionality)**라고 한다.

## 3. 데이터에 대한 이해

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2012.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2012.png)

### 1. 데이터 생성 과정

2개의 주사위를 던져 나온 눈의 합을 x라 하면 x는 2~12 사이의 정수를 가진다.

이때 $y= (x-7)^2+1$점을 받는 게임을 한다고 하자.

훈련 집합 $x = (3,10,8,5), y = (17,10,2,5)$를 얻을 수 있다.
이러한 상황을 두고 데이터 생성 과정(data generating process)을 모두 알고 있다고 할 수 있다. 왜냐하면 샘플 x가 발생할 확률을 정확히 알고, x가 발생했을 때 목푯값 y도 정확히 계산할 수 있기 때문이다.

예를 들어, x가 2일 확률은 1/36로, P(x=2)=1/36과 같이 쓸 수 있다. x가 2라면 목푯값 y는 $(2-7)^2+1=26$이 된다. 이렇듯 정확하게 예측할 수 있다. 또한 가상으로 데이터를 생성할 수도 있다.

하지만 이러한 상황은 인위적으로 설정한 가정에서나 성립하며, 기계 학습이 풀어야 하는 실제 문제에서는 절대 발생하지 않는다. 기계 학습에서는 데이터 생성 과정을 전혀 모른다고 할 수 있다. 단지 미지의 데이터 생성 과정에서 수집한 데이터, 즉 훈련 집합 x만 주어진다.

기계 학습은 **x를 이용하여 예측 모델 또는 생성 모델을 근사 추정할 수 있을 뿐**이다.

### 2. 데이터베이스의 중요성

기계 학습은 훈련집합을 이용하여 데이터 생성 과정을 역으로 추정하는 문제이므로,
데이터베이스의 품질이 무척 중요하다. **주어진 응용에 맞게 충분히 다양한 데이터를 충분한 양만큼 수집하면, 추정의 정확성이 높아진다.**

예를 들어, 좋은 조명 아래에서 정면으로 촬영한 얼굴 영상을 모은 훈련집합을 사용하여 학습한 경우, 기운 얼굴이나 그림자가 진 얼굴을 인식하는 응용에서는 높은 성능을 기대할 수 없다. 따라서 기계 학습에서는 주어진 응용 환경을 자세히 살핀 다음 **그에 맞는 데이터베이스를 확보하는 일이 무엇보다 중요**하다.

### 3. 데이터베이스 크기와 기계 학습 성능

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2013.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2013.png)

예를 들어 MNIST는 훈련집합에 60,000개 샘플이 있는데, 부류당 6,000개씩이므로 꽤 크다고 생각할 수 있다. 샘플 하나가 28*28 흑백 비트맵 형태로 표현되므로, 784차원이고 축마다 0과 1이라는 값을 가진다. 이론적으로 가능한 서로 다른 샘플 데이터의 크기는 $2^{784}$이지만, 이 중 숫자 형태를 이루지 못하는 것이 대다수이고 방대한 크기이지만,

이를 지구와 비교하면 MNIST 데이터베이스는 지구 상의 모래알갱이를 구성하는 원자 하나에도 미치지 못하는 크기이다.

하지만 현대 기계 학습은 이렇게 작은 MNIST 훈련 집합을 이용하여 0.23%의 매우 낮은 오류율을 가진 인식 프로그램을 만든다. 위와 같은게 가능한 이유는 2가지이며,

첫번째는 방대한 공간에서 실제 데이터가 발생하는 곳은 훨씬 작은 부분공간이다.
예를 들어, 필기 숫자라는 도메인에서 숫자가 아닌 샘플이 발생할 확률은 거의 0에 가까울 것이다. 그런데 28*28 크기의 이진 비트맵에서 생성할 수 있는 $2^{784}$가지의 샘플 중에는 숫자 모양이 아닌 것이 훨씬 많으며, 숫자 형태를 띄는 것은 작은 부분 공간에 모여 있을 것이다.

두번째는 발생하는 데이터가 심한 변화를 겪지만 일정한 규칙에 따라 매끄럽게 변화한다.
예를 들어, 2라는 패턴이 조금씩 회전한다면 일정한 규칙에 따라 매끄럽게 변한다. 이러한 가정을 매니폴드 가정이라 한다.

### 4. 데이터 가시화

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2014.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2014.png)

4차원 이상은 하나의 공간에 데이터를 그려 넣을 수 없다.

4차원 이상의 공간을 초공간(hyper space)이라 하는데 예를 들어 앞서 설명한 Iris 데이터베이스에는 붓꽃의 꽃받침 길이, 꽃받침 너비, 꽃잎 길이, 꽃잎 너비가 기록되어 있다.

따라서 4차원 특징 공간이 형성되며, 목푯값은 setosa, versicolor, virginica의 3종 중 하나이다. 위에 그림 처럼 초공간의 분포하는 데이터를 가시화하며, 이 그림에서는 4개의 축 중 2개씩을 조합하여 6개의 2차원 공간에 그린다.

예를 들어, 두 번째 행의 첫 번째 열에 있는 분포에서 가로축은 꽃받침의 길이이고 세로축은 꽃받침의 너비이다.

이 기법을 사용하면  4차원 특징 공간을 2차원 또는 3차원으로 축소한 다음 하나의 공간에 그릴 수 있다.

## 4. 간단한 기계 학습의 예

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%202.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%202.png)

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%204.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%204.png)

해당 예제에서는 직선 모델을 사용하여 회귀 문제를 푸는데, 이러한 기계 학습 알고리즘은 선형 회귀(linear regression)라고 한다.

선형 회귀는 식 (1.2)의 직선 모델을 사용하므로 추정해야 할 매개변수는 w와 b, 2개 이다.
이 매개변수를 묶어 $\Theta = (w,b)^T$로 표기한다.

처음에는 최적의 매개변숫값을 알 수 없으므로 난수를 생성하여 $\Theta_1=(w_1,b_1)^T$가 되었다고 가정하자. 이제 $\Theta_1$을 개선하여 $\Theta_2$를 얻고, $\Theta_2$를 개선하여 $\Theta_3$을 얻는 방식을 반복하여 결국 최적의 매개변수 $\Theta$에 도달해야 한다. 이때 직선을 움직이려면 동력이 필요한데, 수학에서는 목적함수(objective function)가 그 역할을 한다.

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2015.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2015.png)

식 (1.8)의 J는 목적함수이며, 목적함수를 비용함수(cost function)라고도 한다.

$f_\theta$는 $\Theta$를 매개변수로 가지는 직선을 뜻한다. $f_\theta(X_i)$는 예측 함수의 출력이고, $y_i$는 예측 함수가 맞추어야하는 목표값이므로 $f_\theta(X_i)-y_i$는 $x_i$에서의 오차이다. 

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2016.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2016.png)

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2017.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2017.png)

기계 학습 알고리즘이 $\Theta_2$를 한 번 더 개선하여 $\Theta_3$을 만들었다면, 목적함수값은 0.0이 되어 최적의 매개변수를 찾은 셈이다. 이러한 과정을 반영하여 기계 학습이 해야 하는 일을 공식화하면 $\theta = argminJ(\theta)$와 같다.

목적함수는 기계 학습이 최적의 성능을 찾아가는 데 등대 같은 역할을 한다.

기계 학습 알고리즘은 목적함숫값이 작아지는 방향을 찾아 매개변숫값을 조정하는 일을 반복하는데, 목적함수가 0.0 또는 0.0에 아주 가까운 값으로 수렴하면 그때 학습을 마친다.

이처럼 작은 개선을 반복하여 최적해를 찾아가는 방법을 수치적 방법(numerical method)이라고 한다.

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2018.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2018.png)

유도한 식에 훈련집합의 샘플을 대입하여 한꺼번에 최적해를 구하는 방식을 분석적 방법(analytical method)이라 한다.

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2019.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2019.png)

위의 그림 1-12처럼 데이터가 직선이 아닌 상황이다.
데이터 자체가 선형이 아닐 뿐더러 측정 중에 발생한 잡음이 섞여있다. 이 데이터에 직접 근사를 하면, 최적해를 찾더라고 적지 않은 오차를 감수할 수 밖에 없다.

## 5. 모델 선택

### 1. 과소적합과 과잉적합

그림 1-12의 데이터 분포에서는 기계 학습이 최적해를 찾더라도 큰 오차가 생기는데, 오차가 클 수밖에 없는 이유는 '모델의 용량이 작기' 때문이다. 직선 모델은 단지 데이터가 직선을 이루는 경우만 수용할 수 있다. 이러한 현상을 과소적합(underfitting)이라고 한다.

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2020.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2020.png)

이러한 문제를 해결하기 위해서 가장 쉽게 생각할 수 있는 대안은 더 높은 차원의 다항식,
즉, 비선형 모델(nonlinear model)을 사용하는 것이다. 식 (1.10)을 사용하였다고 가정하면, 1차 미분이 0인 극점(extreme point)이 최대 11개인 꽤 복잡한, 즉 용량이 큰 모델을 가진 셈이다. 용량이 커진 대신 추정해야 하는 매개변수가 $w_{12},w_{11},w_{10},,,w_2,w_1,w_0$로 13개나 되어 학습이 어려워진다.

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2021.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2021.png)

식 (1.10)의 비선형 모델을 이용하여 근사화한 결과이다. 훈련 집합에 있는 점 대부분을 지나며, 지나지 않더라고 아주 가까우므로 완벽에 가깝게 학습했다고 할 수 있다.

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2022.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2022.png)

하지만 학습된 모델로 훈련집합에 없던 '새로운' 데이터를 예측한다면 크게 다른 평가를 해야만 한다. $x_0$에서 예측하면 그림 1-14의 빨간색 점이 된다. 이렇게 예측된 점은 훈련집합에 있는 점들의 경항에 어울린다고 말할 수 없다. 직관적으로 가늠하면 빨간 막대로 표시된 범위에서 예측했어야 합리적이다.

식 (1.10)의 12차 다항식도 모델로 적합하지 않다고 볼 수 있다.

해당 원인의 이유는 12차 다항식의 역설적으로 용량이 너무 크기 때문이다.
주어진 데이터 분포와 비교해 용량이 너무 크다 보니 아주 작은 잡음까지 수용한 것이다.

결국, 훈련 집합은 완벽에 가깝게 대변하지만, 새로운 점이 들어오면 제대로 대처하지 못한다. 이러한 현상을 과잉적합(overfitting)이라고 한다.

기계학습의 최종 목표를 훈련 집합에 없는 새로운 샘플을 정확하게 예측하는 것이므로 과소적합이나 과잉적합 모두 바람직하지 않다.

그림 1-13을 보았을 때 3, 4차는 비교적 합리적이라고 할 수 있다. 하지만 '둘 중 어느 것이 더 나을까'라는 의문과 '이보다 더 나은 차수 또는 다른 종류의 함수는 없을까'라는 의문이 여전히 남는다.

이 질문에 답할 수 있는 모델 선택 방법을 다루기 전에 과소적합과 과잉적합을 바이어스와 분산이라는 축면에서 다시 한번 살펴보자.

### 2. 바이어스와 분산

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2023.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2023.png)

기계 학습의 목표는 훈련집합에 없는 새로운 데이터, 즉 테스트 집합에 대해 높은 성능을 보장하는 프로그램을 만드는 것이다. 이러한 특성을 일반화 능력이라고 한다.

그림 1-13의 12차 다항식 모델은 훈련집합에서는 아주 높은 성능을 보이지만, 테스트 집합에서는 아주 낮은 성능을 보일 것이다. 즉, 훈련집합과 테스트집합의 성능에 큰 차이가 있다.이는 일반화 능력이 무척 낮다고 할 수 있다.

반면, 1차나 2차 다항식은 훈련집합과 테스트집합 모두에서 낮은 성능을 보일 것이다.
3차나 4차 다항식은 훈련집합에 대한 성능은 12차보다 낮겠지만, 테스트집합에서는 높은 것이다. 결국 3차, 4차는 훈련집하과 테스트집합의 성능차이가 작아 일반화 능력이 뛰어나다고 할 수 있다.

그림 1-15(a)를 보면 데이터 생성 과정과 2차 다항식 모델 사이에 큰 차이가 있음을 알 수 있다. 이러한 현상을 바이어스(bias)가 크다고 표현한다.

세 가지 훈련집합 모두 데이터와 학습된 모델 사시에 큰 차이가 있어, 2차 모델은 바이어스가 큼을 확실히 알 수 있다. 반면 훈련집합이 바뀌더라도 학습 결과로 얻은 곡선들이 비슷하다는 사실을 확인할 수 있다. 이러한 현상을 분산(variance)이 작다고 표현한다.

대체로 단순한 모델일수록 바이어스는 크고 분산은 작다. 2차 모델보다 단순한 1차 모델은 바이어스는 더 크고 분산은 더 작다. 반대로 복잡한 모델은 바이어스는 작고 분산은 크다.

기계 학습의 목표는 낮은 바이어스와 낮은 분산을 가진 예측기(predictor)을 만드는 것이다. 하지만 바이어스와 분산은 하나를 낮추면 다른 것이 높아지는 트레이드오프(tradeoff)라는 성질이 있다. 결국, 바이어스의 희생을 최소로 유지하면서 분산을 최대로 낮추는 전략을 써야 한다.

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2024.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2024.png)

결국 기계 학습의 목표는 그림 1-16의 왼쪽 아래 상황에 도달하는 것이다.

### 3. 검증집합과 교차검증을 이용한 모델 선택 알고리즘

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2025.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2025.png)

지금까지는 훈련집합으로 모델을 학습하고, 테스트집합으로 학습된 모델의 일반화 능력을 측정하였다. 이러한 과정은 여러 모델을 독립적으로 학습시킨 후 그중 가장 좋은 모델을 선택해야 한다. 이때 모델을 비교하는 데 사용할 별도의 데이터가 필요한데, 이 데이터 집합을 검증집합(validation set)이라고 한다. 알고리즘 1-2는 검증집합을 이용한 모델 선택(model selection)과정을 설명한다.

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2026.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2026.png)

데이터를 수집하는 일은 비용이 많이 들어 데이터의 양이 대부분 부족하다.
이러한 상황에서는 검증집합을 따로 마련하기 힘든데, 교차검증(cross-validation)을 이용하면 효과적이다. k-겹 교차검증에서는 훈련집합을 같은 크기로 나누어 k개의 그룹을 만든다. 그런 다음 그룹 1개를 남기고 k-1개로 모델을 학습시킨 후 남긴 그룹으로 성능을 평가한다. 남기는 그룹을 달리하면서 이 과정을 k번 반복하면 k개의 성능을 얻게 되는데, 이를 평균하여 검증 성능으로 취한다.

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2027.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2027.png)

이 알고리즘은 난수를 이용하여 새로운 훈련집합을 샘플링한다. 이때 대치를 허용하여 같은 샘플이 여러 번 뽑힐 수 있다.

### 4. 모델 선택의 한계와 현실적인 해결책

알고리즘 1-2 ~ 1-4가 하는 일은 $\omega$에 있는 모델 중 가장 좋은 것을 찾아내는 것이다.
이때 모델집합을 어떻게 구성할지가 문제이다.

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2021.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2021.png)

그림 1-13 상황에서는 서로 다른 차수의 여러 다항식 모델을 넣어 둔 셈이다.

하지만 실제 세계에서 발생한 고차원 데이터에서는 다항식 모델을 사용하지 않는다.

현대 기계 학습은 용량이 충분히 큰 모델을 선택한 후, 선택한 모델이 정상을 벗어나지 않도록 여러 가지 규제 기업을 적용하는 현실적인 접근방법을 채택한다.

예를 들어, 그림 1-13의 데이터는 12차 곡선을 모델로 선택한 다음 규제 기법을 적용하는 것이다. 특히 딥러닝에서는 적절한 규제 기법의 적용이 문제 해결의 열쇠가 될 정도로 중요하다.

## 6. 규제

현대 기계 학습이 높은 일반화 능력을 확보하는 기본적인 접근방법은 용량이 충분히 큰 모델에 여러 가지 규제(regulanization)기법을 적용하는 것이다.

### 1. 데이터 확대

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2028.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2028.png)

일반화 능력을 향상하는 가장 확실한 방법은 데이터를 더 많이 수집하는 것이다.

그림 1-17은 12차 다항식 모델을 사용한 예인데, 훈련 집합의 크기가 학습에 미치는 영향을 잘 보여준다. 샘플이 20개인 그림 1-17(a)에서는 과잉 적합이 무척 심한데 40개로 늘어난 그림 1-17(b)에서는 상당히 줄었다. 샘플이 60개인 그림 1-17에서는 거의 완벽하며, 샘플을 꽤 정확하게 예측할 것이라 기대할 수 있다.

하지만 데이터 수집은 비용이 많이 든다. 예를 들어 가로축은 시간이고 세로축은 이동체의 위치라면, 위치 측정을 더 자주 해야 훈련 집합의 크기가 커진다.
필기 숫자는 영상 처리 프로그램이 분할한 문자 샘플마다 사람이 일일이 어떤 부류인지 레이블(label)을 붙여야만 한다.

데이터를 추가로 수집하기 어려운 상황에서는 주로 훈련집합에 있는 샘플을 변형함으로써 인위적으로 데이터를 확대하는 방법을 사용한다. 예를 들어, 숫자 영상을 정도를 달리하면서 약간 회전한다거나 와핑(waping)하여 하나의 샘플에서 수십 또는 수백 개의 변형 샘플을 만든다. 이동, 회전, 크기, 와핑, 잡음 추가 등을 적절히 조합하면 훈련집합을 충분히 크게 확대할 수 있다.

### 2. 가중치 감쇠

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2029.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2029.png)

그림 1-18(a)는 극점에서 곡률(curvature)이 매우 크다. 

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2030.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2030.png)

기계 학습이 찾은 이 곡선의 방정식인데, 잘 살펴보면 매개변수, 즉 계수의 값이 무척 크다는 사실을 알 수 있다. 즉, 값이 너무 커 정상을 벗어났다고 할 수 있다.

가중치 감쇠(weight decay)라는 규제 기법은 가중치를 작게 유지함으로써 일반화 능력ㅇ르 향상시킨다.

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2031.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2031.png)

첫 번째 항은 이전과 마찬가지로 오류를 줄이는 역할을 하는데, 최소점을 찾아가는 동력이 된다.

두 번째 항은 가중치의 크기를 나타내는데, 가중치가 클수록 큰 값을 가진다.

최적화 알고리즘은 두 항을 더한 값을 최소화하는 방향으로 학습을 진행하므로 결국 오류가 적으면서 다음과 같이 계수가 작은 해를 찾아준다.

## 7. 기계 학습 유형

### 1. 지도 방식에 따른 유형

예전에는 비지도 학습, 지도 학습으로 나누었지만, 최근에는 강화 학습이 중요해지면서,
비지도 학습, 지도 학습, 준지도 학습, 강화 학습으로 구분하는 추세이다.

### 2. 다양한 기준에 따른 유형

- 오프라인 학습과 온라인 학습
- 결정론적 학습과 스토캐스틱 학습
- 분별 모델 학습과 생성 모델 학습

이처럼 기계 학습을 여러 기준으로 구분하면 기계 학습의 개념과 특성을 이해하거나 주어진 문제에 가장 적합한 기계학습 알고리즘을 선택하는 데 큰 도움이 된다. 현재는 이들 사이의 경계가 불분명해지고 둘 이상의 서로 다른 기계 학습 알고리즘을 혼합하여 활용하는 사례가 더 많아 지고 있다.

## 8. 기계 학습의 과거와 현재, 미래

### 1. 인공지능과 기계 학습의 간략한 역사

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2032.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2032.png)

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2033.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2033.png)

### 2. 기술 추세

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2034.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2034.png)

기계 학습 알고리즘과 응용이 크게 다양해지고 있으며, 고전적으로 분류와 회귀에 집중하던 응용이 변환, 랭킹, 연관, 추천 등의 응용으로 확대되고 있다.

그림 1-19을 보면 알 수 있듯이 사진을 화가의 화풍으로 변환하는 응용도 있고,
사진 한 장을 비디오로 확장하는 방법도 발표되었다.

딥러닝에서는 은닉층 각각이 고유한 표현을 학습한다고 여긴다.
앞쪽에 있는 은닉층은 구체적인 특징을 추출하는 반면, 뒤쪽으로 갈수록 추상적인 특징을 추출한다.

### 3. 사회적 전망

현재의 기계 학습은 온통 수학과 컴퓨터 알고리즘일 뿐이다.

## 연습문제

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2035.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2035.png)

![%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2036.png](%E1%84%80%E1%85%B5%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%E1%84%89%E1%85%B3%E1%86%B8%20Chapter%201%20%E1%84%89%E1%85%A9%E1%84%80%E1%85%A2%201c00e54eccaa4c1ca49ed0edea657634/Untitled%2036.png)