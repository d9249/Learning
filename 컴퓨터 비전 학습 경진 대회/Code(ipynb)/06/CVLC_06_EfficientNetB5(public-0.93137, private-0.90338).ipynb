{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CVLC_06_EfficientNetB5(public-0.93137, private-0.90338).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN//Q6BZDPbvlPHk5BL0xS/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/d9249/DACON/blob/main/CVLC_06_EfficientNetB5(public-0.93137%2C%20private-0.90338).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmEaPJckuX-D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65693345-f401-48a8-8c47-3610aef523b4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88GAtllsufPj"
      },
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv('/content/drive/MyDrive/DACON_CVLC/data/train.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/DACON_CVLC/data/test.csv')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qBWziyZrqBo"
      },
      "source": [
        "!mkdir images_train\n",
        "!mkdir images_train/0\n",
        "!mkdir images_train/1\n",
        "!mkdir images_train/2\n",
        "!mkdir images_train/3\n",
        "!mkdir images_train/4\n",
        "!mkdir images_train/5\n",
        "!mkdir images_train/6\n",
        "!mkdir images_train/7\n",
        "!mkdir images_train/8\n",
        "!mkdir images_train/9\n",
        "!mkdir images_test"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fjN8mIDrazg"
      },
      "source": [
        "import cv2\n",
        "\n",
        "for idx in range(len(train)) :\n",
        "    img = train.loc[idx, '0':].values.reshape(28, 28).astype(int)\n",
        "    digit = train.loc[idx, 'digit']\n",
        "    cv2.imwrite(f'./images_train/{digit}/{train[\"id\"][idx]}.png', img)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4P9AD1gyotc"
      },
      "source": [
        "import cv2\n",
        "\n",
        "for idx in range(len(test)) :\n",
        "    img = test.loc[idx, '0':].values.reshape(28, 28).astype(int)\n",
        "    cv2.imwrite(f'./images_test/{test[\"id\"][idx]}.png', img)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUJTlJ6GxNmK"
      },
      "source": [
        "import tensorflow as tf\n",
        "EfficientNetB5_model =  tf.keras.applications.EfficientNetB5(weights=None, include_top=True, input_shape=(224, 224, 1), classes=10)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlVMd30ZxUMQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c03427a7-4c5a-4ece-d9c2-58b1187bc824"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "EfficientNetB5_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.002,epsilon=None), metrics=['accuracy'])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1haI0Zjxa74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c3727ca-db08-4882-bf19-5c13a9e41345"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2,\n",
        "                             rotation_range=10,\n",
        "                             width_shift_range=0.1,\n",
        "                             height_shift_range=0.1)\n",
        "\n",
        "train_generator = datagen.flow_from_directory('./images_train', target_size=(224,224), color_mode='grayscale', class_mode='categorical', subset='training')\n",
        "val_generator = datagen.flow_from_directory('./images_train', target_size=(224,224), color_mode='grayscale', class_mode='categorical', subset='validation')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1642 images belonging to 10 classes.\n",
            "Found 406 images belonging to 10 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRP2R9hdxsyY"
      },
      "source": [
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(f'/content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_EfficientNetB5.h5', monitor='val_accuracy', save_best_only=True, verbose=1)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtBpC5-0dZ9q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a271bc2-1a0d-46b1-e605-6a8e373deb1b"
      },
      "source": [
        "EfficientNetB5_model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"efficientnetb5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 224, 224, 1) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "rescaling (Rescaling)           (None, 224, 224, 1)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "normalization (Normalization)   (None, 224, 224, 1)  3           rescaling[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "stem_conv_pad (ZeroPadding2D)   (None, 225, 225, 1)  0           normalization[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "stem_conv (Conv2D)              (None, 112, 112, 48) 432         stem_conv_pad[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "stem_bn (BatchNormalization)    (None, 112, 112, 48) 192         stem_conv[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "stem_activation (Activation)    (None, 112, 112, 48) 0           stem_bn[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "block1a_dwconv (DepthwiseConv2D (None, 112, 112, 48) 432         stem_activation[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block1a_bn (BatchNormalization) (None, 112, 112, 48) 192         block1a_dwconv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block1a_activation (Activation) (None, 112, 112, 48) 0           block1a_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block1a_se_squeeze (GlobalAvera (None, 48)           0           block1a_activation[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block1a_se_reshape (Reshape)    (None, 1, 1, 48)     0           block1a_se_squeeze[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block1a_se_reduce (Conv2D)      (None, 1, 1, 12)     588         block1a_se_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block1a_se_expand (Conv2D)      (None, 1, 1, 48)     624         block1a_se_reduce[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block1a_se_excite (Multiply)    (None, 112, 112, 48) 0           block1a_activation[0][0]         \n",
            "                                                                 block1a_se_expand[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block1a_project_conv (Conv2D)   (None, 112, 112, 24) 1152        block1a_se_excite[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block1a_project_bn (BatchNormal (None, 112, 112, 24) 96          block1a_project_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block1b_dwconv (DepthwiseConv2D (None, 112, 112, 24) 216         block1a_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block1b_bn (BatchNormalization) (None, 112, 112, 24) 96          block1b_dwconv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block1b_activation (Activation) (None, 112, 112, 24) 0           block1b_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block1b_se_squeeze (GlobalAvera (None, 24)           0           block1b_activation[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block1b_se_reshape (Reshape)    (None, 1, 1, 24)     0           block1b_se_squeeze[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block1b_se_reduce (Conv2D)      (None, 1, 1, 6)      150         block1b_se_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block1b_se_expand (Conv2D)      (None, 1, 1, 24)     168         block1b_se_reduce[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block1b_se_excite (Multiply)    (None, 112, 112, 24) 0           block1b_activation[0][0]         \n",
            "                                                                 block1b_se_expand[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block1b_project_conv (Conv2D)   (None, 112, 112, 24) 576         block1b_se_excite[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block1b_project_bn (BatchNormal (None, 112, 112, 24) 96          block1b_project_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block1b_drop (Dropout)          (None, 112, 112, 24) 0           block1b_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block1b_add (Add)               (None, 112, 112, 24) 0           block1b_drop[0][0]               \n",
            "                                                                 block1a_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block1c_dwconv (DepthwiseConv2D (None, 112, 112, 24) 216         block1b_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block1c_bn (BatchNormalization) (None, 112, 112, 24) 96          block1c_dwconv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block1c_activation (Activation) (None, 112, 112, 24) 0           block1c_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block1c_se_squeeze (GlobalAvera (None, 24)           0           block1c_activation[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block1c_se_reshape (Reshape)    (None, 1, 1, 24)     0           block1c_se_squeeze[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block1c_se_reduce (Conv2D)      (None, 1, 1, 6)      150         block1c_se_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block1c_se_expand (Conv2D)      (None, 1, 1, 24)     168         block1c_se_reduce[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block1c_se_excite (Multiply)    (None, 112, 112, 24) 0           block1c_activation[0][0]         \n",
            "                                                                 block1c_se_expand[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block1c_project_conv (Conv2D)   (None, 112, 112, 24) 576         block1c_se_excite[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block1c_project_bn (BatchNormal (None, 112, 112, 24) 96          block1c_project_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block1c_drop (Dropout)          (None, 112, 112, 24) 0           block1c_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block1c_add (Add)               (None, 112, 112, 24) 0           block1c_drop[0][0]               \n",
            "                                                                 block1b_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block2a_expand_conv (Conv2D)    (None, 112, 112, 144 3456        block1c_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block2a_expand_bn (BatchNormali (None, 112, 112, 144 576         block2a_expand_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block2a_expand_activation (Acti (None, 112, 112, 144 0           block2a_expand_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block2a_dwconv_pad (ZeroPadding (None, 113, 113, 144 0           block2a_expand_activation[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "block2a_dwconv (DepthwiseConv2D (None, 56, 56, 144)  1296        block2a_dwconv_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block2a_bn (BatchNormalization) (None, 56, 56, 144)  576         block2a_dwconv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block2a_activation (Activation) (None, 56, 56, 144)  0           block2a_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block2a_se_squeeze (GlobalAvera (None, 144)          0           block2a_activation[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block2a_se_reshape (Reshape)    (None, 1, 1, 144)    0           block2a_se_squeeze[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block2a_se_reduce (Conv2D)      (None, 1, 1, 6)      870         block2a_se_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block2a_se_expand (Conv2D)      (None, 1, 1, 144)    1008        block2a_se_reduce[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block2a_se_excite (Multiply)    (None, 56, 56, 144)  0           block2a_activation[0][0]         \n",
            "                                                                 block2a_se_expand[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block2a_project_conv (Conv2D)   (None, 56, 56, 40)   5760        block2a_se_excite[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block2a_project_bn (BatchNormal (None, 56, 56, 40)   160         block2a_project_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block2b_expand_conv (Conv2D)    (None, 56, 56, 240)  9600        block2a_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block2b_expand_bn (BatchNormali (None, 56, 56, 240)  960         block2b_expand_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block2b_expand_activation (Acti (None, 56, 56, 240)  0           block2b_expand_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block2b_dwconv (DepthwiseConv2D (None, 56, 56, 240)  2160        block2b_expand_activation[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "block2b_bn (BatchNormalization) (None, 56, 56, 240)  960         block2b_dwconv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block2b_activation (Activation) (None, 56, 56, 240)  0           block2b_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block2b_se_squeeze (GlobalAvera (None, 240)          0           block2b_activation[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block2b_se_reshape (Reshape)    (None, 1, 1, 240)    0           block2b_se_squeeze[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block2b_se_reduce (Conv2D)      (None, 1, 1, 10)     2410        block2b_se_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block2b_se_expand (Conv2D)      (None, 1, 1, 240)    2640        block2b_se_reduce[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block2b_se_excite (Multiply)    (None, 56, 56, 240)  0           block2b_activation[0][0]         \n",
            "                                                                 block2b_se_expand[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block2b_project_conv (Conv2D)   (None, 56, 56, 40)   9600        block2b_se_excite[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block2b_project_bn (BatchNormal (None, 56, 56, 40)   160         block2b_project_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block2b_drop (Dropout)          (None, 56, 56, 40)   0           block2b_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block2b_add (Add)               (None, 56, 56, 40)   0           block2b_drop[0][0]               \n",
            "                                                                 block2a_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block2c_expand_conv (Conv2D)    (None, 56, 56, 240)  9600        block2b_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block2c_expand_bn (BatchNormali (None, 56, 56, 240)  960         block2c_expand_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block2c_expand_activation (Acti (None, 56, 56, 240)  0           block2c_expand_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block2c_dwconv (DepthwiseConv2D (None, 56, 56, 240)  2160        block2c_expand_activation[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "block2c_bn (BatchNormalization) (None, 56, 56, 240)  960         block2c_dwconv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block2c_activation (Activation) (None, 56, 56, 240)  0           block2c_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block2c_se_squeeze (GlobalAvera (None, 240)          0           block2c_activation[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block2c_se_reshape (Reshape)    (None, 1, 1, 240)    0           block2c_se_squeeze[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block2c_se_reduce (Conv2D)      (None, 1, 1, 10)     2410        block2c_se_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block2c_se_expand (Conv2D)      (None, 1, 1, 240)    2640        block2c_se_reduce[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block2c_se_excite (Multiply)    (None, 56, 56, 240)  0           block2c_activation[0][0]         \n",
            "                                                                 block2c_se_expand[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block2c_project_conv (Conv2D)   (None, 56, 56, 40)   9600        block2c_se_excite[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block2c_project_bn (BatchNormal (None, 56, 56, 40)   160         block2c_project_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block2c_drop (Dropout)          (None, 56, 56, 40)   0           block2c_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block2c_add (Add)               (None, 56, 56, 40)   0           block2c_drop[0][0]               \n",
            "                                                                 block2b_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block2d_expand_conv (Conv2D)    (None, 56, 56, 240)  9600        block2c_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block2d_expand_bn (BatchNormali (None, 56, 56, 240)  960         block2d_expand_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block2d_expand_activation (Acti (None, 56, 56, 240)  0           block2d_expand_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block2d_dwconv (DepthwiseConv2D (None, 56, 56, 240)  2160        block2d_expand_activation[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "block2d_bn (BatchNormalization) (None, 56, 56, 240)  960         block2d_dwconv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block2d_activation (Activation) (None, 56, 56, 240)  0           block2d_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block2d_se_squeeze (GlobalAvera (None, 240)          0           block2d_activation[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block2d_se_reshape (Reshape)    (None, 1, 1, 240)    0           block2d_se_squeeze[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block2d_se_reduce (Conv2D)      (None, 1, 1, 10)     2410        block2d_se_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block2d_se_expand (Conv2D)      (None, 1, 1, 240)    2640        block2d_se_reduce[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block2d_se_excite (Multiply)    (None, 56, 56, 240)  0           block2d_activation[0][0]         \n",
            "                                                                 block2d_se_expand[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block2d_project_conv (Conv2D)   (None, 56, 56, 40)   9600        block2d_se_excite[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block2d_project_bn (BatchNormal (None, 56, 56, 40)   160         block2d_project_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block2d_drop (Dropout)          (None, 56, 56, 40)   0           block2d_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block2d_add (Add)               (None, 56, 56, 40)   0           block2d_drop[0][0]               \n",
            "                                                                 block2c_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block2e_expand_conv (Conv2D)    (None, 56, 56, 240)  9600        block2d_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block2e_expand_bn (BatchNormali (None, 56, 56, 240)  960         block2e_expand_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block2e_expand_activation (Acti (None, 56, 56, 240)  0           block2e_expand_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block2e_dwconv (DepthwiseConv2D (None, 56, 56, 240)  2160        block2e_expand_activation[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "block2e_bn (BatchNormalization) (None, 56, 56, 240)  960         block2e_dwconv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block2e_activation (Activation) (None, 56, 56, 240)  0           block2e_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block2e_se_squeeze (GlobalAvera (None, 240)          0           block2e_activation[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block2e_se_reshape (Reshape)    (None, 1, 1, 240)    0           block2e_se_squeeze[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block2e_se_reduce (Conv2D)      (None, 1, 1, 10)     2410        block2e_se_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block2e_se_expand (Conv2D)      (None, 1, 1, 240)    2640        block2e_se_reduce[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block2e_se_excite (Multiply)    (None, 56, 56, 240)  0           block2e_activation[0][0]         \n",
            "                                                                 block2e_se_expand[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block2e_project_conv (Conv2D)   (None, 56, 56, 40)   9600        block2e_se_excite[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block2e_project_bn (BatchNormal (None, 56, 56, 40)   160         block2e_project_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block2e_drop (Dropout)          (None, 56, 56, 40)   0           block2e_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block2e_add (Add)               (None, 56, 56, 40)   0           block2e_drop[0][0]               \n",
            "                                                                 block2d_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block3a_expand_conv (Conv2D)    (None, 56, 56, 240)  9600        block2e_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block3a_expand_bn (BatchNormali (None, 56, 56, 240)  960         block3a_expand_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block3a_expand_activation (Acti (None, 56, 56, 240)  0           block3a_expand_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block3a_dwconv_pad (ZeroPadding (None, 59, 59, 240)  0           block3a_expand_activation[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "block3a_dwconv (DepthwiseConv2D (None, 28, 28, 240)  6000        block3a_dwconv_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block3a_bn (BatchNormalization) (None, 28, 28, 240)  960         block3a_dwconv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block3a_activation (Activation) (None, 28, 28, 240)  0           block3a_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block3a_se_squeeze (GlobalAvera (None, 240)          0           block3a_activation[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block3a_se_reshape (Reshape)    (None, 1, 1, 240)    0           block3a_se_squeeze[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block3a_se_reduce (Conv2D)      (None, 1, 1, 10)     2410        block3a_se_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block3a_se_expand (Conv2D)      (None, 1, 1, 240)    2640        block3a_se_reduce[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block3a_se_excite (Multiply)    (None, 28, 28, 240)  0           block3a_activation[0][0]         \n",
            "                                                                 block3a_se_expand[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block3a_project_conv (Conv2D)   (None, 28, 28, 64)   15360       block3a_se_excite[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block3a_project_bn (BatchNormal (None, 28, 28, 64)   256         block3a_project_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block3b_expand_conv (Conv2D)    (None, 28, 28, 384)  24576       block3a_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block3b_expand_bn (BatchNormali (None, 28, 28, 384)  1536        block3b_expand_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block3b_expand_activation (Acti (None, 28, 28, 384)  0           block3b_expand_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block3b_dwconv (DepthwiseConv2D (None, 28, 28, 384)  9600        block3b_expand_activation[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "block3b_bn (BatchNormalization) (None, 28, 28, 384)  1536        block3b_dwconv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block3b_activation (Activation) (None, 28, 28, 384)  0           block3b_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block3b_se_squeeze (GlobalAvera (None, 384)          0           block3b_activation[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block3b_se_reshape (Reshape)    (None, 1, 1, 384)    0           block3b_se_squeeze[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block3b_se_reduce (Conv2D)      (None, 1, 1, 16)     6160        block3b_se_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block3b_se_expand (Conv2D)      (None, 1, 1, 384)    6528        block3b_se_reduce[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block3b_se_excite (Multiply)    (None, 28, 28, 384)  0           block3b_activation[0][0]         \n",
            "                                                                 block3b_se_expand[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block3b_project_conv (Conv2D)   (None, 28, 28, 64)   24576       block3b_se_excite[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block3b_project_bn (BatchNormal (None, 28, 28, 64)   256         block3b_project_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block3b_drop (Dropout)          (None, 28, 28, 64)   0           block3b_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block3b_add (Add)               (None, 28, 28, 64)   0           block3b_drop[0][0]               \n",
            "                                                                 block3a_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block3c_expand_conv (Conv2D)    (None, 28, 28, 384)  24576       block3b_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block3c_expand_bn (BatchNormali (None, 28, 28, 384)  1536        block3c_expand_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block3c_expand_activation (Acti (None, 28, 28, 384)  0           block3c_expand_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block3c_dwconv (DepthwiseConv2D (None, 28, 28, 384)  9600        block3c_expand_activation[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "block3c_bn (BatchNormalization) (None, 28, 28, 384)  1536        block3c_dwconv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block3c_activation (Activation) (None, 28, 28, 384)  0           block3c_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block3c_se_squeeze (GlobalAvera (None, 384)          0           block3c_activation[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block3c_se_reshape (Reshape)    (None, 1, 1, 384)    0           block3c_se_squeeze[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block3c_se_reduce (Conv2D)      (None, 1, 1, 16)     6160        block3c_se_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block3c_se_expand (Conv2D)      (None, 1, 1, 384)    6528        block3c_se_reduce[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block3c_se_excite (Multiply)    (None, 28, 28, 384)  0           block3c_activation[0][0]         \n",
            "                                                                 block3c_se_expand[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block3c_project_conv (Conv2D)   (None, 28, 28, 64)   24576       block3c_se_excite[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block3c_project_bn (BatchNormal (None, 28, 28, 64)   256         block3c_project_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block3c_drop (Dropout)          (None, 28, 28, 64)   0           block3c_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block3c_add (Add)               (None, 28, 28, 64)   0           block3c_drop[0][0]               \n",
            "                                                                 block3b_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block3d_expand_conv (Conv2D)    (None, 28, 28, 384)  24576       block3c_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block3d_expand_bn (BatchNormali (None, 28, 28, 384)  1536        block3d_expand_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block3d_expand_activation (Acti (None, 28, 28, 384)  0           block3d_expand_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block3d_dwconv (DepthwiseConv2D (None, 28, 28, 384)  9600        block3d_expand_activation[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "block3d_bn (BatchNormalization) (None, 28, 28, 384)  1536        block3d_dwconv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block3d_activation (Activation) (None, 28, 28, 384)  0           block3d_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block3d_se_squeeze (GlobalAvera (None, 384)          0           block3d_activation[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block3d_se_reshape (Reshape)    (None, 1, 1, 384)    0           block3d_se_squeeze[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block3d_se_reduce (Conv2D)      (None, 1, 1, 16)     6160        block3d_se_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block3d_se_expand (Conv2D)      (None, 1, 1, 384)    6528        block3d_se_reduce[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block3d_se_excite (Multiply)    (None, 28, 28, 384)  0           block3d_activation[0][0]         \n",
            "                                                                 block3d_se_expand[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block3d_project_conv (Conv2D)   (None, 28, 28, 64)   24576       block3d_se_excite[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block3d_project_bn (BatchNormal (None, 28, 28, 64)   256         block3d_project_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block3d_drop (Dropout)          (None, 28, 28, 64)   0           block3d_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block3d_add (Add)               (None, 28, 28, 64)   0           block3d_drop[0][0]               \n",
            "                                                                 block3c_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block3e_expand_conv (Conv2D)    (None, 28, 28, 384)  24576       block3d_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block3e_expand_bn (BatchNormali (None, 28, 28, 384)  1536        block3e_expand_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block3e_expand_activation (Acti (None, 28, 28, 384)  0           block3e_expand_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block3e_dwconv (DepthwiseConv2D (None, 28, 28, 384)  9600        block3e_expand_activation[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "block3e_bn (BatchNormalization) (None, 28, 28, 384)  1536        block3e_dwconv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block3e_activation (Activation) (None, 28, 28, 384)  0           block3e_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block3e_se_squeeze (GlobalAvera (None, 384)          0           block3e_activation[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block3e_se_reshape (Reshape)    (None, 1, 1, 384)    0           block3e_se_squeeze[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block3e_se_reduce (Conv2D)      (None, 1, 1, 16)     6160        block3e_se_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block3e_se_expand (Conv2D)      (None, 1, 1, 384)    6528        block3e_se_reduce[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block3e_se_excite (Multiply)    (None, 28, 28, 384)  0           block3e_activation[0][0]         \n",
            "                                                                 block3e_se_expand[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block3e_project_conv (Conv2D)   (None, 28, 28, 64)   24576       block3e_se_excite[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block3e_project_bn (BatchNormal (None, 28, 28, 64)   256         block3e_project_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block3e_drop (Dropout)          (None, 28, 28, 64)   0           block3e_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block3e_add (Add)               (None, 28, 28, 64)   0           block3e_drop[0][0]               \n",
            "                                                                 block3d_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block4a_expand_conv (Conv2D)    (None, 28, 28, 384)  24576       block3e_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block4a_expand_bn (BatchNormali (None, 28, 28, 384)  1536        block4a_expand_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block4a_expand_activation (Acti (None, 28, 28, 384)  0           block4a_expand_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block4a_dwconv_pad (ZeroPadding (None, 29, 29, 384)  0           block4a_expand_activation[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "block4a_dwconv (DepthwiseConv2D (None, 14, 14, 384)  3456        block4a_dwconv_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4a_bn (BatchNormalization) (None, 14, 14, 384)  1536        block4a_dwconv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block4a_activation (Activation) (None, 14, 14, 384)  0           block4a_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block4a_se_squeeze (GlobalAvera (None, 384)          0           block4a_activation[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4a_se_reshape (Reshape)    (None, 1, 1, 384)    0           block4a_se_squeeze[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4a_se_reduce (Conv2D)      (None, 1, 1, 16)     6160        block4a_se_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4a_se_expand (Conv2D)      (None, 1, 1, 384)    6528        block4a_se_reduce[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block4a_se_excite (Multiply)    (None, 14, 14, 384)  0           block4a_activation[0][0]         \n",
            "                                                                 block4a_se_expand[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block4a_project_conv (Conv2D)   (None, 14, 14, 128)  49152       block4a_se_excite[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block4a_project_bn (BatchNormal (None, 14, 14, 128)  512         block4a_project_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block4b_expand_conv (Conv2D)    (None, 14, 14, 768)  98304       block4a_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4b_expand_bn (BatchNormali (None, 14, 14, 768)  3072        block4b_expand_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block4b_expand_activation (Acti (None, 14, 14, 768)  0           block4b_expand_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block4b_dwconv (DepthwiseConv2D (None, 14, 14, 768)  6912        block4b_expand_activation[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "block4b_bn (BatchNormalization) (None, 14, 14, 768)  3072        block4b_dwconv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block4b_activation (Activation) (None, 14, 14, 768)  0           block4b_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block4b_se_squeeze (GlobalAvera (None, 768)          0           block4b_activation[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4b_se_reshape (Reshape)    (None, 1, 1, 768)    0           block4b_se_squeeze[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4b_se_reduce (Conv2D)      (None, 1, 1, 32)     24608       block4b_se_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4b_se_expand (Conv2D)      (None, 1, 1, 768)    25344       block4b_se_reduce[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block4b_se_excite (Multiply)    (None, 14, 14, 768)  0           block4b_activation[0][0]         \n",
            "                                                                 block4b_se_expand[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block4b_project_conv (Conv2D)   (None, 14, 14, 128)  98304       block4b_se_excite[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block4b_project_bn (BatchNormal (None, 14, 14, 128)  512         block4b_project_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block4b_drop (Dropout)          (None, 14, 14, 128)  0           block4b_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4b_add (Add)               (None, 14, 14, 128)  0           block4b_drop[0][0]               \n",
            "                                                                 block4a_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4c_expand_conv (Conv2D)    (None, 14, 14, 768)  98304       block4b_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block4c_expand_bn (BatchNormali (None, 14, 14, 768)  3072        block4c_expand_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block4c_expand_activation (Acti (None, 14, 14, 768)  0           block4c_expand_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block4c_dwconv (DepthwiseConv2D (None, 14, 14, 768)  6912        block4c_expand_activation[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "block4c_bn (BatchNormalization) (None, 14, 14, 768)  3072        block4c_dwconv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block4c_activation (Activation) (None, 14, 14, 768)  0           block4c_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block4c_se_squeeze (GlobalAvera (None, 768)          0           block4c_activation[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4c_se_reshape (Reshape)    (None, 1, 1, 768)    0           block4c_se_squeeze[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4c_se_reduce (Conv2D)      (None, 1, 1, 32)     24608       block4c_se_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4c_se_expand (Conv2D)      (None, 1, 1, 768)    25344       block4c_se_reduce[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block4c_se_excite (Multiply)    (None, 14, 14, 768)  0           block4c_activation[0][0]         \n",
            "                                                                 block4c_se_expand[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block4c_project_conv (Conv2D)   (None, 14, 14, 128)  98304       block4c_se_excite[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block4c_project_bn (BatchNormal (None, 14, 14, 128)  512         block4c_project_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block4c_drop (Dropout)          (None, 14, 14, 128)  0           block4c_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4c_add (Add)               (None, 14, 14, 128)  0           block4c_drop[0][0]               \n",
            "                                                                 block4b_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block4d_expand_conv (Conv2D)    (None, 14, 14, 768)  98304       block4c_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block4d_expand_bn (BatchNormali (None, 14, 14, 768)  3072        block4d_expand_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block4d_expand_activation (Acti (None, 14, 14, 768)  0           block4d_expand_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block4d_dwconv (DepthwiseConv2D (None, 14, 14, 768)  6912        block4d_expand_activation[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "block4d_bn (BatchNormalization) (None, 14, 14, 768)  3072        block4d_dwconv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block4d_activation (Activation) (None, 14, 14, 768)  0           block4d_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block4d_se_squeeze (GlobalAvera (None, 768)          0           block4d_activation[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4d_se_reshape (Reshape)    (None, 1, 1, 768)    0           block4d_se_squeeze[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4d_se_reduce (Conv2D)      (None, 1, 1, 32)     24608       block4d_se_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4d_se_expand (Conv2D)      (None, 1, 1, 768)    25344       block4d_se_reduce[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block4d_se_excite (Multiply)    (None, 14, 14, 768)  0           block4d_activation[0][0]         \n",
            "                                                                 block4d_se_expand[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block4d_project_conv (Conv2D)   (None, 14, 14, 128)  98304       block4d_se_excite[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block4d_project_bn (BatchNormal (None, 14, 14, 128)  512         block4d_project_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block4d_drop (Dropout)          (None, 14, 14, 128)  0           block4d_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4d_add (Add)               (None, 14, 14, 128)  0           block4d_drop[0][0]               \n",
            "                                                                 block4c_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block4e_expand_conv (Conv2D)    (None, 14, 14, 768)  98304       block4d_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block4e_expand_bn (BatchNormali (None, 14, 14, 768)  3072        block4e_expand_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block4e_expand_activation (Acti (None, 14, 14, 768)  0           block4e_expand_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block4e_dwconv (DepthwiseConv2D (None, 14, 14, 768)  6912        block4e_expand_activation[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "block4e_bn (BatchNormalization) (None, 14, 14, 768)  3072        block4e_dwconv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block4e_activation (Activation) (None, 14, 14, 768)  0           block4e_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block4e_se_squeeze (GlobalAvera (None, 768)          0           block4e_activation[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4e_se_reshape (Reshape)    (None, 1, 1, 768)    0           block4e_se_squeeze[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4e_se_reduce (Conv2D)      (None, 1, 1, 32)     24608       block4e_se_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4e_se_expand (Conv2D)      (None, 1, 1, 768)    25344       block4e_se_reduce[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block4e_se_excite (Multiply)    (None, 14, 14, 768)  0           block4e_activation[0][0]         \n",
            "                                                                 block4e_se_expand[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block4e_project_conv (Conv2D)   (None, 14, 14, 128)  98304       block4e_se_excite[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block4e_project_bn (BatchNormal (None, 14, 14, 128)  512         block4e_project_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block4e_drop (Dropout)          (None, 14, 14, 128)  0           block4e_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4e_add (Add)               (None, 14, 14, 128)  0           block4e_drop[0][0]               \n",
            "                                                                 block4d_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block4f_expand_conv (Conv2D)    (None, 14, 14, 768)  98304       block4e_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block4f_expand_bn (BatchNormali (None, 14, 14, 768)  3072        block4f_expand_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block4f_expand_activation (Acti (None, 14, 14, 768)  0           block4f_expand_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block4f_dwconv (DepthwiseConv2D (None, 14, 14, 768)  6912        block4f_expand_activation[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "block4f_bn (BatchNormalization) (None, 14, 14, 768)  3072        block4f_dwconv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block4f_activation (Activation) (None, 14, 14, 768)  0           block4f_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block4f_se_squeeze (GlobalAvera (None, 768)          0           block4f_activation[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4f_se_reshape (Reshape)    (None, 1, 1, 768)    0           block4f_se_squeeze[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4f_se_reduce (Conv2D)      (None, 1, 1, 32)     24608       block4f_se_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4f_se_expand (Conv2D)      (None, 1, 1, 768)    25344       block4f_se_reduce[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block4f_se_excite (Multiply)    (None, 14, 14, 768)  0           block4f_activation[0][0]         \n",
            "                                                                 block4f_se_expand[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block4f_project_conv (Conv2D)   (None, 14, 14, 128)  98304       block4f_se_excite[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block4f_project_bn (BatchNormal (None, 14, 14, 128)  512         block4f_project_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block4f_drop (Dropout)          (None, 14, 14, 128)  0           block4f_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4f_add (Add)               (None, 14, 14, 128)  0           block4f_drop[0][0]               \n",
            "                                                                 block4e_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block4g_expand_conv (Conv2D)    (None, 14, 14, 768)  98304       block4f_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block4g_expand_bn (BatchNormali (None, 14, 14, 768)  3072        block4g_expand_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block4g_expand_activation (Acti (None, 14, 14, 768)  0           block4g_expand_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block4g_dwconv (DepthwiseConv2D (None, 14, 14, 768)  6912        block4g_expand_activation[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "block4g_bn (BatchNormalization) (None, 14, 14, 768)  3072        block4g_dwconv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block4g_activation (Activation) (None, 14, 14, 768)  0           block4g_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block4g_se_squeeze (GlobalAvera (None, 768)          0           block4g_activation[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4g_se_reshape (Reshape)    (None, 1, 1, 768)    0           block4g_se_squeeze[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4g_se_reduce (Conv2D)      (None, 1, 1, 32)     24608       block4g_se_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4g_se_expand (Conv2D)      (None, 1, 1, 768)    25344       block4g_se_reduce[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block4g_se_excite (Multiply)    (None, 14, 14, 768)  0           block4g_activation[0][0]         \n",
            "                                                                 block4g_se_expand[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block4g_project_conv (Conv2D)   (None, 14, 14, 128)  98304       block4g_se_excite[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block4g_project_bn (BatchNormal (None, 14, 14, 128)  512         block4g_project_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block4g_drop (Dropout)          (None, 14, 14, 128)  0           block4g_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4g_add (Add)               (None, 14, 14, 128)  0           block4g_drop[0][0]               \n",
            "                                                                 block4f_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block5a_expand_conv (Conv2D)    (None, 14, 14, 768)  98304       block4g_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block5a_expand_bn (BatchNormali (None, 14, 14, 768)  3072        block5a_expand_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5a_expand_activation (Acti (None, 14, 14, 768)  0           block5a_expand_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block5a_dwconv (DepthwiseConv2D (None, 14, 14, 768)  19200       block5a_expand_activation[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "block5a_bn (BatchNormalization) (None, 14, 14, 768)  3072        block5a_dwconv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block5a_activation (Activation) (None, 14, 14, 768)  0           block5a_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block5a_se_squeeze (GlobalAvera (None, 768)          0           block5a_activation[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5a_se_reshape (Reshape)    (None, 1, 1, 768)    0           block5a_se_squeeze[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5a_se_reduce (Conv2D)      (None, 1, 1, 32)     24608       block5a_se_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5a_se_expand (Conv2D)      (None, 1, 1, 768)    25344       block5a_se_reduce[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block5a_se_excite (Multiply)    (None, 14, 14, 768)  0           block5a_activation[0][0]         \n",
            "                                                                 block5a_se_expand[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block5a_project_conv (Conv2D)   (None, 14, 14, 176)  135168      block5a_se_excite[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block5a_project_bn (BatchNormal (None, 14, 14, 176)  704         block5a_project_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block5b_expand_conv (Conv2D)    (None, 14, 14, 1056) 185856      block5a_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5b_expand_bn (BatchNormali (None, 14, 14, 1056) 4224        block5b_expand_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5b_expand_activation (Acti (None, 14, 14, 1056) 0           block5b_expand_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block5b_dwconv (DepthwiseConv2D (None, 14, 14, 1056) 26400       block5b_expand_activation[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "block5b_bn (BatchNormalization) (None, 14, 14, 1056) 4224        block5b_dwconv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block5b_activation (Activation) (None, 14, 14, 1056) 0           block5b_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block5b_se_squeeze (GlobalAvera (None, 1056)         0           block5b_activation[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5b_se_reshape (Reshape)    (None, 1, 1, 1056)   0           block5b_se_squeeze[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5b_se_reduce (Conv2D)      (None, 1, 1, 44)     46508       block5b_se_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5b_se_expand (Conv2D)      (None, 1, 1, 1056)   47520       block5b_se_reduce[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block5b_se_excite (Multiply)    (None, 14, 14, 1056) 0           block5b_activation[0][0]         \n",
            "                                                                 block5b_se_expand[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block5b_project_conv (Conv2D)   (None, 14, 14, 176)  185856      block5b_se_excite[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block5b_project_bn (BatchNormal (None, 14, 14, 176)  704         block5b_project_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block5b_drop (Dropout)          (None, 14, 14, 176)  0           block5b_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5b_add (Add)               (None, 14, 14, 176)  0           block5b_drop[0][0]               \n",
            "                                                                 block5a_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5c_expand_conv (Conv2D)    (None, 14, 14, 1056) 185856      block5b_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block5c_expand_bn (BatchNormali (None, 14, 14, 1056) 4224        block5c_expand_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5c_expand_activation (Acti (None, 14, 14, 1056) 0           block5c_expand_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block5c_dwconv (DepthwiseConv2D (None, 14, 14, 1056) 26400       block5c_expand_activation[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "block5c_bn (BatchNormalization) (None, 14, 14, 1056) 4224        block5c_dwconv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block5c_activation (Activation) (None, 14, 14, 1056) 0           block5c_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block5c_se_squeeze (GlobalAvera (None, 1056)         0           block5c_activation[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5c_se_reshape (Reshape)    (None, 1, 1, 1056)   0           block5c_se_squeeze[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5c_se_reduce (Conv2D)      (None, 1, 1, 44)     46508       block5c_se_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5c_se_expand (Conv2D)      (None, 1, 1, 1056)   47520       block5c_se_reduce[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block5c_se_excite (Multiply)    (None, 14, 14, 1056) 0           block5c_activation[0][0]         \n",
            "                                                                 block5c_se_expand[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block5c_project_conv (Conv2D)   (None, 14, 14, 176)  185856      block5c_se_excite[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block5c_project_bn (BatchNormal (None, 14, 14, 176)  704         block5c_project_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block5c_drop (Dropout)          (None, 14, 14, 176)  0           block5c_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5c_add (Add)               (None, 14, 14, 176)  0           block5c_drop[0][0]               \n",
            "                                                                 block5b_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block5d_expand_conv (Conv2D)    (None, 14, 14, 1056) 185856      block5c_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block5d_expand_bn (BatchNormali (None, 14, 14, 1056) 4224        block5d_expand_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5d_expand_activation (Acti (None, 14, 14, 1056) 0           block5d_expand_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block5d_dwconv (DepthwiseConv2D (None, 14, 14, 1056) 26400       block5d_expand_activation[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "block5d_bn (BatchNormalization) (None, 14, 14, 1056) 4224        block5d_dwconv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block5d_activation (Activation) (None, 14, 14, 1056) 0           block5d_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block5d_se_squeeze (GlobalAvera (None, 1056)         0           block5d_activation[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5d_se_reshape (Reshape)    (None, 1, 1, 1056)   0           block5d_se_squeeze[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5d_se_reduce (Conv2D)      (None, 1, 1, 44)     46508       block5d_se_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5d_se_expand (Conv2D)      (None, 1, 1, 1056)   47520       block5d_se_reduce[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block5d_se_excite (Multiply)    (None, 14, 14, 1056) 0           block5d_activation[0][0]         \n",
            "                                                                 block5d_se_expand[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block5d_project_conv (Conv2D)   (None, 14, 14, 176)  185856      block5d_se_excite[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block5d_project_bn (BatchNormal (None, 14, 14, 176)  704         block5d_project_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block5d_drop (Dropout)          (None, 14, 14, 176)  0           block5d_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5d_add (Add)               (None, 14, 14, 176)  0           block5d_drop[0][0]               \n",
            "                                                                 block5c_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block5e_expand_conv (Conv2D)    (None, 14, 14, 1056) 185856      block5d_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block5e_expand_bn (BatchNormali (None, 14, 14, 1056) 4224        block5e_expand_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5e_expand_activation (Acti (None, 14, 14, 1056) 0           block5e_expand_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block5e_dwconv (DepthwiseConv2D (None, 14, 14, 1056) 26400       block5e_expand_activation[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "block5e_bn (BatchNormalization) (None, 14, 14, 1056) 4224        block5e_dwconv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block5e_activation (Activation) (None, 14, 14, 1056) 0           block5e_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block5e_se_squeeze (GlobalAvera (None, 1056)         0           block5e_activation[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5e_se_reshape (Reshape)    (None, 1, 1, 1056)   0           block5e_se_squeeze[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5e_se_reduce (Conv2D)      (None, 1, 1, 44)     46508       block5e_se_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5e_se_expand (Conv2D)      (None, 1, 1, 1056)   47520       block5e_se_reduce[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block5e_se_excite (Multiply)    (None, 14, 14, 1056) 0           block5e_activation[0][0]         \n",
            "                                                                 block5e_se_expand[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block5e_project_conv (Conv2D)   (None, 14, 14, 176)  185856      block5e_se_excite[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block5e_project_bn (BatchNormal (None, 14, 14, 176)  704         block5e_project_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block5e_drop (Dropout)          (None, 14, 14, 176)  0           block5e_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5e_add (Add)               (None, 14, 14, 176)  0           block5e_drop[0][0]               \n",
            "                                                                 block5d_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block5f_expand_conv (Conv2D)    (None, 14, 14, 1056) 185856      block5e_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block5f_expand_bn (BatchNormali (None, 14, 14, 1056) 4224        block5f_expand_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5f_expand_activation (Acti (None, 14, 14, 1056) 0           block5f_expand_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block5f_dwconv (DepthwiseConv2D (None, 14, 14, 1056) 26400       block5f_expand_activation[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "block5f_bn (BatchNormalization) (None, 14, 14, 1056) 4224        block5f_dwconv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block5f_activation (Activation) (None, 14, 14, 1056) 0           block5f_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block5f_se_squeeze (GlobalAvera (None, 1056)         0           block5f_activation[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5f_se_reshape (Reshape)    (None, 1, 1, 1056)   0           block5f_se_squeeze[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5f_se_reduce (Conv2D)      (None, 1, 1, 44)     46508       block5f_se_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5f_se_expand (Conv2D)      (None, 1, 1, 1056)   47520       block5f_se_reduce[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block5f_se_excite (Multiply)    (None, 14, 14, 1056) 0           block5f_activation[0][0]         \n",
            "                                                                 block5f_se_expand[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block5f_project_conv (Conv2D)   (None, 14, 14, 176)  185856      block5f_se_excite[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block5f_project_bn (BatchNormal (None, 14, 14, 176)  704         block5f_project_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block5f_drop (Dropout)          (None, 14, 14, 176)  0           block5f_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5f_add (Add)               (None, 14, 14, 176)  0           block5f_drop[0][0]               \n",
            "                                                                 block5e_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block5g_expand_conv (Conv2D)    (None, 14, 14, 1056) 185856      block5f_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block5g_expand_bn (BatchNormali (None, 14, 14, 1056) 4224        block5g_expand_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5g_expand_activation (Acti (None, 14, 14, 1056) 0           block5g_expand_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block5g_dwconv (DepthwiseConv2D (None, 14, 14, 1056) 26400       block5g_expand_activation[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "block5g_bn (BatchNormalization) (None, 14, 14, 1056) 4224        block5g_dwconv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block5g_activation (Activation) (None, 14, 14, 1056) 0           block5g_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block5g_se_squeeze (GlobalAvera (None, 1056)         0           block5g_activation[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5g_se_reshape (Reshape)    (None, 1, 1, 1056)   0           block5g_se_squeeze[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5g_se_reduce (Conv2D)      (None, 1, 1, 44)     46508       block5g_se_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5g_se_expand (Conv2D)      (None, 1, 1, 1056)   47520       block5g_se_reduce[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block5g_se_excite (Multiply)    (None, 14, 14, 1056) 0           block5g_activation[0][0]         \n",
            "                                                                 block5g_se_expand[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block5g_project_conv (Conv2D)   (None, 14, 14, 176)  185856      block5g_se_excite[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block5g_project_bn (BatchNormal (None, 14, 14, 176)  704         block5g_project_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block5g_drop (Dropout)          (None, 14, 14, 176)  0           block5g_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5g_add (Add)               (None, 14, 14, 176)  0           block5g_drop[0][0]               \n",
            "                                                                 block5f_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block6a_expand_conv (Conv2D)    (None, 14, 14, 1056) 185856      block5g_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block6a_expand_bn (BatchNormali (None, 14, 14, 1056) 4224        block6a_expand_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6a_expand_activation (Acti (None, 14, 14, 1056) 0           block6a_expand_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block6a_dwconv_pad (ZeroPadding (None, 17, 17, 1056) 0           block6a_expand_activation[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "block6a_dwconv (DepthwiseConv2D (None, 7, 7, 1056)   26400       block6a_dwconv_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6a_bn (BatchNormalization) (None, 7, 7, 1056)   4224        block6a_dwconv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block6a_activation (Activation) (None, 7, 7, 1056)   0           block6a_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block6a_se_squeeze (GlobalAvera (None, 1056)         0           block6a_activation[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6a_se_reshape (Reshape)    (None, 1, 1, 1056)   0           block6a_se_squeeze[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6a_se_reduce (Conv2D)      (None, 1, 1, 44)     46508       block6a_se_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6a_se_expand (Conv2D)      (None, 1, 1, 1056)   47520       block6a_se_reduce[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block6a_se_excite (Multiply)    (None, 7, 7, 1056)   0           block6a_activation[0][0]         \n",
            "                                                                 block6a_se_expand[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block6a_project_conv (Conv2D)   (None, 7, 7, 304)    321024      block6a_se_excite[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block6a_project_bn (BatchNormal (None, 7, 7, 304)    1216        block6a_project_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block6b_expand_conv (Conv2D)    (None, 7, 7, 1824)   554496      block6a_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6b_expand_bn (BatchNormali (None, 7, 7, 1824)   7296        block6b_expand_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6b_expand_activation (Acti (None, 7, 7, 1824)   0           block6b_expand_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block6b_dwconv (DepthwiseConv2D (None, 7, 7, 1824)   45600       block6b_expand_activation[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "block6b_bn (BatchNormalization) (None, 7, 7, 1824)   7296        block6b_dwconv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block6b_activation (Activation) (None, 7, 7, 1824)   0           block6b_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block6b_se_squeeze (GlobalAvera (None, 1824)         0           block6b_activation[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6b_se_reshape (Reshape)    (None, 1, 1, 1824)   0           block6b_se_squeeze[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6b_se_reduce (Conv2D)      (None, 1, 1, 76)     138700      block6b_se_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6b_se_expand (Conv2D)      (None, 1, 1, 1824)   140448      block6b_se_reduce[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block6b_se_excite (Multiply)    (None, 7, 7, 1824)   0           block6b_activation[0][0]         \n",
            "                                                                 block6b_se_expand[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block6b_project_conv (Conv2D)   (None, 7, 7, 304)    554496      block6b_se_excite[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block6b_project_bn (BatchNormal (None, 7, 7, 304)    1216        block6b_project_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block6b_drop (Dropout)          (None, 7, 7, 304)    0           block6b_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6b_add (Add)               (None, 7, 7, 304)    0           block6b_drop[0][0]               \n",
            "                                                                 block6a_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6c_expand_conv (Conv2D)    (None, 7, 7, 1824)   554496      block6b_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block6c_expand_bn (BatchNormali (None, 7, 7, 1824)   7296        block6c_expand_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6c_expand_activation (Acti (None, 7, 7, 1824)   0           block6c_expand_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block6c_dwconv (DepthwiseConv2D (None, 7, 7, 1824)   45600       block6c_expand_activation[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "block6c_bn (BatchNormalization) (None, 7, 7, 1824)   7296        block6c_dwconv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block6c_activation (Activation) (None, 7, 7, 1824)   0           block6c_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block6c_se_squeeze (GlobalAvera (None, 1824)         0           block6c_activation[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6c_se_reshape (Reshape)    (None, 1, 1, 1824)   0           block6c_se_squeeze[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6c_se_reduce (Conv2D)      (None, 1, 1, 76)     138700      block6c_se_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6c_se_expand (Conv2D)      (None, 1, 1, 1824)   140448      block6c_se_reduce[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block6c_se_excite (Multiply)    (None, 7, 7, 1824)   0           block6c_activation[0][0]         \n",
            "                                                                 block6c_se_expand[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block6c_project_conv (Conv2D)   (None, 7, 7, 304)    554496      block6c_se_excite[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block6c_project_bn (BatchNormal (None, 7, 7, 304)    1216        block6c_project_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block6c_drop (Dropout)          (None, 7, 7, 304)    0           block6c_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6c_add (Add)               (None, 7, 7, 304)    0           block6c_drop[0][0]               \n",
            "                                                                 block6b_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block6d_expand_conv (Conv2D)    (None, 7, 7, 1824)   554496      block6c_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block6d_expand_bn (BatchNormali (None, 7, 7, 1824)   7296        block6d_expand_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6d_expand_activation (Acti (None, 7, 7, 1824)   0           block6d_expand_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block6d_dwconv (DepthwiseConv2D (None, 7, 7, 1824)   45600       block6d_expand_activation[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "block6d_bn (BatchNormalization) (None, 7, 7, 1824)   7296        block6d_dwconv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block6d_activation (Activation) (None, 7, 7, 1824)   0           block6d_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block6d_se_squeeze (GlobalAvera (None, 1824)         0           block6d_activation[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6d_se_reshape (Reshape)    (None, 1, 1, 1824)   0           block6d_se_squeeze[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6d_se_reduce (Conv2D)      (None, 1, 1, 76)     138700      block6d_se_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6d_se_expand (Conv2D)      (None, 1, 1, 1824)   140448      block6d_se_reduce[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block6d_se_excite (Multiply)    (None, 7, 7, 1824)   0           block6d_activation[0][0]         \n",
            "                                                                 block6d_se_expand[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block6d_project_conv (Conv2D)   (None, 7, 7, 304)    554496      block6d_se_excite[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block6d_project_bn (BatchNormal (None, 7, 7, 304)    1216        block6d_project_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block6d_drop (Dropout)          (None, 7, 7, 304)    0           block6d_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6d_add (Add)               (None, 7, 7, 304)    0           block6d_drop[0][0]               \n",
            "                                                                 block6c_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block6e_expand_conv (Conv2D)    (None, 7, 7, 1824)   554496      block6d_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block6e_expand_bn (BatchNormali (None, 7, 7, 1824)   7296        block6e_expand_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6e_expand_activation (Acti (None, 7, 7, 1824)   0           block6e_expand_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block6e_dwconv (DepthwiseConv2D (None, 7, 7, 1824)   45600       block6e_expand_activation[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "block6e_bn (BatchNormalization) (None, 7, 7, 1824)   7296        block6e_dwconv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block6e_activation (Activation) (None, 7, 7, 1824)   0           block6e_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block6e_se_squeeze (GlobalAvera (None, 1824)         0           block6e_activation[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6e_se_reshape (Reshape)    (None, 1, 1, 1824)   0           block6e_se_squeeze[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6e_se_reduce (Conv2D)      (None, 1, 1, 76)     138700      block6e_se_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6e_se_expand (Conv2D)      (None, 1, 1, 1824)   140448      block6e_se_reduce[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block6e_se_excite (Multiply)    (None, 7, 7, 1824)   0           block6e_activation[0][0]         \n",
            "                                                                 block6e_se_expand[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block6e_project_conv (Conv2D)   (None, 7, 7, 304)    554496      block6e_se_excite[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block6e_project_bn (BatchNormal (None, 7, 7, 304)    1216        block6e_project_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block6e_drop (Dropout)          (None, 7, 7, 304)    0           block6e_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6e_add (Add)               (None, 7, 7, 304)    0           block6e_drop[0][0]               \n",
            "                                                                 block6d_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block6f_expand_conv (Conv2D)    (None, 7, 7, 1824)   554496      block6e_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block6f_expand_bn (BatchNormali (None, 7, 7, 1824)   7296        block6f_expand_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6f_expand_activation (Acti (None, 7, 7, 1824)   0           block6f_expand_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block6f_dwconv (DepthwiseConv2D (None, 7, 7, 1824)   45600       block6f_expand_activation[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "block6f_bn (BatchNormalization) (None, 7, 7, 1824)   7296        block6f_dwconv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block6f_activation (Activation) (None, 7, 7, 1824)   0           block6f_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block6f_se_squeeze (GlobalAvera (None, 1824)         0           block6f_activation[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6f_se_reshape (Reshape)    (None, 1, 1, 1824)   0           block6f_se_squeeze[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6f_se_reduce (Conv2D)      (None, 1, 1, 76)     138700      block6f_se_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6f_se_expand (Conv2D)      (None, 1, 1, 1824)   140448      block6f_se_reduce[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block6f_se_excite (Multiply)    (None, 7, 7, 1824)   0           block6f_activation[0][0]         \n",
            "                                                                 block6f_se_expand[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block6f_project_conv (Conv2D)   (None, 7, 7, 304)    554496      block6f_se_excite[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block6f_project_bn (BatchNormal (None, 7, 7, 304)    1216        block6f_project_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block6f_drop (Dropout)          (None, 7, 7, 304)    0           block6f_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6f_add (Add)               (None, 7, 7, 304)    0           block6f_drop[0][0]               \n",
            "                                                                 block6e_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block6g_expand_conv (Conv2D)    (None, 7, 7, 1824)   554496      block6f_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block6g_expand_bn (BatchNormali (None, 7, 7, 1824)   7296        block6g_expand_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6g_expand_activation (Acti (None, 7, 7, 1824)   0           block6g_expand_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block6g_dwconv (DepthwiseConv2D (None, 7, 7, 1824)   45600       block6g_expand_activation[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "block6g_bn (BatchNormalization) (None, 7, 7, 1824)   7296        block6g_dwconv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block6g_activation (Activation) (None, 7, 7, 1824)   0           block6g_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block6g_se_squeeze (GlobalAvera (None, 1824)         0           block6g_activation[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6g_se_reshape (Reshape)    (None, 1, 1, 1824)   0           block6g_se_squeeze[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6g_se_reduce (Conv2D)      (None, 1, 1, 76)     138700      block6g_se_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6g_se_expand (Conv2D)      (None, 1, 1, 1824)   140448      block6g_se_reduce[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block6g_se_excite (Multiply)    (None, 7, 7, 1824)   0           block6g_activation[0][0]         \n",
            "                                                                 block6g_se_expand[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block6g_project_conv (Conv2D)   (None, 7, 7, 304)    554496      block6g_se_excite[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block6g_project_bn (BatchNormal (None, 7, 7, 304)    1216        block6g_project_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block6g_drop (Dropout)          (None, 7, 7, 304)    0           block6g_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6g_add (Add)               (None, 7, 7, 304)    0           block6g_drop[0][0]               \n",
            "                                                                 block6f_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block6h_expand_conv (Conv2D)    (None, 7, 7, 1824)   554496      block6g_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block6h_expand_bn (BatchNormali (None, 7, 7, 1824)   7296        block6h_expand_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6h_expand_activation (Acti (None, 7, 7, 1824)   0           block6h_expand_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block6h_dwconv (DepthwiseConv2D (None, 7, 7, 1824)   45600       block6h_expand_activation[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "block6h_bn (BatchNormalization) (None, 7, 7, 1824)   7296        block6h_dwconv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block6h_activation (Activation) (None, 7, 7, 1824)   0           block6h_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block6h_se_squeeze (GlobalAvera (None, 1824)         0           block6h_activation[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6h_se_reshape (Reshape)    (None, 1, 1, 1824)   0           block6h_se_squeeze[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6h_se_reduce (Conv2D)      (None, 1, 1, 76)     138700      block6h_se_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6h_se_expand (Conv2D)      (None, 1, 1, 1824)   140448      block6h_se_reduce[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block6h_se_excite (Multiply)    (None, 7, 7, 1824)   0           block6h_activation[0][0]         \n",
            "                                                                 block6h_se_expand[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block6h_project_conv (Conv2D)   (None, 7, 7, 304)    554496      block6h_se_excite[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block6h_project_bn (BatchNormal (None, 7, 7, 304)    1216        block6h_project_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block6h_drop (Dropout)          (None, 7, 7, 304)    0           block6h_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6h_add (Add)               (None, 7, 7, 304)    0           block6h_drop[0][0]               \n",
            "                                                                 block6g_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block6i_expand_conv (Conv2D)    (None, 7, 7, 1824)   554496      block6h_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block6i_expand_bn (BatchNormali (None, 7, 7, 1824)   7296        block6i_expand_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6i_expand_activation (Acti (None, 7, 7, 1824)   0           block6i_expand_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block6i_dwconv (DepthwiseConv2D (None, 7, 7, 1824)   45600       block6i_expand_activation[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "block6i_bn (BatchNormalization) (None, 7, 7, 1824)   7296        block6i_dwconv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block6i_activation (Activation) (None, 7, 7, 1824)   0           block6i_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block6i_se_squeeze (GlobalAvera (None, 1824)         0           block6i_activation[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6i_se_reshape (Reshape)    (None, 1, 1, 1824)   0           block6i_se_squeeze[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6i_se_reduce (Conv2D)      (None, 1, 1, 76)     138700      block6i_se_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6i_se_expand (Conv2D)      (None, 1, 1, 1824)   140448      block6i_se_reduce[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block6i_se_excite (Multiply)    (None, 7, 7, 1824)   0           block6i_activation[0][0]         \n",
            "                                                                 block6i_se_expand[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block6i_project_conv (Conv2D)   (None, 7, 7, 304)    554496      block6i_se_excite[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block6i_project_bn (BatchNormal (None, 7, 7, 304)    1216        block6i_project_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block6i_drop (Dropout)          (None, 7, 7, 304)    0           block6i_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6i_add (Add)               (None, 7, 7, 304)    0           block6i_drop[0][0]               \n",
            "                                                                 block6h_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block7a_expand_conv (Conv2D)    (None, 7, 7, 1824)   554496      block6i_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block7a_expand_bn (BatchNormali (None, 7, 7, 1824)   7296        block7a_expand_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7a_expand_activation (Acti (None, 7, 7, 1824)   0           block7a_expand_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block7a_dwconv (DepthwiseConv2D (None, 7, 7, 1824)   16416       block7a_expand_activation[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "block7a_bn (BatchNormalization) (None, 7, 7, 1824)   7296        block7a_dwconv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block7a_activation (Activation) (None, 7, 7, 1824)   0           block7a_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block7a_se_squeeze (GlobalAvera (None, 1824)         0           block7a_activation[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7a_se_reshape (Reshape)    (None, 1, 1, 1824)   0           block7a_se_squeeze[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7a_se_reduce (Conv2D)      (None, 1, 1, 76)     138700      block7a_se_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7a_se_expand (Conv2D)      (None, 1, 1, 1824)   140448      block7a_se_reduce[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block7a_se_excite (Multiply)    (None, 7, 7, 1824)   0           block7a_activation[0][0]         \n",
            "                                                                 block7a_se_expand[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block7a_project_conv (Conv2D)   (None, 7, 7, 512)    933888      block7a_se_excite[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block7a_project_bn (BatchNormal (None, 7, 7, 512)    2048        block7a_project_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block7b_expand_conv (Conv2D)    (None, 7, 7, 3072)   1572864     block7a_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7b_expand_bn (BatchNormali (None, 7, 7, 3072)   12288       block7b_expand_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7b_expand_activation (Acti (None, 7, 7, 3072)   0           block7b_expand_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block7b_dwconv (DepthwiseConv2D (None, 7, 7, 3072)   27648       block7b_expand_activation[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "block7b_bn (BatchNormalization) (None, 7, 7, 3072)   12288       block7b_dwconv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block7b_activation (Activation) (None, 7, 7, 3072)   0           block7b_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block7b_se_squeeze (GlobalAvera (None, 3072)         0           block7b_activation[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7b_se_reshape (Reshape)    (None, 1, 1, 3072)   0           block7b_se_squeeze[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7b_se_reduce (Conv2D)      (None, 1, 1, 128)    393344      block7b_se_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7b_se_expand (Conv2D)      (None, 1, 1, 3072)   396288      block7b_se_reduce[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block7b_se_excite (Multiply)    (None, 7, 7, 3072)   0           block7b_activation[0][0]         \n",
            "                                                                 block7b_se_expand[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block7b_project_conv (Conv2D)   (None, 7, 7, 512)    1572864     block7b_se_excite[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block7b_project_bn (BatchNormal (None, 7, 7, 512)    2048        block7b_project_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block7b_drop (Dropout)          (None, 7, 7, 512)    0           block7b_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7b_add (Add)               (None, 7, 7, 512)    0           block7b_drop[0][0]               \n",
            "                                                                 block7a_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7c_expand_conv (Conv2D)    (None, 7, 7, 3072)   1572864     block7b_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block7c_expand_bn (BatchNormali (None, 7, 7, 3072)   12288       block7c_expand_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7c_expand_activation (Acti (None, 7, 7, 3072)   0           block7c_expand_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block7c_dwconv (DepthwiseConv2D (None, 7, 7, 3072)   27648       block7c_expand_activation[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "block7c_bn (BatchNormalization) (None, 7, 7, 3072)   12288       block7c_dwconv[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "block7c_activation (Activation) (None, 7, 7, 3072)   0           block7c_bn[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "block7c_se_squeeze (GlobalAvera (None, 3072)         0           block7c_activation[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7c_se_reshape (Reshape)    (None, 1, 1, 3072)   0           block7c_se_squeeze[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7c_se_reduce (Conv2D)      (None, 1, 1, 128)    393344      block7c_se_reshape[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7c_se_expand (Conv2D)      (None, 1, 1, 3072)   396288      block7c_se_reduce[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block7c_se_excite (Multiply)    (None, 7, 7, 3072)   0           block7c_activation[0][0]         \n",
            "                                                                 block7c_se_expand[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block7c_project_conv (Conv2D)   (None, 7, 7, 512)    1572864     block7c_se_excite[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "block7c_project_bn (BatchNormal (None, 7, 7, 512)    2048        block7c_project_conv[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block7c_drop (Dropout)          (None, 7, 7, 512)    0           block7c_project_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7c_add (Add)               (None, 7, 7, 512)    0           block7c_drop[0][0]               \n",
            "                                                                 block7b_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "top_conv (Conv2D)               (None, 7, 7, 2048)   1048576     block7c_add[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "top_bn (BatchNormalization)     (None, 7, 7, 2048)   8192        top_conv[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "top_activation (Activation)     (None, 7, 7, 2048)   0           top_bn[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "avg_pool (GlobalAveragePooling2 (None, 2048)         0           top_activation[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "top_dropout (Dropout)           (None, 2048)         0           avg_pool[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "predictions (Dense)             (None, 10)           20490       top_dropout[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 28,533,149\n",
            "Trainable params: 28,360,410\n",
            "Non-trainable params: 172,739\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKMJhbFnxotA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b89d66a-4ec1-49d3-b251-cc6bd9083ede"
      },
      "source": [
        "EfficientNetB5_model.fit_generator(train_generator, epochs=500, validation_data=val_generator, callbacks=[checkpoint])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "52/52 [==============================] - 81s 807ms/step - loss: 4.6690 - accuracy: 0.1005 - val_loss: 2.4015 - val_accuracy: 0.0936\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.09360, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_EfficientNetB5.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 2.7577 - accuracy: 0.1248 - val_loss: 2.3763 - val_accuracy: 0.0936\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.09360\n",
            "Epoch 3/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 2.5746 - accuracy: 0.1285 - val_loss: 2.3644 - val_accuracy: 0.0936\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.09360\n",
            "Epoch 4/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 2.5181 - accuracy: 0.1364 - val_loss: 2.4592 - val_accuracy: 0.0985\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.09360 to 0.09852, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_EfficientNetB5.h5\n",
            "Epoch 5/500\n",
            "52/52 [==============================] - 38s 722ms/step - loss: 2.2754 - accuracy: 0.2083 - val_loss: 2.7291 - val_accuracy: 0.0936\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.09852\n",
            "Epoch 6/500\n",
            "52/52 [==============================] - 37s 716ms/step - loss: 2.1783 - accuracy: 0.2814 - val_loss: 3.6218 - val_accuracy: 0.0985\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.09852\n",
            "Epoch 7/500\n",
            "52/52 [==============================] - 37s 715ms/step - loss: 2.0172 - accuracy: 0.3374 - val_loss: 13.5634 - val_accuracy: 0.0985\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.09852\n",
            "Epoch 8/500\n",
            "52/52 [==============================] - 37s 714ms/step - loss: 1.8282 - accuracy: 0.3892 - val_loss: 2.4766 - val_accuracy: 0.1108\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.09852 to 0.11084, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_EfficientNetB5.h5\n",
            "Epoch 9/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 1.7265 - accuracy: 0.4647 - val_loss: 2.7988 - val_accuracy: 0.0961\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.11084\n",
            "Epoch 10/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 1.5981 - accuracy: 0.5055 - val_loss: 81.8471 - val_accuracy: 0.0985\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.11084\n",
            "Epoch 11/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 1.3220 - accuracy: 0.5901 - val_loss: 468.0206 - val_accuracy: 0.0985\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.11084\n",
            "Epoch 12/500\n",
            "52/52 [==============================] - 37s 711ms/step - loss: 1.1771 - accuracy: 0.6315 - val_loss: 19.3334 - val_accuracy: 0.0936\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.11084\n",
            "Epoch 13/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 1.0234 - accuracy: 0.6772 - val_loss: 4.1445 - val_accuracy: 0.0985\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.11084\n",
            "Epoch 14/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.9092 - accuracy: 0.7077 - val_loss: 3.6390 - val_accuracy: 0.1158\n",
            "\n",
            "Epoch 00014: val_accuracy improved from 0.11084 to 0.11576, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_EfficientNetB5.h5\n",
            "Epoch 15/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.7892 - accuracy: 0.7594 - val_loss: 2.8320 - val_accuracy: 0.3522\n",
            "\n",
            "Epoch 00015: val_accuracy improved from 0.11576 to 0.35222, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_EfficientNetB5.h5\n",
            "Epoch 16/500\n",
            "52/52 [==============================] - 38s 719ms/step - loss: 0.7404 - accuracy: 0.7503 - val_loss: 1.8209 - val_accuracy: 0.5936\n",
            "\n",
            "Epoch 00016: val_accuracy improved from 0.35222 to 0.59360, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_EfficientNetB5.h5\n",
            "Epoch 17/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.6760 - accuracy: 0.7917 - val_loss: 1.3503 - val_accuracy: 0.6256\n",
            "\n",
            "Epoch 00017: val_accuracy improved from 0.59360 to 0.62562, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_EfficientNetB5.h5\n",
            "Epoch 18/500\n",
            "52/52 [==============================] - 37s 713ms/step - loss: 0.6753 - accuracy: 0.7887 - val_loss: 1.1906 - val_accuracy: 0.7118\n",
            "\n",
            "Epoch 00018: val_accuracy improved from 0.62562 to 0.71182, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_EfficientNetB5.h5\n",
            "Epoch 19/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.5343 - accuracy: 0.8307 - val_loss: 0.4845 - val_accuracy: 0.8276\n",
            "\n",
            "Epoch 00019: val_accuracy improved from 0.71182 to 0.82759, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_EfficientNetB5.h5\n",
            "Epoch 20/500\n",
            "52/52 [==============================] - 38s 719ms/step - loss: 0.6087 - accuracy: 0.8106 - val_loss: 7.1213 - val_accuracy: 0.1355\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.82759\n",
            "Epoch 21/500\n",
            "52/52 [==============================] - 37s 716ms/step - loss: 0.4954 - accuracy: 0.8289 - val_loss: 2.5055 - val_accuracy: 0.5443\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.82759\n",
            "Epoch 22/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.5191 - accuracy: 0.8337 - val_loss: 4.9357 - val_accuracy: 0.2783\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.82759\n",
            "Epoch 23/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.4398 - accuracy: 0.8563 - val_loss: 7.1790 - val_accuracy: 0.1798\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.82759\n",
            "Epoch 24/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.4350 - accuracy: 0.8520 - val_loss: 3.7561 - val_accuracy: 0.3793\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.82759\n",
            "Epoch 25/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.4433 - accuracy: 0.8581 - val_loss: 0.5480 - val_accuracy: 0.8276\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.82759\n",
            "Epoch 26/500\n",
            "52/52 [==============================] - 37s 713ms/step - loss: 0.4313 - accuracy: 0.8599 - val_loss: 0.9423 - val_accuracy: 0.7586\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.82759\n",
            "Epoch 27/500\n",
            "52/52 [==============================] - 37s 715ms/step - loss: 0.3950 - accuracy: 0.8752 - val_loss: 7.2314 - val_accuracy: 0.1182\n",
            "\n",
            "Epoch 00027: val_accuracy did not improve from 0.82759\n",
            "Epoch 28/500\n",
            "52/52 [==============================] - 37s 714ms/step - loss: 0.3472 - accuracy: 0.8934 - val_loss: 0.8119 - val_accuracy: 0.7635\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.82759\n",
            "Epoch 29/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.3359 - accuracy: 0.8983 - val_loss: 4.8355 - val_accuracy: 0.1502\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.82759\n",
            "Epoch 30/500\n",
            "52/52 [==============================] - 37s 716ms/step - loss: 0.3714 - accuracy: 0.8825 - val_loss: 5.9096 - val_accuracy: 0.1773\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.82759\n",
            "Epoch 31/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.2996 - accuracy: 0.9044 - val_loss: 6.4113 - val_accuracy: 0.1823\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.82759\n",
            "Epoch 32/500\n",
            "52/52 [==============================] - 37s 715ms/step - loss: 0.3409 - accuracy: 0.8873 - val_loss: 0.6903 - val_accuracy: 0.8054\n",
            "\n",
            "Epoch 00032: val_accuracy did not improve from 0.82759\n",
            "Epoch 33/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.3043 - accuracy: 0.9019 - val_loss: 0.8625 - val_accuracy: 0.7685\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.82759\n",
            "Epoch 34/500\n",
            "52/52 [==============================] - 37s 715ms/step - loss: 0.2584 - accuracy: 0.9184 - val_loss: 0.5633 - val_accuracy: 0.8177\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.82759\n",
            "Epoch 35/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.3432 - accuracy: 0.8922 - val_loss: 0.6980 - val_accuracy: 0.8153\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.82759\n",
            "Epoch 36/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.2929 - accuracy: 0.9093 - val_loss: 0.9328 - val_accuracy: 0.7685\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.82759\n",
            "Epoch 37/500\n",
            "52/52 [==============================] - 37s 712ms/step - loss: 0.2816 - accuracy: 0.9105 - val_loss: 1.9732 - val_accuracy: 0.6700\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.82759\n",
            "Epoch 38/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.2374 - accuracy: 0.9275 - val_loss: 0.5514 - val_accuracy: 0.8374\n",
            "\n",
            "Epoch 00038: val_accuracy improved from 0.82759 to 0.83744, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_EfficientNetB5.h5\n",
            "Epoch 39/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.1988 - accuracy: 0.9324 - val_loss: 0.9922 - val_accuracy: 0.7291\n",
            "\n",
            "Epoch 00039: val_accuracy did not improve from 0.83744\n",
            "Epoch 40/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.1858 - accuracy: 0.9397 - val_loss: 0.5203 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.83744\n",
            "Epoch 41/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.3034 - accuracy: 0.9032 - val_loss: 0.5531 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00041: val_accuracy improved from 0.83744 to 0.85714, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_EfficientNetB5.h5\n",
            "Epoch 42/500\n",
            "52/52 [==============================] - 37s 715ms/step - loss: 0.2509 - accuracy: 0.9147 - val_loss: 0.6848 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.85714\n",
            "Epoch 43/500\n",
            "52/52 [==============================] - 38s 722ms/step - loss: 0.2218 - accuracy: 0.9239 - val_loss: 2.5402 - val_accuracy: 0.5714\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.85714\n",
            "Epoch 44/500\n",
            "52/52 [==============================] - 37s 715ms/step - loss: 0.2390 - accuracy: 0.9233 - val_loss: 6.3036 - val_accuracy: 0.1552\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.85714\n",
            "Epoch 45/500\n",
            "52/52 [==============================] - 37s 714ms/step - loss: 0.2134 - accuracy: 0.9287 - val_loss: 0.5130 - val_accuracy: 0.8522\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.85714\n",
            "Epoch 46/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.1635 - accuracy: 0.9537 - val_loss: 0.5640 - val_accuracy: 0.8473\n",
            "\n",
            "Epoch 00046: val_accuracy did not improve from 0.85714\n",
            "Epoch 47/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.1597 - accuracy: 0.9476 - val_loss: 0.6190 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.85714\n",
            "Epoch 48/500\n",
            "52/52 [==============================] - 37s 716ms/step - loss: 0.1902 - accuracy: 0.9385 - val_loss: 2.6511 - val_accuracy: 0.5148\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.85714\n",
            "Epoch 49/500\n",
            "52/52 [==============================] - 38s 722ms/step - loss: 0.1898 - accuracy: 0.9361 - val_loss: 0.4569 - val_accuracy: 0.8645\n",
            "\n",
            "Epoch 00049: val_accuracy improved from 0.85714 to 0.86453, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_EfficientNetB5.h5\n",
            "Epoch 50/500\n",
            "52/52 [==============================] - 37s 716ms/step - loss: 0.1546 - accuracy: 0.9464 - val_loss: 1.3529 - val_accuracy: 0.7340\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.86453\n",
            "Epoch 51/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.1574 - accuracy: 0.9470 - val_loss: 0.9393 - val_accuracy: 0.8103\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.86453\n",
            "Epoch 52/500\n",
            "52/52 [==============================] - 38s 722ms/step - loss: 0.2096 - accuracy: 0.9330 - val_loss: 5.0537 - val_accuracy: 0.2709\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.86453\n",
            "Epoch 53/500\n",
            "52/52 [==============================] - 37s 714ms/step - loss: 0.1796 - accuracy: 0.9330 - val_loss: 2.6159 - val_accuracy: 0.5862\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.86453\n",
            "Epoch 54/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.1380 - accuracy: 0.9543 - val_loss: 0.7811 - val_accuracy: 0.7906\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.86453\n",
            "Epoch 55/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.1828 - accuracy: 0.9409 - val_loss: 0.6250 - val_accuracy: 0.8399\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.86453\n",
            "Epoch 56/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.1726 - accuracy: 0.9519 - val_loss: 0.5859 - val_accuracy: 0.8621\n",
            "\n",
            "Epoch 00056: val_accuracy did not improve from 0.86453\n",
            "Epoch 57/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.1909 - accuracy: 0.9421 - val_loss: 0.9261 - val_accuracy: 0.7808\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.86453\n",
            "Epoch 58/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.1614 - accuracy: 0.9446 - val_loss: 0.5489 - val_accuracy: 0.8547\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.86453\n",
            "Epoch 59/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.1027 - accuracy: 0.9665 - val_loss: 0.4787 - val_accuracy: 0.8719\n",
            "\n",
            "Epoch 00059: val_accuracy improved from 0.86453 to 0.87192, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_EfficientNetB5.h5\n",
            "Epoch 60/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.2020 - accuracy: 0.9415 - val_loss: 1.6735 - val_accuracy: 0.6355\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.87192\n",
            "Epoch 61/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.1643 - accuracy: 0.9470 - val_loss: 1.4189 - val_accuracy: 0.7217\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.87192\n",
            "Epoch 62/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.1736 - accuracy: 0.9415 - val_loss: 4.7529 - val_accuracy: 0.3350\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.87192\n",
            "Epoch 63/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.1491 - accuracy: 0.9549 - val_loss: 0.5084 - val_accuracy: 0.8695\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.87192\n",
            "Epoch 64/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.1159 - accuracy: 0.9629 - val_loss: 0.6632 - val_accuracy: 0.8374\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.87192\n",
            "Epoch 65/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.2387 - accuracy: 0.9281 - val_loss: 6.0158 - val_accuracy: 0.1059\n",
            "\n",
            "Epoch 00065: val_accuracy did not improve from 0.87192\n",
            "Epoch 66/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.1831 - accuracy: 0.9440 - val_loss: 6.2570 - val_accuracy: 0.1453\n",
            "\n",
            "Epoch 00066: val_accuracy did not improve from 0.87192\n",
            "Epoch 67/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0949 - accuracy: 0.9683 - val_loss: 0.4230 - val_accuracy: 0.8744\n",
            "\n",
            "Epoch 00067: val_accuracy improved from 0.87192 to 0.87438, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_EfficientNetB5.h5\n",
            "Epoch 68/500\n",
            "52/52 [==============================] - 38s 726ms/step - loss: 0.1099 - accuracy: 0.9708 - val_loss: 0.6355 - val_accuracy: 0.8399\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.87438\n",
            "Epoch 69/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.1175 - accuracy: 0.9537 - val_loss: 0.6195 - val_accuracy: 0.8350\n",
            "\n",
            "Epoch 00069: val_accuracy did not improve from 0.87438\n",
            "Epoch 70/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.1302 - accuracy: 0.9574 - val_loss: 0.9356 - val_accuracy: 0.8054\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.87438\n",
            "Epoch 71/500\n",
            "52/52 [==============================] - 37s 716ms/step - loss: 0.1783 - accuracy: 0.9458 - val_loss: 1.5277 - val_accuracy: 0.7069\n",
            "\n",
            "Epoch 00071: val_accuracy did not improve from 0.87438\n",
            "Epoch 72/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.1224 - accuracy: 0.9568 - val_loss: 1.3531 - val_accuracy: 0.7167\n",
            "\n",
            "Epoch 00072: val_accuracy did not improve from 0.87438\n",
            "Epoch 73/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.1070 - accuracy: 0.9647 - val_loss: 0.6138 - val_accuracy: 0.8424\n",
            "\n",
            "Epoch 00073: val_accuracy did not improve from 0.87438\n",
            "Epoch 74/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.1299 - accuracy: 0.9531 - val_loss: 0.6483 - val_accuracy: 0.8522\n",
            "\n",
            "Epoch 00074: val_accuracy did not improve from 0.87438\n",
            "Epoch 75/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.1638 - accuracy: 0.9421 - val_loss: 0.9383 - val_accuracy: 0.7980\n",
            "\n",
            "Epoch 00075: val_accuracy did not improve from 0.87438\n",
            "Epoch 76/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.1103 - accuracy: 0.9635 - val_loss: 0.5265 - val_accuracy: 0.8621\n",
            "\n",
            "Epoch 00076: val_accuracy did not improve from 0.87438\n",
            "Epoch 77/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.1317 - accuracy: 0.9574 - val_loss: 0.5467 - val_accuracy: 0.8522\n",
            "\n",
            "Epoch 00077: val_accuracy did not improve from 0.87438\n",
            "Epoch 78/500\n",
            "52/52 [==============================] - 38s 722ms/step - loss: 0.0929 - accuracy: 0.9683 - val_loss: 0.5424 - val_accuracy: 0.8670\n",
            "\n",
            "Epoch 00078: val_accuracy did not improve from 0.87438\n",
            "Epoch 79/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.0933 - accuracy: 0.9714 - val_loss: 0.5198 - val_accuracy: 0.8621\n",
            "\n",
            "Epoch 00079: val_accuracy did not improve from 0.87438\n",
            "Epoch 80/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.1087 - accuracy: 0.9592 - val_loss: 2.5610 - val_accuracy: 0.6010\n",
            "\n",
            "Epoch 00080: val_accuracy did not improve from 0.87438\n",
            "Epoch 81/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0886 - accuracy: 0.9702 - val_loss: 0.5975 - val_accuracy: 0.8473\n",
            "\n",
            "Epoch 00081: val_accuracy did not improve from 0.87438\n",
            "Epoch 82/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.1701 - accuracy: 0.9488 - val_loss: 7.8616 - val_accuracy: 0.1158\n",
            "\n",
            "Epoch 00082: val_accuracy did not improve from 0.87438\n",
            "Epoch 83/500\n",
            "52/52 [==============================] - 37s 712ms/step - loss: 0.1821 - accuracy: 0.9446 - val_loss: 3.7477 - val_accuracy: 0.4852\n",
            "\n",
            "Epoch 00083: val_accuracy did not improve from 0.87438\n",
            "Epoch 84/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.1137 - accuracy: 0.9622 - val_loss: 5.1832 - val_accuracy: 0.3251\n",
            "\n",
            "Epoch 00084: val_accuracy did not improve from 0.87438\n",
            "Epoch 85/500\n",
            "52/52 [==============================] - 38s 722ms/step - loss: 0.0999 - accuracy: 0.9677 - val_loss: 7.3056 - val_accuracy: 0.1626\n",
            "\n",
            "Epoch 00085: val_accuracy did not improve from 0.87438\n",
            "Epoch 86/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0903 - accuracy: 0.9708 - val_loss: 10.4559 - val_accuracy: 0.1108\n",
            "\n",
            "Epoch 00086: val_accuracy did not improve from 0.87438\n",
            "Epoch 87/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.1142 - accuracy: 0.9665 - val_loss: 0.9511 - val_accuracy: 0.8079\n",
            "\n",
            "Epoch 00087: val_accuracy did not improve from 0.87438\n",
            "Epoch 88/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0792 - accuracy: 0.9695 - val_loss: 8.8306 - val_accuracy: 0.1207\n",
            "\n",
            "Epoch 00088: val_accuracy did not improve from 0.87438\n",
            "Epoch 89/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0952 - accuracy: 0.9702 - val_loss: 0.7486 - val_accuracy: 0.8276\n",
            "\n",
            "Epoch 00089: val_accuracy did not improve from 0.87438\n",
            "Epoch 90/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.1738 - accuracy: 0.9458 - val_loss: 1.5097 - val_accuracy: 0.7315\n",
            "\n",
            "Epoch 00090: val_accuracy did not improve from 0.87438\n",
            "Epoch 91/500\n",
            "52/52 [==============================] - 37s 715ms/step - loss: 0.1612 - accuracy: 0.9525 - val_loss: 9.5227 - val_accuracy: 0.1330\n",
            "\n",
            "Epoch 00091: val_accuracy did not improve from 0.87438\n",
            "Epoch 92/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0752 - accuracy: 0.9750 - val_loss: 5.7971 - val_accuracy: 0.3202\n",
            "\n",
            "Epoch 00092: val_accuracy did not improve from 0.87438\n",
            "Epoch 93/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.1186 - accuracy: 0.9616 - val_loss: 5.3503 - val_accuracy: 0.2192\n",
            "\n",
            "Epoch 00093: val_accuracy did not improve from 0.87438\n",
            "Epoch 94/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.1164 - accuracy: 0.9598 - val_loss: 0.5848 - val_accuracy: 0.8645\n",
            "\n",
            "Epoch 00094: val_accuracy did not improve from 0.87438\n",
            "Epoch 95/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.1493 - accuracy: 0.9555 - val_loss: 0.8303 - val_accuracy: 0.8177\n",
            "\n",
            "Epoch 00095: val_accuracy did not improve from 0.87438\n",
            "Epoch 96/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.1175 - accuracy: 0.9598 - val_loss: 0.5731 - val_accuracy: 0.8645\n",
            "\n",
            "Epoch 00096: val_accuracy did not improve from 0.87438\n",
            "Epoch 97/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0867 - accuracy: 0.9726 - val_loss: 7.2693 - val_accuracy: 0.1700\n",
            "\n",
            "Epoch 00097: val_accuracy did not improve from 0.87438\n",
            "Epoch 98/500\n",
            "52/52 [==============================] - 38s 724ms/step - loss: 0.1338 - accuracy: 0.9537 - val_loss: 0.7950 - val_accuracy: 0.8350\n",
            "\n",
            "Epoch 00098: val_accuracy did not improve from 0.87438\n",
            "Epoch 99/500\n",
            "52/52 [==============================] - 38s 723ms/step - loss: 0.1005 - accuracy: 0.9671 - val_loss: 0.8830 - val_accuracy: 0.7783\n",
            "\n",
            "Epoch 00099: val_accuracy did not improve from 0.87438\n",
            "Epoch 100/500\n",
            "52/52 [==============================] - 38s 722ms/step - loss: 0.1103 - accuracy: 0.9653 - val_loss: 1.2310 - val_accuracy: 0.7808\n",
            "\n",
            "Epoch 00100: val_accuracy did not improve from 0.87438\n",
            "Epoch 101/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0928 - accuracy: 0.9695 - val_loss: 0.6099 - val_accuracy: 0.8621\n",
            "\n",
            "Epoch 00101: val_accuracy did not improve from 0.87438\n",
            "Epoch 102/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0818 - accuracy: 0.9750 - val_loss: 2.8438 - val_accuracy: 0.5936\n",
            "\n",
            "Epoch 00102: val_accuracy did not improve from 0.87438\n",
            "Epoch 103/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.1354 - accuracy: 0.9537 - val_loss: 3.1216 - val_accuracy: 0.5320\n",
            "\n",
            "Epoch 00103: val_accuracy did not improve from 0.87438\n",
            "Epoch 104/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.1165 - accuracy: 0.9647 - val_loss: 1.0394 - val_accuracy: 0.7882\n",
            "\n",
            "Epoch 00104: val_accuracy did not improve from 0.87438\n",
            "Epoch 105/500\n",
            "52/52 [==============================] - 38s 722ms/step - loss: 0.0802 - accuracy: 0.9762 - val_loss: 0.5971 - val_accuracy: 0.8768\n",
            "\n",
            "Epoch 00105: val_accuracy improved from 0.87438 to 0.87685, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_EfficientNetB5.h5\n",
            "Epoch 106/500\n",
            "52/52 [==============================] - 38s 719ms/step - loss: 0.0826 - accuracy: 0.9720 - val_loss: 0.5110 - val_accuracy: 0.8818\n",
            "\n",
            "Epoch 00106: val_accuracy improved from 0.87685 to 0.88177, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_EfficientNetB5.h5\n",
            "Epoch 107/500\n",
            "52/52 [==============================] - 38s 723ms/step - loss: 0.0967 - accuracy: 0.9732 - val_loss: 5.4222 - val_accuracy: 0.3202\n",
            "\n",
            "Epoch 00107: val_accuracy did not improve from 0.88177\n",
            "Epoch 108/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0757 - accuracy: 0.9708 - val_loss: 5.6033 - val_accuracy: 0.2635\n",
            "\n",
            "Epoch 00108: val_accuracy did not improve from 0.88177\n",
            "Epoch 109/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0622 - accuracy: 0.9817 - val_loss: 0.5438 - val_accuracy: 0.8768\n",
            "\n",
            "Epoch 00109: val_accuracy did not improve from 0.88177\n",
            "Epoch 110/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0780 - accuracy: 0.9769 - val_loss: 0.6933 - val_accuracy: 0.8473\n",
            "\n",
            "Epoch 00110: val_accuracy did not improve from 0.88177\n",
            "Epoch 111/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.1046 - accuracy: 0.9677 - val_loss: 0.6797 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00111: val_accuracy did not improve from 0.88177\n",
            "Epoch 112/500\n",
            "52/52 [==============================] - 37s 716ms/step - loss: 0.0748 - accuracy: 0.9750 - val_loss: 1.8874 - val_accuracy: 0.6207\n",
            "\n",
            "Epoch 00112: val_accuracy did not improve from 0.88177\n",
            "Epoch 113/500\n",
            "52/52 [==============================] - 37s 716ms/step - loss: 0.0898 - accuracy: 0.9714 - val_loss: 0.5660 - val_accuracy: 0.8621\n",
            "\n",
            "Epoch 00113: val_accuracy did not improve from 0.88177\n",
            "Epoch 114/500\n",
            "52/52 [==============================] - 38s 731ms/step - loss: 0.1128 - accuracy: 0.9647 - val_loss: 0.7983 - val_accuracy: 0.8276\n",
            "\n",
            "Epoch 00114: val_accuracy did not improve from 0.88177\n",
            "Epoch 115/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0830 - accuracy: 0.9677 - val_loss: 0.8267 - val_accuracy: 0.8498\n",
            "\n",
            "Epoch 00115: val_accuracy did not improve from 0.88177\n",
            "Epoch 116/500\n",
            "52/52 [==============================] - 38s 722ms/step - loss: 0.1308 - accuracy: 0.9622 - val_loss: 0.8440 - val_accuracy: 0.8399\n",
            "\n",
            "Epoch 00116: val_accuracy did not improve from 0.88177\n",
            "Epoch 117/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0937 - accuracy: 0.9708 - val_loss: 0.8999 - val_accuracy: 0.8399\n",
            "\n",
            "Epoch 00117: val_accuracy did not improve from 0.88177\n",
            "Epoch 118/500\n",
            "52/52 [==============================] - 37s 715ms/step - loss: 0.1108 - accuracy: 0.9653 - val_loss: 4.4451 - val_accuracy: 0.4064\n",
            "\n",
            "Epoch 00118: val_accuracy did not improve from 0.88177\n",
            "Epoch 119/500\n",
            "52/52 [==============================] - 38s 722ms/step - loss: 0.1024 - accuracy: 0.9720 - val_loss: 0.9227 - val_accuracy: 0.8227\n",
            "\n",
            "Epoch 00119: val_accuracy did not improve from 0.88177\n",
            "Epoch 120/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.1351 - accuracy: 0.9568 - val_loss: 0.5172 - val_accuracy: 0.8768\n",
            "\n",
            "Epoch 00120: val_accuracy did not improve from 0.88177\n",
            "Epoch 121/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0436 - accuracy: 0.9854 - val_loss: 0.6842 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00121: val_accuracy did not improve from 0.88177\n",
            "Epoch 122/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0822 - accuracy: 0.9708 - val_loss: 1.1086 - val_accuracy: 0.8103\n",
            "\n",
            "Epoch 00122: val_accuracy did not improve from 0.88177\n",
            "Epoch 123/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.1010 - accuracy: 0.9695 - val_loss: 0.7060 - val_accuracy: 0.8498\n",
            "\n",
            "Epoch 00123: val_accuracy did not improve from 0.88177\n",
            "Epoch 124/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0744 - accuracy: 0.9793 - val_loss: 0.7775 - val_accuracy: 0.8596\n",
            "\n",
            "Epoch 00124: val_accuracy did not improve from 0.88177\n",
            "Epoch 125/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0452 - accuracy: 0.9823 - val_loss: 0.5573 - val_accuracy: 0.8719\n",
            "\n",
            "Epoch 00125: val_accuracy did not improve from 0.88177\n",
            "Epoch 126/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0516 - accuracy: 0.9866 - val_loss: 0.5051 - val_accuracy: 0.8768\n",
            "\n",
            "Epoch 00126: val_accuracy did not improve from 0.88177\n",
            "Epoch 127/500\n",
            "52/52 [==============================] - 38s 722ms/step - loss: 0.0739 - accuracy: 0.9769 - val_loss: 6.4464 - val_accuracy: 0.2906\n",
            "\n",
            "Epoch 00127: val_accuracy did not improve from 0.88177\n",
            "Epoch 128/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0649 - accuracy: 0.9829 - val_loss: 0.6016 - val_accuracy: 0.8768\n",
            "\n",
            "Epoch 00128: val_accuracy did not improve from 0.88177\n",
            "Epoch 129/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0720 - accuracy: 0.9787 - val_loss: 6.7673 - val_accuracy: 0.2414\n",
            "\n",
            "Epoch 00129: val_accuracy did not improve from 0.88177\n",
            "Epoch 130/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0822 - accuracy: 0.9708 - val_loss: 9.0280 - val_accuracy: 0.1502\n",
            "\n",
            "Epoch 00130: val_accuracy did not improve from 0.88177\n",
            "Epoch 131/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0455 - accuracy: 0.9872 - val_loss: 0.6452 - val_accuracy: 0.8621\n",
            "\n",
            "Epoch 00131: val_accuracy did not improve from 0.88177\n",
            "Epoch 132/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0603 - accuracy: 0.9805 - val_loss: 0.6407 - val_accuracy: 0.8719\n",
            "\n",
            "Epoch 00132: val_accuracy did not improve from 0.88177\n",
            "Epoch 133/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0679 - accuracy: 0.9799 - val_loss: 0.5927 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00133: val_accuracy did not improve from 0.88177\n",
            "Epoch 134/500\n",
            "52/52 [==============================] - 37s 714ms/step - loss: 0.0533 - accuracy: 0.9811 - val_loss: 3.1133 - val_accuracy: 0.5911\n",
            "\n",
            "Epoch 00134: val_accuracy did not improve from 0.88177\n",
            "Epoch 135/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.1010 - accuracy: 0.9708 - val_loss: 2.9089 - val_accuracy: 0.5616\n",
            "\n",
            "Epoch 00135: val_accuracy did not improve from 0.88177\n",
            "Epoch 136/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.1780 - accuracy: 0.9495 - val_loss: 7.6956 - val_accuracy: 0.1059\n",
            "\n",
            "Epoch 00136: val_accuracy did not improve from 0.88177\n",
            "Epoch 137/500\n",
            "52/52 [==============================] - 37s 711ms/step - loss: 0.1288 - accuracy: 0.9598 - val_loss: 8.3921 - val_accuracy: 0.0985\n",
            "\n",
            "Epoch 00137: val_accuracy did not improve from 0.88177\n",
            "Epoch 138/500\n",
            "52/52 [==============================] - 37s 716ms/step - loss: 0.0678 - accuracy: 0.9829 - val_loss: 6.7304 - val_accuracy: 0.0985\n",
            "\n",
            "Epoch 00138: val_accuracy did not improve from 0.88177\n",
            "Epoch 139/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0535 - accuracy: 0.9817 - val_loss: 1.3211 - val_accuracy: 0.7192\n",
            "\n",
            "Epoch 00139: val_accuracy did not improve from 0.88177\n",
            "Epoch 140/500\n",
            "52/52 [==============================] - 38s 723ms/step - loss: 0.0504 - accuracy: 0.9817 - val_loss: 0.4408 - val_accuracy: 0.8892\n",
            "\n",
            "Epoch 00140: val_accuracy improved from 0.88177 to 0.88916, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_EfficientNetB5.h5\n",
            "Epoch 141/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0567 - accuracy: 0.9848 - val_loss: 0.8073 - val_accuracy: 0.8424\n",
            "\n",
            "Epoch 00141: val_accuracy did not improve from 0.88916\n",
            "Epoch 142/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0834 - accuracy: 0.9750 - val_loss: 0.6047 - val_accuracy: 0.8621\n",
            "\n",
            "Epoch 00142: val_accuracy did not improve from 0.88916\n",
            "Epoch 143/500\n",
            "52/52 [==============================] - 38s 733ms/step - loss: 0.0740 - accuracy: 0.9756 - val_loss: 1.2258 - val_accuracy: 0.8079\n",
            "\n",
            "Epoch 00143: val_accuracy did not improve from 0.88916\n",
            "Epoch 144/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0913 - accuracy: 0.9708 - val_loss: 1.2797 - val_accuracy: 0.7562\n",
            "\n",
            "Epoch 00144: val_accuracy did not improve from 0.88916\n",
            "Epoch 145/500\n",
            "52/52 [==============================] - 38s 719ms/step - loss: 0.0940 - accuracy: 0.9677 - val_loss: 0.6999 - val_accuracy: 0.8227\n",
            "\n",
            "Epoch 00145: val_accuracy did not improve from 0.88916\n",
            "Epoch 146/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0910 - accuracy: 0.9702 - val_loss: 6.0095 - val_accuracy: 0.2291\n",
            "\n",
            "Epoch 00146: val_accuracy did not improve from 0.88916\n",
            "Epoch 147/500\n",
            "52/52 [==============================] - 37s 715ms/step - loss: 0.0420 - accuracy: 0.9866 - val_loss: 2.2589 - val_accuracy: 0.6010\n",
            "\n",
            "Epoch 00147: val_accuracy did not improve from 0.88916\n",
            "Epoch 148/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0584 - accuracy: 0.9799 - val_loss: 2.5009 - val_accuracy: 0.5690\n",
            "\n",
            "Epoch 00148: val_accuracy did not improve from 0.88916\n",
            "Epoch 149/500\n",
            "52/52 [==============================] - 37s 716ms/step - loss: 0.0374 - accuracy: 0.9884 - val_loss: 0.8306 - val_accuracy: 0.8399\n",
            "\n",
            "Epoch 00149: val_accuracy did not improve from 0.88916\n",
            "Epoch 150/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.1066 - accuracy: 0.9732 - val_loss: 1.7097 - val_accuracy: 0.7167\n",
            "\n",
            "Epoch 00150: val_accuracy did not improve from 0.88916\n",
            "Epoch 151/500\n",
            "52/52 [==============================] - 37s 714ms/step - loss: 0.1192 - accuracy: 0.9616 - val_loss: 5.2044 - val_accuracy: 0.3054\n",
            "\n",
            "Epoch 00151: val_accuracy did not improve from 0.88916\n",
            "Epoch 152/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0840 - accuracy: 0.9689 - val_loss: 8.4816 - val_accuracy: 0.1182\n",
            "\n",
            "Epoch 00152: val_accuracy did not improve from 0.88916\n",
            "Epoch 153/500\n",
            "52/52 [==============================] - 37s 716ms/step - loss: 0.0405 - accuracy: 0.9848 - val_loss: 9.0532 - val_accuracy: 0.1207\n",
            "\n",
            "Epoch 00153: val_accuracy did not improve from 0.88916\n",
            "Epoch 154/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0514 - accuracy: 0.9866 - val_loss: 1.0962 - val_accuracy: 0.8005\n",
            "\n",
            "Epoch 00154: val_accuracy did not improve from 0.88916\n",
            "Epoch 155/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.1381 - accuracy: 0.9574 - val_loss: 1.2082 - val_accuracy: 0.7709\n",
            "\n",
            "Epoch 00155: val_accuracy did not improve from 0.88916\n",
            "Epoch 156/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0473 - accuracy: 0.9829 - val_loss: 0.5783 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00156: val_accuracy did not improve from 0.88916\n",
            "Epoch 157/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0214 - accuracy: 0.9933 - val_loss: 0.4557 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00157: val_accuracy improved from 0.88916 to 0.89655, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_EfficientNetB5.h5\n",
            "Epoch 158/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0362 - accuracy: 0.9866 - val_loss: 0.5105 - val_accuracy: 0.8793\n",
            "\n",
            "Epoch 00158: val_accuracy did not improve from 0.89655\n",
            "Epoch 159/500\n",
            "52/52 [==============================] - 38s 722ms/step - loss: 0.0385 - accuracy: 0.9896 - val_loss: 8.0364 - val_accuracy: 0.1232\n",
            "\n",
            "Epoch 00159: val_accuracy did not improve from 0.89655\n",
            "Epoch 160/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0184 - accuracy: 0.9933 - val_loss: 7.5451 - val_accuracy: 0.1724\n",
            "\n",
            "Epoch 00160: val_accuracy did not improve from 0.89655\n",
            "Epoch 161/500\n",
            "52/52 [==============================] - 37s 714ms/step - loss: 0.0272 - accuracy: 0.9903 - val_loss: 8.6688 - val_accuracy: 0.2069\n",
            "\n",
            "Epoch 00161: val_accuracy did not improve from 0.89655\n",
            "Epoch 162/500\n",
            "52/52 [==============================] - 37s 714ms/step - loss: 0.0345 - accuracy: 0.9866 - val_loss: 0.7785 - val_accuracy: 0.8670\n",
            "\n",
            "Epoch 00162: val_accuracy did not improve from 0.89655\n",
            "Epoch 163/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0634 - accuracy: 0.9811 - val_loss: 0.8879 - val_accuracy: 0.8498\n",
            "\n",
            "Epoch 00163: val_accuracy did not improve from 0.89655\n",
            "Epoch 164/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.1041 - accuracy: 0.9689 - val_loss: 1.1558 - val_accuracy: 0.7931\n",
            "\n",
            "Epoch 00164: val_accuracy did not improve from 0.89655\n",
            "Epoch 165/500\n",
            "52/52 [==============================] - 38s 719ms/step - loss: 0.0716 - accuracy: 0.9750 - val_loss: 0.7378 - val_accuracy: 0.8399\n",
            "\n",
            "Epoch 00165: val_accuracy did not improve from 0.89655\n",
            "Epoch 166/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0414 - accuracy: 0.9848 - val_loss: 0.6320 - val_accuracy: 0.8744\n",
            "\n",
            "Epoch 00166: val_accuracy did not improve from 0.89655\n",
            "Epoch 167/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0422 - accuracy: 0.9878 - val_loss: 0.7315 - val_accuracy: 0.8695\n",
            "\n",
            "Epoch 00167: val_accuracy did not improve from 0.89655\n",
            "Epoch 168/500\n",
            "52/52 [==============================] - 37s 714ms/step - loss: 0.1132 - accuracy: 0.9695 - val_loss: 0.9716 - val_accuracy: 0.8276\n",
            "\n",
            "Epoch 00168: val_accuracy did not improve from 0.89655\n",
            "Epoch 169/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0953 - accuracy: 0.9695 - val_loss: 11.7987 - val_accuracy: 0.1379\n",
            "\n",
            "Epoch 00169: val_accuracy did not improve from 0.89655\n",
            "Epoch 170/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0797 - accuracy: 0.9708 - val_loss: 8.4837 - val_accuracy: 0.1576\n",
            "\n",
            "Epoch 00170: val_accuracy did not improve from 0.89655\n",
            "Epoch 171/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.0503 - accuracy: 0.9854 - val_loss: 11.0310 - val_accuracy: 0.1158\n",
            "\n",
            "Epoch 00171: val_accuracy did not improve from 0.89655\n",
            "Epoch 172/500\n",
            "52/52 [==============================] - 37s 716ms/step - loss: 0.0443 - accuracy: 0.9854 - val_loss: 1.7465 - val_accuracy: 0.6823\n",
            "\n",
            "Epoch 00172: val_accuracy did not improve from 0.89655\n",
            "Epoch 173/500\n",
            "52/52 [==============================] - 37s 715ms/step - loss: 0.0454 - accuracy: 0.9872 - val_loss: 0.7595 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00173: val_accuracy did not improve from 0.89655\n",
            "Epoch 174/500\n",
            "52/52 [==============================] - 37s 715ms/step - loss: 0.0431 - accuracy: 0.9848 - val_loss: 3.7670 - val_accuracy: 0.4138\n",
            "\n",
            "Epoch 00174: val_accuracy did not improve from 0.89655\n",
            "Epoch 175/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0349 - accuracy: 0.9903 - val_loss: 1.0038 - val_accuracy: 0.8424\n",
            "\n",
            "Epoch 00175: val_accuracy did not improve from 0.89655\n",
            "Epoch 176/500\n",
            "52/52 [==============================] - 37s 712ms/step - loss: 0.0706 - accuracy: 0.9775 - val_loss: 3.0144 - val_accuracy: 0.6158\n",
            "\n",
            "Epoch 00176: val_accuracy did not improve from 0.89655\n",
            "Epoch 177/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0617 - accuracy: 0.9817 - val_loss: 1.2038 - val_accuracy: 0.7833\n",
            "\n",
            "Epoch 00177: val_accuracy did not improve from 0.89655\n",
            "Epoch 178/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0522 - accuracy: 0.9836 - val_loss: 1.9686 - val_accuracy: 0.7118\n",
            "\n",
            "Epoch 00178: val_accuracy did not improve from 0.89655\n",
            "Epoch 179/500\n",
            "52/52 [==============================] - 38s 723ms/step - loss: 0.0361 - accuracy: 0.9884 - val_loss: 2.0826 - val_accuracy: 0.6823\n",
            "\n",
            "Epoch 00179: val_accuracy did not improve from 0.89655\n",
            "Epoch 180/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0279 - accuracy: 0.9921 - val_loss: 0.6191 - val_accuracy: 0.8695\n",
            "\n",
            "Epoch 00180: val_accuracy did not improve from 0.89655\n",
            "Epoch 181/500\n",
            "52/52 [==============================] - 38s 722ms/step - loss: 0.0424 - accuracy: 0.9878 - val_loss: 8.8719 - val_accuracy: 0.1330\n",
            "\n",
            "Epoch 00181: val_accuracy did not improve from 0.89655\n",
            "Epoch 182/500\n",
            "52/52 [==============================] - 37s 711ms/step - loss: 0.1142 - accuracy: 0.9671 - val_loss: 6.3141 - val_accuracy: 0.3276\n",
            "\n",
            "Epoch 00182: val_accuracy did not improve from 0.89655\n",
            "Epoch 183/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0994 - accuracy: 0.9677 - val_loss: 0.7055 - val_accuracy: 0.8547\n",
            "\n",
            "Epoch 00183: val_accuracy did not improve from 0.89655\n",
            "Epoch 184/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.0608 - accuracy: 0.9805 - val_loss: 0.4884 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00184: val_accuracy did not improve from 0.89655\n",
            "Epoch 185/500\n",
            "52/52 [==============================] - 38s 719ms/step - loss: 0.0241 - accuracy: 0.9927 - val_loss: 0.5025 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00185: val_accuracy did not improve from 0.89655\n",
            "Epoch 186/500\n",
            "52/52 [==============================] - 37s 726ms/step - loss: 0.0340 - accuracy: 0.9927 - val_loss: 1.8067 - val_accuracy: 0.6946\n",
            "\n",
            "Epoch 00186: val_accuracy did not improve from 0.89655\n",
            "Epoch 187/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0363 - accuracy: 0.9854 - val_loss: 0.8828 - val_accuracy: 0.8350\n",
            "\n",
            "Epoch 00187: val_accuracy did not improve from 0.89655\n",
            "Epoch 188/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0368 - accuracy: 0.9890 - val_loss: 0.7330 - val_accuracy: 0.8719\n",
            "\n",
            "Epoch 00188: val_accuracy did not improve from 0.89655\n",
            "Epoch 189/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0599 - accuracy: 0.9817 - val_loss: 0.9509 - val_accuracy: 0.8374\n",
            "\n",
            "Epoch 00189: val_accuracy did not improve from 0.89655\n",
            "Epoch 190/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0770 - accuracy: 0.9750 - val_loss: 0.5514 - val_accuracy: 0.8818\n",
            "\n",
            "Epoch 00190: val_accuracy did not improve from 0.89655\n",
            "Epoch 191/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.0404 - accuracy: 0.9854 - val_loss: 4.7596 - val_accuracy: 0.3522\n",
            "\n",
            "Epoch 00191: val_accuracy did not improve from 0.89655\n",
            "Epoch 192/500\n",
            "52/52 [==============================] - 37s 716ms/step - loss: 0.0228 - accuracy: 0.9933 - val_loss: 1.6142 - val_accuracy: 0.7167\n",
            "\n",
            "Epoch 00192: val_accuracy did not improve from 0.89655\n",
            "Epoch 193/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0217 - accuracy: 0.9945 - val_loss: 4.6511 - val_accuracy: 0.3498\n",
            "\n",
            "Epoch 00193: val_accuracy did not improve from 0.89655\n",
            "Epoch 194/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0200 - accuracy: 0.9939 - val_loss: 0.6195 - val_accuracy: 0.8941\n",
            "\n",
            "Epoch 00194: val_accuracy did not improve from 0.89655\n",
            "Epoch 195/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.0469 - accuracy: 0.9823 - val_loss: 6.2372 - val_accuracy: 0.2734\n",
            "\n",
            "Epoch 00195: val_accuracy did not improve from 0.89655\n",
            "Epoch 196/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0487 - accuracy: 0.9854 - val_loss: 0.5741 - val_accuracy: 0.8941\n",
            "\n",
            "Epoch 00196: val_accuracy did not improve from 0.89655\n",
            "Epoch 197/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0391 - accuracy: 0.9878 - val_loss: 3.4727 - val_accuracy: 0.5591\n",
            "\n",
            "Epoch 00197: val_accuracy did not improve from 0.89655\n",
            "Epoch 198/500\n",
            "52/52 [==============================] - 38s 724ms/step - loss: 0.0600 - accuracy: 0.9829 - val_loss: 2.7048 - val_accuracy: 0.6207\n",
            "\n",
            "Epoch 00198: val_accuracy did not improve from 0.89655\n",
            "Epoch 199/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0862 - accuracy: 0.9750 - val_loss: 1.0431 - val_accuracy: 0.8153\n",
            "\n",
            "Epoch 00199: val_accuracy did not improve from 0.89655\n",
            "Epoch 200/500\n",
            "52/52 [==============================] - 37s 713ms/step - loss: 0.0675 - accuracy: 0.9744 - val_loss: 7.0326 - val_accuracy: 0.1429\n",
            "\n",
            "Epoch 00200: val_accuracy did not improve from 0.89655\n",
            "Epoch 201/500\n",
            "52/52 [==============================] - 37s 713ms/step - loss: 0.0939 - accuracy: 0.9695 - val_loss: 6.6413 - val_accuracy: 0.1379\n",
            "\n",
            "Epoch 00201: val_accuracy did not improve from 0.89655\n",
            "Epoch 202/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.0700 - accuracy: 0.9805 - val_loss: 2.4446 - val_accuracy: 0.5887\n",
            "\n",
            "Epoch 00202: val_accuracy did not improve from 0.89655\n",
            "Epoch 203/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0728 - accuracy: 0.9805 - val_loss: 0.9984 - val_accuracy: 0.8177\n",
            "\n",
            "Epoch 00203: val_accuracy did not improve from 0.89655\n",
            "Epoch 204/500\n",
            "52/52 [==============================] - 38s 719ms/step - loss: 0.0474 - accuracy: 0.9866 - val_loss: 6.2480 - val_accuracy: 0.1626\n",
            "\n",
            "Epoch 00204: val_accuracy did not improve from 0.89655\n",
            "Epoch 205/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0406 - accuracy: 0.9878 - val_loss: 1.6923 - val_accuracy: 0.7069\n",
            "\n",
            "Epoch 00205: val_accuracy did not improve from 0.89655\n",
            "Epoch 206/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0280 - accuracy: 0.9921 - val_loss: 1.6000 - val_accuracy: 0.7143\n",
            "\n",
            "Epoch 00206: val_accuracy did not improve from 0.89655\n",
            "Epoch 207/500\n",
            "52/52 [==============================] - 37s 716ms/step - loss: 0.0441 - accuracy: 0.9866 - val_loss: 0.7016 - val_accuracy: 0.8695\n",
            "\n",
            "Epoch 00207: val_accuracy did not improve from 0.89655\n",
            "Epoch 208/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0370 - accuracy: 0.9884 - val_loss: 0.7769 - val_accuracy: 0.8498\n",
            "\n",
            "Epoch 00208: val_accuracy did not improve from 0.89655\n",
            "Epoch 209/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.0427 - accuracy: 0.9878 - val_loss: 0.7705 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00209: val_accuracy did not improve from 0.89655\n",
            "Epoch 210/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0226 - accuracy: 0.9933 - val_loss: 0.6995 - val_accuracy: 0.8670\n",
            "\n",
            "Epoch 00210: val_accuracy did not improve from 0.89655\n",
            "Epoch 211/500\n",
            "52/52 [==============================] - 37s 716ms/step - loss: 0.0135 - accuracy: 0.9957 - val_loss: 0.5292 - val_accuracy: 0.8818\n",
            "\n",
            "Epoch 00211: val_accuracy did not improve from 0.89655\n",
            "Epoch 212/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0280 - accuracy: 0.9896 - val_loss: 0.6868 - val_accuracy: 0.8719\n",
            "\n",
            "Epoch 00212: val_accuracy did not improve from 0.89655\n",
            "Epoch 213/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.0488 - accuracy: 0.9866 - val_loss: 0.6739 - val_accuracy: 0.8744\n",
            "\n",
            "Epoch 00213: val_accuracy did not improve from 0.89655\n",
            "Epoch 214/500\n",
            "52/52 [==============================] - 38s 722ms/step - loss: 0.1028 - accuracy: 0.9708 - val_loss: 4.6308 - val_accuracy: 0.4877\n",
            "\n",
            "Epoch 00214: val_accuracy did not improve from 0.89655\n",
            "Epoch 215/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.0718 - accuracy: 0.9762 - val_loss: 0.7238 - val_accuracy: 0.8818\n",
            "\n",
            "Epoch 00215: val_accuracy did not improve from 0.89655\n",
            "Epoch 216/500\n",
            "52/52 [==============================] - 37s 715ms/step - loss: 0.0276 - accuracy: 0.9896 - val_loss: 0.5838 - val_accuracy: 0.8941\n",
            "\n",
            "Epoch 00216: val_accuracy did not improve from 0.89655\n",
            "Epoch 217/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.0348 - accuracy: 0.9872 - val_loss: 0.7613 - val_accuracy: 0.8670\n",
            "\n",
            "Epoch 00217: val_accuracy did not improve from 0.89655\n",
            "Epoch 218/500\n",
            "52/52 [==============================] - 38s 722ms/step - loss: 0.0512 - accuracy: 0.9848 - val_loss: 7.6845 - val_accuracy: 0.1182\n",
            "\n",
            "Epoch 00218: val_accuracy did not improve from 0.89655\n",
            "Epoch 219/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0320 - accuracy: 0.9878 - val_loss: 4.6942 - val_accuracy: 0.3670\n",
            "\n",
            "Epoch 00219: val_accuracy did not improve from 0.89655\n",
            "Epoch 220/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0171 - accuracy: 0.9933 - val_loss: 0.5581 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00220: val_accuracy improved from 0.89655 to 0.90887, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_EfficientNetB5.h5\n",
            "Epoch 221/500\n",
            "52/52 [==============================] - 38s 719ms/step - loss: 0.0073 - accuracy: 0.9970 - val_loss: 0.5252 - val_accuracy: 0.9039\n",
            "\n",
            "Epoch 00221: val_accuracy did not improve from 0.90887\n",
            "Epoch 222/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0299 - accuracy: 0.9933 - val_loss: 0.6031 - val_accuracy: 0.9039\n",
            "\n",
            "Epoch 00222: val_accuracy did not improve from 0.90887\n",
            "Epoch 223/500\n",
            "52/52 [==============================] - 38s 723ms/step - loss: 0.0338 - accuracy: 0.9890 - val_loss: 0.7083 - val_accuracy: 0.8744\n",
            "\n",
            "Epoch 00223: val_accuracy did not improve from 0.90887\n",
            "Epoch 224/500\n",
            "52/52 [==============================] - 37s 716ms/step - loss: 0.0490 - accuracy: 0.9829 - val_loss: 0.6740 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00224: val_accuracy did not improve from 0.90887\n",
            "Epoch 225/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0278 - accuracy: 0.9896 - val_loss: 2.6957 - val_accuracy: 0.6626\n",
            "\n",
            "Epoch 00225: val_accuracy did not improve from 0.90887\n",
            "Epoch 226/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.1000 - accuracy: 0.9720 - val_loss: 4.2230 - val_accuracy: 0.5099\n",
            "\n",
            "Epoch 00226: val_accuracy did not improve from 0.90887\n",
            "Epoch 227/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0567 - accuracy: 0.9836 - val_loss: 9.5936 - val_accuracy: 0.1034\n",
            "\n",
            "Epoch 00227: val_accuracy did not improve from 0.90887\n",
            "Epoch 228/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0734 - accuracy: 0.9769 - val_loss: 7.8137 - val_accuracy: 0.1823\n",
            "\n",
            "Epoch 00228: val_accuracy did not improve from 0.90887\n",
            "Epoch 229/500\n",
            "52/52 [==============================] - 37s 716ms/step - loss: 0.0559 - accuracy: 0.9829 - val_loss: 7.5524 - val_accuracy: 0.2291\n",
            "\n",
            "Epoch 00229: val_accuracy did not improve from 0.90887\n",
            "Epoch 230/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0289 - accuracy: 0.9878 - val_loss: 0.6562 - val_accuracy: 0.8768\n",
            "\n",
            "Epoch 00230: val_accuracy did not improve from 0.90887\n",
            "Epoch 231/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0307 - accuracy: 0.9872 - val_loss: 0.6268 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00231: val_accuracy did not improve from 0.90887\n",
            "Epoch 232/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0432 - accuracy: 0.9866 - val_loss: 8.5745 - val_accuracy: 0.1232\n",
            "\n",
            "Epoch 00232: val_accuracy did not improve from 0.90887\n",
            "Epoch 233/500\n",
            "52/52 [==============================] - 38s 722ms/step - loss: 0.0598 - accuracy: 0.9842 - val_loss: 8.5646 - val_accuracy: 0.1675\n",
            "\n",
            "Epoch 00233: val_accuracy did not improve from 0.90887\n",
            "Epoch 234/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0208 - accuracy: 0.9927 - val_loss: 7.8374 - val_accuracy: 0.1749\n",
            "\n",
            "Epoch 00234: val_accuracy did not improve from 0.90887\n",
            "Epoch 235/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.0153 - accuracy: 0.9933 - val_loss: 4.1275 - val_accuracy: 0.4828\n",
            "\n",
            "Epoch 00235: val_accuracy did not improve from 0.90887\n",
            "Epoch 236/500\n",
            "52/52 [==============================] - 38s 722ms/step - loss: 0.0518 - accuracy: 0.9866 - val_loss: 0.4812 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00236: val_accuracy did not improve from 0.90887\n",
            "Epoch 237/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0373 - accuracy: 0.9884 - val_loss: 0.4228 - val_accuracy: 0.9015\n",
            "\n",
            "Epoch 00237: val_accuracy did not improve from 0.90887\n",
            "Epoch 238/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0296 - accuracy: 0.9921 - val_loss: 0.5407 - val_accuracy: 0.9015\n",
            "\n",
            "Epoch 00238: val_accuracy did not improve from 0.90887\n",
            "Epoch 239/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0120 - accuracy: 0.9927 - val_loss: 1.2822 - val_accuracy: 0.7783\n",
            "\n",
            "Epoch 00239: val_accuracy did not improve from 0.90887\n",
            "Epoch 240/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0220 - accuracy: 0.9921 - val_loss: 0.5005 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00240: val_accuracy did not improve from 0.90887\n",
            "Epoch 241/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.0066 - accuracy: 0.9994 - val_loss: 0.6637 - val_accuracy: 0.8719\n",
            "\n",
            "Epoch 00241: val_accuracy did not improve from 0.90887\n",
            "Epoch 242/500\n",
            "52/52 [==============================] - 37s 714ms/step - loss: 0.0192 - accuracy: 0.9945 - val_loss: 1.0754 - val_accuracy: 0.8399\n",
            "\n",
            "Epoch 00242: val_accuracy did not improve from 0.90887\n",
            "Epoch 243/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0238 - accuracy: 0.9903 - val_loss: 0.5621 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00243: val_accuracy did not improve from 0.90887\n",
            "Epoch 244/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0470 - accuracy: 0.9866 - val_loss: 0.6742 - val_accuracy: 0.8818\n",
            "\n",
            "Epoch 00244: val_accuracy did not improve from 0.90887\n",
            "Epoch 245/500\n",
            "52/52 [==============================] - 38s 725ms/step - loss: 0.0220 - accuracy: 0.9909 - val_loss: 0.5275 - val_accuracy: 0.9015\n",
            "\n",
            "Epoch 00245: val_accuracy did not improve from 0.90887\n",
            "Epoch 246/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0230 - accuracy: 0.9915 - val_loss: 0.5781 - val_accuracy: 0.9015\n",
            "\n",
            "Epoch 00246: val_accuracy did not improve from 0.90887\n",
            "Epoch 247/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.0441 - accuracy: 0.9896 - val_loss: 0.6743 - val_accuracy: 0.8744\n",
            "\n",
            "Epoch 00247: val_accuracy did not improve from 0.90887\n",
            "Epoch 248/500\n",
            "52/52 [==============================] - 38s 725ms/step - loss: 0.0502 - accuracy: 0.9842 - val_loss: 7.6590 - val_accuracy: 0.2783\n",
            "\n",
            "Epoch 00248: val_accuracy did not improve from 0.90887\n",
            "Epoch 249/500\n",
            "52/52 [==============================] - 37s 716ms/step - loss: 0.0568 - accuracy: 0.9829 - val_loss: 1.6381 - val_accuracy: 0.7167\n",
            "\n",
            "Epoch 00249: val_accuracy did not improve from 0.90887\n",
            "Epoch 250/500\n",
            "52/52 [==============================] - 37s 716ms/step - loss: 0.0462 - accuracy: 0.9878 - val_loss: 2.1946 - val_accuracy: 0.6798\n",
            "\n",
            "Epoch 00250: val_accuracy did not improve from 0.90887\n",
            "Epoch 251/500\n",
            "52/52 [==============================] - 38s 722ms/step - loss: 0.0460 - accuracy: 0.9823 - val_loss: 1.1184 - val_accuracy: 0.8030\n",
            "\n",
            "Epoch 00251: val_accuracy did not improve from 0.90887\n",
            "Epoch 252/500\n",
            "52/52 [==============================] - 37s 715ms/step - loss: 0.0550 - accuracy: 0.9829 - val_loss: 6.4442 - val_accuracy: 0.2660\n",
            "\n",
            "Epoch 00252: val_accuracy did not improve from 0.90887\n",
            "Epoch 253/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0297 - accuracy: 0.9903 - val_loss: 6.2643 - val_accuracy: 0.2635\n",
            "\n",
            "Epoch 00253: val_accuracy did not improve from 0.90887\n",
            "Epoch 254/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0209 - accuracy: 0.9915 - val_loss: 1.4415 - val_accuracy: 0.7340\n",
            "\n",
            "Epoch 00254: val_accuracy did not improve from 0.90887\n",
            "Epoch 255/500\n",
            "52/52 [==============================] - 37s 720ms/step - loss: 0.0147 - accuracy: 0.9957 - val_loss: 0.5070 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00255: val_accuracy did not improve from 0.90887\n",
            "Epoch 256/500\n",
            "52/52 [==============================] - 37s 720ms/step - loss: 0.0158 - accuracy: 0.9945 - val_loss: 0.5520 - val_accuracy: 0.8793\n",
            "\n",
            "Epoch 00256: val_accuracy did not improve from 0.90887\n",
            "Epoch 257/500\n",
            "52/52 [==============================] - 37s 715ms/step - loss: 0.0137 - accuracy: 0.9945 - val_loss: 0.4422 - val_accuracy: 0.9163\n",
            "\n",
            "Epoch 00257: val_accuracy improved from 0.90887 to 0.91626, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_EfficientNetB5.h5\n",
            "Epoch 258/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0358 - accuracy: 0.9909 - val_loss: 7.0878 - val_accuracy: 0.3128\n",
            "\n",
            "Epoch 00258: val_accuracy did not improve from 0.91626\n",
            "Epoch 259/500\n",
            "52/52 [==============================] - 38s 723ms/step - loss: 0.0228 - accuracy: 0.9939 - val_loss: 0.8200 - val_accuracy: 0.8424\n",
            "\n",
            "Epoch 00259: val_accuracy did not improve from 0.91626\n",
            "Epoch 260/500\n",
            "52/52 [==============================] - 37s 715ms/step - loss: 0.0261 - accuracy: 0.9909 - val_loss: 0.6312 - val_accuracy: 0.8719\n",
            "\n",
            "Epoch 00260: val_accuracy did not improve from 0.91626\n",
            "Epoch 261/500\n",
            "52/52 [==============================] - 37s 714ms/step - loss: 0.0338 - accuracy: 0.9896 - val_loss: 1.6401 - val_accuracy: 0.7241\n",
            "\n",
            "Epoch 00261: val_accuracy did not improve from 0.91626\n",
            "Epoch 262/500\n",
            "52/52 [==============================] - 38s 722ms/step - loss: 0.0670 - accuracy: 0.9829 - val_loss: 2.1658 - val_accuracy: 0.6921\n",
            "\n",
            "Epoch 00262: val_accuracy did not improve from 0.91626\n",
            "Epoch 263/500\n",
            "52/52 [==============================] - 37s 714ms/step - loss: 0.0494 - accuracy: 0.9823 - val_loss: 1.9586 - val_accuracy: 0.7217\n",
            "\n",
            "Epoch 00263: val_accuracy did not improve from 0.91626\n",
            "Epoch 264/500\n",
            "52/52 [==============================] - 37s 716ms/step - loss: 0.0887 - accuracy: 0.9775 - val_loss: 2.0148 - val_accuracy: 0.7365\n",
            "\n",
            "Epoch 00264: val_accuracy did not improve from 0.91626\n",
            "Epoch 265/500\n",
            "52/52 [==============================] - 38s 719ms/step - loss: 0.0553 - accuracy: 0.9787 - val_loss: 2.2077 - val_accuracy: 0.7241\n",
            "\n",
            "Epoch 00265: val_accuracy did not improve from 0.91626\n",
            "Epoch 266/500\n",
            "52/52 [==============================] - 37s 716ms/step - loss: 0.0509 - accuracy: 0.9860 - val_loss: 0.6076 - val_accuracy: 0.8793\n",
            "\n",
            "Epoch 00266: val_accuracy did not improve from 0.91626\n",
            "Epoch 267/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0307 - accuracy: 0.9884 - val_loss: 0.6757 - val_accuracy: 0.8768\n",
            "\n",
            "Epoch 00267: val_accuracy did not improve from 0.91626\n",
            "Epoch 268/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0349 - accuracy: 0.9878 - val_loss: 0.7735 - val_accuracy: 0.8498\n",
            "\n",
            "Epoch 00268: val_accuracy did not improve from 0.91626\n",
            "Epoch 269/500\n",
            "52/52 [==============================] - 38s 724ms/step - loss: 0.0430 - accuracy: 0.9866 - val_loss: 8.3733 - val_accuracy: 0.2167\n",
            "\n",
            "Epoch 00269: val_accuracy did not improve from 0.91626\n",
            "Epoch 270/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0255 - accuracy: 0.9933 - val_loss: 2.2825 - val_accuracy: 0.6552\n",
            "\n",
            "Epoch 00270: val_accuracy did not improve from 0.91626\n",
            "Epoch 271/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0236 - accuracy: 0.9909 - val_loss: 3.1145 - val_accuracy: 0.6379\n",
            "\n",
            "Epoch 00271: val_accuracy did not improve from 0.91626\n",
            "Epoch 272/500\n",
            "52/52 [==============================] - 38s 723ms/step - loss: 0.0319 - accuracy: 0.9921 - val_loss: 1.5548 - val_accuracy: 0.7537\n",
            "\n",
            "Epoch 00272: val_accuracy did not improve from 0.91626\n",
            "Epoch 273/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0255 - accuracy: 0.9915 - val_loss: 9.6608 - val_accuracy: 0.1281\n",
            "\n",
            "Epoch 00273: val_accuracy did not improve from 0.91626\n",
            "Epoch 274/500\n",
            "52/52 [==============================] - 38s 725ms/step - loss: 0.0214 - accuracy: 0.9933 - val_loss: 1.9262 - val_accuracy: 0.6404\n",
            "\n",
            "Epoch 00274: val_accuracy did not improve from 0.91626\n",
            "Epoch 275/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0081 - accuracy: 0.9976 - val_loss: 1.5552 - val_accuracy: 0.7931\n",
            "\n",
            "Epoch 00275: val_accuracy did not improve from 0.91626\n",
            "Epoch 276/500\n",
            "52/52 [==============================] - 37s 715ms/step - loss: 0.0057 - accuracy: 0.9982 - val_loss: 1.4055 - val_accuracy: 0.7857\n",
            "\n",
            "Epoch 00276: val_accuracy did not improve from 0.91626\n",
            "Epoch 277/500\n",
            "52/52 [==============================] - 37s 716ms/step - loss: 0.0250 - accuracy: 0.9939 - val_loss: 0.6213 - val_accuracy: 0.8941\n",
            "\n",
            "Epoch 00277: val_accuracy did not improve from 0.91626\n",
            "Epoch 278/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0575 - accuracy: 0.9823 - val_loss: 6.9679 - val_accuracy: 0.2709\n",
            "\n",
            "Epoch 00278: val_accuracy did not improve from 0.91626\n",
            "Epoch 279/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0698 - accuracy: 0.9787 - val_loss: 0.6082 - val_accuracy: 0.8768\n",
            "\n",
            "Epoch 00279: val_accuracy did not improve from 0.91626\n",
            "Epoch 280/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.0572 - accuracy: 0.9793 - val_loss: 4.8290 - val_accuracy: 0.4557\n",
            "\n",
            "Epoch 00280: val_accuracy did not improve from 0.91626\n",
            "Epoch 281/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0497 - accuracy: 0.9854 - val_loss: 0.6520 - val_accuracy: 0.8670\n",
            "\n",
            "Epoch 00281: val_accuracy did not improve from 0.91626\n",
            "Epoch 282/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0270 - accuracy: 0.9921 - val_loss: 0.4595 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00282: val_accuracy did not improve from 0.91626\n",
            "Epoch 283/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0250 - accuracy: 0.9909 - val_loss: 0.4869 - val_accuracy: 0.9064\n",
            "\n",
            "Epoch 00283: val_accuracy did not improve from 0.91626\n",
            "Epoch 284/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0621 - accuracy: 0.9817 - val_loss: 2.4837 - val_accuracy: 0.6700\n",
            "\n",
            "Epoch 00284: val_accuracy did not improve from 0.91626\n",
            "Epoch 285/500\n",
            "52/52 [==============================] - 38s 725ms/step - loss: 0.0347 - accuracy: 0.9890 - val_loss: 3.3672 - val_accuracy: 0.5616\n",
            "\n",
            "Epoch 00285: val_accuracy did not improve from 0.91626\n",
            "Epoch 286/500\n",
            "52/52 [==============================] - 37s 715ms/step - loss: 0.0196 - accuracy: 0.9933 - val_loss: 3.9013 - val_accuracy: 0.5690\n",
            "\n",
            "Epoch 00286: val_accuracy did not improve from 0.91626\n",
            "Epoch 287/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0110 - accuracy: 0.9970 - val_loss: 0.7500 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00287: val_accuracy did not improve from 0.91626\n",
            "Epoch 288/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0346 - accuracy: 0.9896 - val_loss: 0.7295 - val_accuracy: 0.8621\n",
            "\n",
            "Epoch 00288: val_accuracy did not improve from 0.91626\n",
            "Epoch 289/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0268 - accuracy: 0.9909 - val_loss: 0.6841 - val_accuracy: 0.8941\n",
            "\n",
            "Epoch 00289: val_accuracy did not improve from 0.91626\n",
            "Epoch 290/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.0475 - accuracy: 0.9842 - val_loss: 0.8736 - val_accuracy: 0.8596\n",
            "\n",
            "Epoch 00290: val_accuracy did not improve from 0.91626\n",
            "Epoch 291/500\n",
            "52/52 [==============================] - 37s 715ms/step - loss: 0.0397 - accuracy: 0.9878 - val_loss: 1.5474 - val_accuracy: 0.7882\n",
            "\n",
            "Epoch 00291: val_accuracy did not improve from 0.91626\n",
            "Epoch 292/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0262 - accuracy: 0.9921 - val_loss: 0.7627 - val_accuracy: 0.8793\n",
            "\n",
            "Epoch 00292: val_accuracy did not improve from 0.91626\n",
            "Epoch 293/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0087 - accuracy: 0.9982 - val_loss: 0.6255 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00293: val_accuracy did not improve from 0.91626\n",
            "Epoch 294/500\n",
            "52/52 [==============================] - 38s 731ms/step - loss: 0.0228 - accuracy: 0.9921 - val_loss: 0.6318 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00294: val_accuracy did not improve from 0.91626\n",
            "Epoch 295/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0501 - accuracy: 0.9915 - val_loss: 3.2014 - val_accuracy: 0.6527\n",
            "\n",
            "Epoch 00295: val_accuracy did not improve from 0.91626\n",
            "Epoch 296/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0173 - accuracy: 0.9939 - val_loss: 3.2495 - val_accuracy: 0.5961\n",
            "\n",
            "Epoch 00296: val_accuracy did not improve from 0.91626\n",
            "Epoch 297/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0216 - accuracy: 0.9933 - val_loss: 8.5386 - val_accuracy: 0.1650\n",
            "\n",
            "Epoch 00297: val_accuracy did not improve from 0.91626\n",
            "Epoch 298/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.0361 - accuracy: 0.9860 - val_loss: 2.2667 - val_accuracy: 0.7094\n",
            "\n",
            "Epoch 00298: val_accuracy did not improve from 0.91626\n",
            "Epoch 299/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.0450 - accuracy: 0.9866 - val_loss: 0.5943 - val_accuracy: 0.8892\n",
            "\n",
            "Epoch 00299: val_accuracy did not improve from 0.91626\n",
            "Epoch 300/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0298 - accuracy: 0.9921 - val_loss: 10.3000 - val_accuracy: 0.1330\n",
            "\n",
            "Epoch 00300: val_accuracy did not improve from 0.91626\n",
            "Epoch 301/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0152 - accuracy: 0.9945 - val_loss: 4.9406 - val_accuracy: 0.4631\n",
            "\n",
            "Epoch 00301: val_accuracy did not improve from 0.91626\n",
            "Epoch 302/500\n",
            "52/52 [==============================] - 37s 715ms/step - loss: 0.0261 - accuracy: 0.9927 - val_loss: 0.5747 - val_accuracy: 0.8941\n",
            "\n",
            "Epoch 00302: val_accuracy did not improve from 0.91626\n",
            "Epoch 303/500\n",
            "52/52 [==============================] - 38s 727ms/step - loss: 0.0366 - accuracy: 0.9878 - val_loss: 9.5589 - val_accuracy: 0.1108\n",
            "\n",
            "Epoch 00303: val_accuracy did not improve from 0.91626\n",
            "Epoch 304/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.0354 - accuracy: 0.9884 - val_loss: 6.6551 - val_accuracy: 0.2020\n",
            "\n",
            "Epoch 00304: val_accuracy did not improve from 0.91626\n",
            "Epoch 305/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0218 - accuracy: 0.9921 - val_loss: 2.4566 - val_accuracy: 0.6232\n",
            "\n",
            "Epoch 00305: val_accuracy did not improve from 0.91626\n",
            "Epoch 306/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0248 - accuracy: 0.9921 - val_loss: 0.6876 - val_accuracy: 0.8596\n",
            "\n",
            "Epoch 00306: val_accuracy did not improve from 0.91626\n",
            "Epoch 307/500\n",
            "52/52 [==============================] - 37s 715ms/step - loss: 0.0138 - accuracy: 0.9951 - val_loss: 0.8625 - val_accuracy: 0.8547\n",
            "\n",
            "Epoch 00307: val_accuracy did not improve from 0.91626\n",
            "Epoch 308/500\n",
            "52/52 [==============================] - 37s 715ms/step - loss: 0.0086 - accuracy: 0.9970 - val_loss: 0.6004 - val_accuracy: 0.8744\n",
            "\n",
            "Epoch 00308: val_accuracy did not improve from 0.91626\n",
            "Epoch 309/500\n",
            "52/52 [==============================] - 37s 716ms/step - loss: 0.0184 - accuracy: 0.9945 - val_loss: 0.5456 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00309: val_accuracy did not improve from 0.91626\n",
            "Epoch 310/500\n",
            "52/52 [==============================] - 38s 723ms/step - loss: 0.0313 - accuracy: 0.9915 - val_loss: 0.5812 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00310: val_accuracy did not improve from 0.91626\n",
            "Epoch 311/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0187 - accuracy: 0.9945 - val_loss: 0.5788 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00311: val_accuracy did not improve from 0.91626\n",
            "Epoch 312/500\n",
            "52/52 [==============================] - 37s 726ms/step - loss: 0.0387 - accuracy: 0.9890 - val_loss: 0.6902 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00312: val_accuracy did not improve from 0.91626\n",
            "Epoch 313/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0624 - accuracy: 0.9805 - val_loss: 0.6775 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00313: val_accuracy did not improve from 0.91626\n",
            "Epoch 314/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0411 - accuracy: 0.9860 - val_loss: 0.5441 - val_accuracy: 0.9039\n",
            "\n",
            "Epoch 00314: val_accuracy did not improve from 0.91626\n",
            "Epoch 315/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0188 - accuracy: 0.9939 - val_loss: 0.6773 - val_accuracy: 0.8892\n",
            "\n",
            "Epoch 00315: val_accuracy did not improve from 0.91626\n",
            "Epoch 316/500\n",
            "52/52 [==============================] - 38s 723ms/step - loss: 0.0291 - accuracy: 0.9909 - val_loss: 0.4777 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00316: val_accuracy did not improve from 0.91626\n",
            "Epoch 317/500\n",
            "52/52 [==============================] - 38s 732ms/step - loss: 0.0358 - accuracy: 0.9872 - val_loss: 3.2109 - val_accuracy: 0.5813\n",
            "\n",
            "Epoch 00317: val_accuracy did not improve from 0.91626\n",
            "Epoch 318/500\n",
            "52/52 [==============================] - 38s 723ms/step - loss: 0.0297 - accuracy: 0.9915 - val_loss: 0.4841 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00318: val_accuracy did not improve from 0.91626\n",
            "Epoch 319/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0176 - accuracy: 0.9939 - val_loss: 7.5241 - val_accuracy: 0.2118\n",
            "\n",
            "Epoch 00319: val_accuracy did not improve from 0.91626\n",
            "Epoch 320/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0116 - accuracy: 0.9945 - val_loss: 0.5485 - val_accuracy: 0.9039\n",
            "\n",
            "Epoch 00320: val_accuracy did not improve from 0.91626\n",
            "Epoch 321/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0364 - accuracy: 0.9878 - val_loss: 0.9213 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00321: val_accuracy did not improve from 0.91626\n",
            "Epoch 322/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.0989 - accuracy: 0.9769 - val_loss: 0.6838 - val_accuracy: 0.8596\n",
            "\n",
            "Epoch 00322: val_accuracy did not improve from 0.91626\n",
            "Epoch 323/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0436 - accuracy: 0.9848 - val_loss: 0.6390 - val_accuracy: 0.8818\n",
            "\n",
            "Epoch 00323: val_accuracy did not improve from 0.91626\n",
            "Epoch 324/500\n",
            "52/52 [==============================] - 38s 732ms/step - loss: 0.0174 - accuracy: 0.9939 - val_loss: 0.5537 - val_accuracy: 0.9039\n",
            "\n",
            "Epoch 00324: val_accuracy did not improve from 0.91626\n",
            "Epoch 325/500\n",
            "52/52 [==============================] - 38s 722ms/step - loss: 0.0278 - accuracy: 0.9915 - val_loss: 0.5702 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00325: val_accuracy did not improve from 0.91626\n",
            "Epoch 326/500\n",
            "52/52 [==============================] - 38s 724ms/step - loss: 0.0154 - accuracy: 0.9951 - val_loss: 0.6971 - val_accuracy: 0.8670\n",
            "\n",
            "Epoch 00326: val_accuracy did not improve from 0.91626\n",
            "Epoch 327/500\n",
            "52/52 [==============================] - 37s 716ms/step - loss: 0.0355 - accuracy: 0.9921 - val_loss: 1.1070 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00327: val_accuracy did not improve from 0.91626\n",
            "Epoch 328/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0545 - accuracy: 0.9866 - val_loss: 0.9235 - val_accuracy: 0.8374\n",
            "\n",
            "Epoch 00328: val_accuracy did not improve from 0.91626\n",
            "Epoch 329/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0136 - accuracy: 0.9957 - val_loss: 0.6475 - val_accuracy: 0.8818\n",
            "\n",
            "Epoch 00329: val_accuracy did not improve from 0.91626\n",
            "Epoch 330/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0223 - accuracy: 0.9933 - val_loss: 0.7211 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00330: val_accuracy did not improve from 0.91626\n",
            "Epoch 331/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.0097 - accuracy: 0.9957 - val_loss: 1.0050 - val_accuracy: 0.8227\n",
            "\n",
            "Epoch 00331: val_accuracy did not improve from 0.91626\n",
            "Epoch 332/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.0093 - accuracy: 0.9970 - val_loss: 0.4752 - val_accuracy: 0.9163\n",
            "\n",
            "Epoch 00332: val_accuracy did not improve from 0.91626\n",
            "Epoch 333/500\n",
            "52/52 [==============================] - 38s 722ms/step - loss: 0.0135 - accuracy: 0.9945 - val_loss: 0.5774 - val_accuracy: 0.8768\n",
            "\n",
            "Epoch 00333: val_accuracy did not improve from 0.91626\n",
            "Epoch 334/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.0182 - accuracy: 0.9951 - val_loss: 0.5601 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00334: val_accuracy did not improve from 0.91626\n",
            "Epoch 335/500\n",
            "52/52 [==============================] - 37s 712ms/step - loss: 0.0148 - accuracy: 0.9945 - val_loss: 0.8953 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00335: val_accuracy did not improve from 0.91626\n",
            "Epoch 336/500\n",
            "52/52 [==============================] - 38s 719ms/step - loss: 0.0174 - accuracy: 0.9933 - val_loss: 0.5509 - val_accuracy: 0.9015\n",
            "\n",
            "Epoch 00336: val_accuracy did not improve from 0.91626\n",
            "Epoch 337/500\n",
            "52/52 [==============================] - 38s 724ms/step - loss: 0.0245 - accuracy: 0.9927 - val_loss: 0.4886 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00337: val_accuracy did not improve from 0.91626\n",
            "Epoch 338/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0079 - accuracy: 0.9976 - val_loss: 0.4744 - val_accuracy: 0.9212\n",
            "\n",
            "Epoch 00338: val_accuracy improved from 0.91626 to 0.92118, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_EfficientNetB5.h5\n",
            "Epoch 339/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.4287 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00339: val_accuracy did not improve from 0.92118\n",
            "Epoch 340/500\n",
            "52/52 [==============================] - 38s 725ms/step - loss: 0.0044 - accuracy: 0.9988 - val_loss: 0.4664 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00340: val_accuracy did not improve from 0.92118\n",
            "Epoch 341/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0096 - accuracy: 0.9976 - val_loss: 0.4038 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00341: val_accuracy did not improve from 0.92118\n",
            "Epoch 342/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0062 - accuracy: 0.9970 - val_loss: 0.4723 - val_accuracy: 0.9015\n",
            "\n",
            "Epoch 00342: val_accuracy did not improve from 0.92118\n",
            "Epoch 343/500\n",
            "52/52 [==============================] - 37s 720ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.4553 - val_accuracy: 0.9039\n",
            "\n",
            "Epoch 00343: val_accuracy did not improve from 0.92118\n",
            "Epoch 344/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0044 - accuracy: 0.9976 - val_loss: 0.4604 - val_accuracy: 0.9064\n",
            "\n",
            "Epoch 00344: val_accuracy did not improve from 0.92118\n",
            "Epoch 345/500\n",
            "52/52 [==============================] - 38s 719ms/step - loss: 0.0200 - accuracy: 0.9963 - val_loss: 0.7532 - val_accuracy: 0.8941\n",
            "\n",
            "Epoch 00345: val_accuracy did not improve from 0.92118\n",
            "Epoch 346/500\n",
            "52/52 [==============================] - 38s 722ms/step - loss: 0.0218 - accuracy: 0.9951 - val_loss: 0.7344 - val_accuracy: 0.8719\n",
            "\n",
            "Epoch 00346: val_accuracy did not improve from 0.92118\n",
            "Epoch 347/500\n",
            "52/52 [==============================] - 38s 722ms/step - loss: 0.0103 - accuracy: 0.9970 - val_loss: 6.7463 - val_accuracy: 0.2808\n",
            "\n",
            "Epoch 00347: val_accuracy did not improve from 0.92118\n",
            "Epoch 348/500\n",
            "52/52 [==============================] - 37s 715ms/step - loss: 0.0092 - accuracy: 0.9963 - val_loss: 0.5782 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00348: val_accuracy improved from 0.92118 to 0.92857, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_EfficientNetB5.h5\n",
            "Epoch 349/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.0083 - accuracy: 0.9976 - val_loss: 0.5881 - val_accuracy: 0.9015\n",
            "\n",
            "Epoch 00349: val_accuracy did not improve from 0.92857\n",
            "Epoch 350/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.0199 - accuracy: 0.9933 - val_loss: 3.7149 - val_accuracy: 0.5961\n",
            "\n",
            "Epoch 00350: val_accuracy did not improve from 0.92857\n",
            "Epoch 351/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.0163 - accuracy: 0.9963 - val_loss: 3.4503 - val_accuracy: 0.6084\n",
            "\n",
            "Epoch 00351: val_accuracy did not improve from 0.92857\n",
            "Epoch 352/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0166 - accuracy: 0.9933 - val_loss: 0.6244 - val_accuracy: 0.8916\n",
            "\n",
            "Epoch 00352: val_accuracy did not improve from 0.92857\n",
            "Epoch 353/500\n",
            "52/52 [==============================] - 38s 719ms/step - loss: 0.0157 - accuracy: 0.9945 - val_loss: 1.8288 - val_accuracy: 0.7586\n",
            "\n",
            "Epoch 00353: val_accuracy did not improve from 0.92857\n",
            "Epoch 354/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0438 - accuracy: 0.9860 - val_loss: 4.0289 - val_accuracy: 0.5320\n",
            "\n",
            "Epoch 00354: val_accuracy did not improve from 0.92857\n",
            "Epoch 355/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0376 - accuracy: 0.9878 - val_loss: 0.8849 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00355: val_accuracy did not improve from 0.92857\n",
            "Epoch 356/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0136 - accuracy: 0.9957 - val_loss: 4.4843 - val_accuracy: 0.5345\n",
            "\n",
            "Epoch 00356: val_accuracy did not improve from 0.92857\n",
            "Epoch 357/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0305 - accuracy: 0.9896 - val_loss: 1.4174 - val_accuracy: 0.7906\n",
            "\n",
            "Epoch 00357: val_accuracy did not improve from 0.92857\n",
            "Epoch 358/500\n",
            "52/52 [==============================] - 38s 723ms/step - loss: 0.0481 - accuracy: 0.9836 - val_loss: 7.9502 - val_accuracy: 0.2586\n",
            "\n",
            "Epoch 00358: val_accuracy did not improve from 0.92857\n",
            "Epoch 359/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0570 - accuracy: 0.9811 - val_loss: 14.8897 - val_accuracy: 0.1084\n",
            "\n",
            "Epoch 00359: val_accuracy did not improve from 0.92857\n",
            "Epoch 360/500\n",
            "52/52 [==============================] - 38s 724ms/step - loss: 0.0551 - accuracy: 0.9854 - val_loss: 4.7149 - val_accuracy: 0.4433\n",
            "\n",
            "Epoch 00360: val_accuracy did not improve from 0.92857\n",
            "Epoch 361/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0169 - accuracy: 0.9939 - val_loss: 3.0473 - val_accuracy: 0.5936\n",
            "\n",
            "Epoch 00361: val_accuracy did not improve from 0.92857\n",
            "Epoch 362/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0288 - accuracy: 0.9890 - val_loss: 0.5832 - val_accuracy: 0.9064\n",
            "\n",
            "Epoch 00362: val_accuracy did not improve from 0.92857\n",
            "Epoch 363/500\n",
            "52/52 [==============================] - 37s 715ms/step - loss: 0.0177 - accuracy: 0.9957 - val_loss: 0.6142 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00363: val_accuracy did not improve from 0.92857\n",
            "Epoch 364/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0241 - accuracy: 0.9921 - val_loss: 0.5641 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00364: val_accuracy did not improve from 0.92857\n",
            "Epoch 365/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.0242 - accuracy: 0.9921 - val_loss: 0.9359 - val_accuracy: 0.8645\n",
            "\n",
            "Epoch 00365: val_accuracy did not improve from 0.92857\n",
            "Epoch 366/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0536 - accuracy: 0.9854 - val_loss: 2.6705 - val_accuracy: 0.6527\n",
            "\n",
            "Epoch 00366: val_accuracy did not improve from 0.92857\n",
            "Epoch 367/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0373 - accuracy: 0.9866 - val_loss: 1.3935 - val_accuracy: 0.7956\n",
            "\n",
            "Epoch 00367: val_accuracy did not improve from 0.92857\n",
            "Epoch 368/500\n",
            "52/52 [==============================] - 38s 724ms/step - loss: 0.0148 - accuracy: 0.9970 - val_loss: 0.6551 - val_accuracy: 0.8916\n",
            "\n",
            "Epoch 00368: val_accuracy did not improve from 0.92857\n",
            "Epoch 369/500\n",
            "52/52 [==============================] - 38s 719ms/step - loss: 0.0098 - accuracy: 0.9976 - val_loss: 0.7224 - val_accuracy: 0.8916\n",
            "\n",
            "Epoch 00369: val_accuracy did not improve from 0.92857\n",
            "Epoch 370/500\n",
            "52/52 [==============================] - 37s 708ms/step - loss: 0.0276 - accuracy: 0.9921 - val_loss: 0.7455 - val_accuracy: 0.8892\n",
            "\n",
            "Epoch 00370: val_accuracy did not improve from 0.92857\n",
            "Epoch 371/500\n",
            "52/52 [==============================] - 38s 722ms/step - loss: 0.0106 - accuracy: 0.9957 - val_loss: 4.5216 - val_accuracy: 0.5172\n",
            "\n",
            "Epoch 00371: val_accuracy did not improve from 0.92857\n",
            "Epoch 372/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0103 - accuracy: 0.9957 - val_loss: 8.8280 - val_accuracy: 0.2931\n",
            "\n",
            "Epoch 00372: val_accuracy did not improve from 0.92857\n",
            "Epoch 373/500\n",
            "52/52 [==============================] - 38s 723ms/step - loss: 0.0108 - accuracy: 0.9970 - val_loss: 9.8466 - val_accuracy: 0.1675\n",
            "\n",
            "Epoch 00373: val_accuracy did not improve from 0.92857\n",
            "Epoch 374/500\n",
            "52/52 [==============================] - 37s 714ms/step - loss: 0.0185 - accuracy: 0.9927 - val_loss: 10.7908 - val_accuracy: 0.1576\n",
            "\n",
            "Epoch 00374: val_accuracy did not improve from 0.92857\n",
            "Epoch 375/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0312 - accuracy: 0.9933 - val_loss: 3.1728 - val_accuracy: 0.5665\n",
            "\n",
            "Epoch 00375: val_accuracy did not improve from 0.92857\n",
            "Epoch 376/500\n",
            "52/52 [==============================] - 38s 722ms/step - loss: 0.0284 - accuracy: 0.9921 - val_loss: 0.9467 - val_accuracy: 0.8350\n",
            "\n",
            "Epoch 00376: val_accuracy did not improve from 0.92857\n",
            "Epoch 377/500\n",
            "52/52 [==============================] - 38s 722ms/step - loss: 0.0318 - accuracy: 0.9933 - val_loss: 8.8714 - val_accuracy: 0.3251\n",
            "\n",
            "Epoch 00377: val_accuracy did not improve from 0.92857\n",
            "Epoch 378/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0606 - accuracy: 0.9799 - val_loss: 0.8931 - val_accuracy: 0.8768\n",
            "\n",
            "Epoch 00378: val_accuracy did not improve from 0.92857\n",
            "Epoch 379/500\n",
            "52/52 [==============================] - 37s 716ms/step - loss: 0.0284 - accuracy: 0.9903 - val_loss: 0.6950 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00379: val_accuracy did not improve from 0.92857\n",
            "Epoch 380/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0272 - accuracy: 0.9921 - val_loss: 0.8427 - val_accuracy: 0.8670\n",
            "\n",
            "Epoch 00380: val_accuracy did not improve from 0.92857\n",
            "Epoch 381/500\n",
            "52/52 [==============================] - 38s 719ms/step - loss: 0.0120 - accuracy: 0.9951 - val_loss: 0.6159 - val_accuracy: 0.8916\n",
            "\n",
            "Epoch 00381: val_accuracy did not improve from 0.92857\n",
            "Epoch 382/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.0217 - accuracy: 0.9921 - val_loss: 0.6366 - val_accuracy: 0.8818\n",
            "\n",
            "Epoch 00382: val_accuracy did not improve from 0.92857\n",
            "Epoch 383/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0261 - accuracy: 0.9903 - val_loss: 0.7074 - val_accuracy: 0.8941\n",
            "\n",
            "Epoch 00383: val_accuracy did not improve from 0.92857\n",
            "Epoch 384/500\n",
            "52/52 [==============================] - 38s 722ms/step - loss: 0.0323 - accuracy: 0.9915 - val_loss: 0.5797 - val_accuracy: 0.8892\n",
            "\n",
            "Epoch 00384: val_accuracy did not improve from 0.92857\n",
            "Epoch 385/500\n",
            "52/52 [==============================] - 37s 720ms/step - loss: 0.0204 - accuracy: 0.9933 - val_loss: 0.5107 - val_accuracy: 0.9015\n",
            "\n",
            "Epoch 00385: val_accuracy did not improve from 0.92857\n",
            "Epoch 386/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0221 - accuracy: 0.9933 - val_loss: 0.9139 - val_accuracy: 0.8473\n",
            "\n",
            "Epoch 00386: val_accuracy did not improve from 0.92857\n",
            "Epoch 387/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0147 - accuracy: 0.9951 - val_loss: 0.5302 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00387: val_accuracy did not improve from 0.92857\n",
            "Epoch 388/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0146 - accuracy: 0.9933 - val_loss: 0.5766 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00388: val_accuracy did not improve from 0.92857\n",
            "Epoch 389/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0190 - accuracy: 0.9957 - val_loss: 0.5466 - val_accuracy: 0.9015\n",
            "\n",
            "Epoch 00389: val_accuracy did not improve from 0.92857\n",
            "Epoch 390/500\n",
            "52/52 [==============================] - 37s 715ms/step - loss: 0.0131 - accuracy: 0.9970 - val_loss: 0.4426 - val_accuracy: 0.9163\n",
            "\n",
            "Epoch 00390: val_accuracy did not improve from 0.92857\n",
            "Epoch 391/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0080 - accuracy: 0.9970 - val_loss: 0.5467 - val_accuracy: 0.9064\n",
            "\n",
            "Epoch 00391: val_accuracy did not improve from 0.92857\n",
            "Epoch 392/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0211 - accuracy: 0.9945 - val_loss: 0.7951 - val_accuracy: 0.8719\n",
            "\n",
            "Epoch 00392: val_accuracy did not improve from 0.92857\n",
            "Epoch 393/500\n",
            "52/52 [==============================] - 38s 724ms/step - loss: 0.0504 - accuracy: 0.9909 - val_loss: 1.1910 - val_accuracy: 0.8374\n",
            "\n",
            "Epoch 00393: val_accuracy did not improve from 0.92857\n",
            "Epoch 394/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0298 - accuracy: 0.9890 - val_loss: 0.7704 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00394: val_accuracy did not improve from 0.92857\n",
            "Epoch 395/500\n",
            "52/52 [==============================] - 37s 716ms/step - loss: 0.0358 - accuracy: 0.9903 - val_loss: 0.6485 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00395: val_accuracy did not improve from 0.92857\n",
            "Epoch 396/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0259 - accuracy: 0.9921 - val_loss: 0.6905 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00396: val_accuracy did not improve from 0.92857\n",
            "Epoch 397/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0105 - accuracy: 0.9976 - val_loss: 0.6381 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00397: val_accuracy did not improve from 0.92857\n",
            "Epoch 398/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0332 - accuracy: 0.9909 - val_loss: 0.6523 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00398: val_accuracy did not improve from 0.92857\n",
            "Epoch 399/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0227 - accuracy: 0.9939 - val_loss: 0.7069 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00399: val_accuracy did not improve from 0.92857\n",
            "Epoch 400/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.0267 - accuracy: 0.9903 - val_loss: 0.8204 - val_accuracy: 0.8818\n",
            "\n",
            "Epoch 00400: val_accuracy did not improve from 0.92857\n",
            "Epoch 401/500\n",
            "52/52 [==============================] - 37s 715ms/step - loss: 0.0158 - accuracy: 0.9951 - val_loss: 0.6336 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00401: val_accuracy did not improve from 0.92857\n",
            "Epoch 402/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0162 - accuracy: 0.9951 - val_loss: 0.5608 - val_accuracy: 0.9113\n",
            "\n",
            "Epoch 00402: val_accuracy did not improve from 0.92857\n",
            "Epoch 403/500\n",
            "52/52 [==============================] - 38s 724ms/step - loss: 0.0098 - accuracy: 0.9976 - val_loss: 0.6094 - val_accuracy: 0.8818\n",
            "\n",
            "Epoch 00403: val_accuracy did not improve from 0.92857\n",
            "Epoch 404/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0107 - accuracy: 0.9976 - val_loss: 0.6319 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00404: val_accuracy did not improve from 0.92857\n",
            "Epoch 405/500\n",
            "52/52 [==============================] - 37s 715ms/step - loss: 0.0063 - accuracy: 0.9970 - val_loss: 0.4856 - val_accuracy: 0.9113\n",
            "\n",
            "Epoch 00405: val_accuracy did not improve from 0.92857\n",
            "Epoch 406/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.0060 - accuracy: 0.9976 - val_loss: 0.5514 - val_accuracy: 0.9113\n",
            "\n",
            "Epoch 00406: val_accuracy did not improve from 0.92857\n",
            "Epoch 407/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0047 - accuracy: 0.9988 - val_loss: 0.4621 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00407: val_accuracy did not improve from 0.92857\n",
            "Epoch 408/500\n",
            "52/52 [==============================] - 37s 713ms/step - loss: 0.0087 - accuracy: 0.9970 - val_loss: 0.5216 - val_accuracy: 0.9138\n",
            "\n",
            "Epoch 00408: val_accuracy did not improve from 0.92857\n",
            "Epoch 409/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.0123 - accuracy: 0.9970 - val_loss: 0.5398 - val_accuracy: 0.9064\n",
            "\n",
            "Epoch 00409: val_accuracy did not improve from 0.92857\n",
            "Epoch 410/500\n",
            "52/52 [==============================] - 38s 719ms/step - loss: 0.0027 - accuracy: 0.9988 - val_loss: 0.5703 - val_accuracy: 0.9039\n",
            "\n",
            "Epoch 00410: val_accuracy did not improve from 0.92857\n",
            "Epoch 411/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0023 - accuracy: 0.9988 - val_loss: 0.4941 - val_accuracy: 0.9163\n",
            "\n",
            "Epoch 00411: val_accuracy did not improve from 0.92857\n",
            "Epoch 412/500\n",
            "52/52 [==============================] - 37s 716ms/step - loss: 0.0069 - accuracy: 0.9988 - val_loss: 0.5152 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00412: val_accuracy did not improve from 0.92857\n",
            "Epoch 413/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0062 - accuracy: 0.9982 - val_loss: 0.6389 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00413: val_accuracy did not improve from 0.92857\n",
            "Epoch 414/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0151 - accuracy: 0.9970 - val_loss: 0.6647 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00414: val_accuracy did not improve from 0.92857\n",
            "Epoch 415/500\n",
            "52/52 [==============================] - 37s 715ms/step - loss: 0.0046 - accuracy: 0.9988 - val_loss: 0.6901 - val_accuracy: 0.9015\n",
            "\n",
            "Epoch 00415: val_accuracy did not improve from 0.92857\n",
            "Epoch 416/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0131 - accuracy: 0.9970 - val_loss: 0.9798 - val_accuracy: 0.8547\n",
            "\n",
            "Epoch 00416: val_accuracy did not improve from 0.92857\n",
            "Epoch 417/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0187 - accuracy: 0.9957 - val_loss: 0.4992 - val_accuracy: 0.9212\n",
            "\n",
            "Epoch 00417: val_accuracy did not improve from 0.92857\n",
            "Epoch 418/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0058 - accuracy: 0.9982 - val_loss: 0.5885 - val_accuracy: 0.8916\n",
            "\n",
            "Epoch 00418: val_accuracy did not improve from 0.92857\n",
            "Epoch 419/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0183 - accuracy: 0.9957 - val_loss: 0.3231 - val_accuracy: 0.9236\n",
            "\n",
            "Epoch 00419: val_accuracy did not improve from 0.92857\n",
            "Epoch 420/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.0160 - accuracy: 0.9963 - val_loss: 0.4165 - val_accuracy: 0.9113\n",
            "\n",
            "Epoch 00420: val_accuracy did not improve from 0.92857\n",
            "Epoch 421/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0077 - accuracy: 0.9988 - val_loss: 0.4294 - val_accuracy: 0.9163\n",
            "\n",
            "Epoch 00421: val_accuracy did not improve from 0.92857\n",
            "Epoch 422/500\n",
            "52/52 [==============================] - 37s 715ms/step - loss: 0.0253 - accuracy: 0.9915 - val_loss: 1.0592 - val_accuracy: 0.8424\n",
            "\n",
            "Epoch 00422: val_accuracy did not improve from 0.92857\n",
            "Epoch 423/500\n",
            "52/52 [==============================] - 37s 715ms/step - loss: 0.0578 - accuracy: 0.9842 - val_loss: 0.6648 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00423: val_accuracy did not improve from 0.92857\n",
            "Epoch 424/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0142 - accuracy: 0.9933 - val_loss: 0.5221 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00424: val_accuracy did not improve from 0.92857\n",
            "Epoch 425/500\n",
            "52/52 [==============================] - 37s 716ms/step - loss: 0.0288 - accuracy: 0.9909 - val_loss: 10.7432 - val_accuracy: 0.0911\n",
            "\n",
            "Epoch 00425: val_accuracy did not improve from 0.92857\n",
            "Epoch 426/500\n",
            "52/52 [==============================] - 38s 722ms/step - loss: 0.0213 - accuracy: 0.9903 - val_loss: 12.5141 - val_accuracy: 0.1798\n",
            "\n",
            "Epoch 00426: val_accuracy did not improve from 0.92857\n",
            "Epoch 427/500\n",
            "52/52 [==============================] - 37s 715ms/step - loss: 0.0182 - accuracy: 0.9933 - val_loss: 13.2091 - val_accuracy: 0.1552\n",
            "\n",
            "Epoch 00427: val_accuracy did not improve from 0.92857\n",
            "Epoch 428/500\n",
            "52/52 [==============================] - 37s 715ms/step - loss: 0.0294 - accuracy: 0.9915 - val_loss: 5.8951 - val_accuracy: 0.3768\n",
            "\n",
            "Epoch 00428: val_accuracy did not improve from 0.92857\n",
            "Epoch 429/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.0502 - accuracy: 0.9854 - val_loss: 4.1671 - val_accuracy: 0.5320\n",
            "\n",
            "Epoch 00429: val_accuracy did not improve from 0.92857\n",
            "Epoch 430/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0258 - accuracy: 0.9933 - val_loss: 2.9814 - val_accuracy: 0.6084\n",
            "\n",
            "Epoch 00430: val_accuracy did not improve from 0.92857\n",
            "Epoch 431/500\n",
            "52/52 [==============================] - 37s 715ms/step - loss: 0.0153 - accuracy: 0.9957 - val_loss: 2.7310 - val_accuracy: 0.6478\n",
            "\n",
            "Epoch 00431: val_accuracy did not improve from 0.92857\n",
            "Epoch 432/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.0107 - accuracy: 0.9957 - val_loss: 0.6330 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00432: val_accuracy did not improve from 0.92857\n",
            "Epoch 433/500\n",
            "52/52 [==============================] - 37s 712ms/step - loss: 0.0167 - accuracy: 0.9957 - val_loss: 0.8661 - val_accuracy: 0.8645\n",
            "\n",
            "Epoch 00433: val_accuracy did not improve from 0.92857\n",
            "Epoch 434/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0103 - accuracy: 0.9970 - val_loss: 0.6002 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00434: val_accuracy did not improve from 0.92857\n",
            "Epoch 435/500\n",
            "52/52 [==============================] - 38s 723ms/step - loss: 0.0227 - accuracy: 0.9921 - val_loss: 0.6395 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00435: val_accuracy did not improve from 0.92857\n",
            "Epoch 436/500\n",
            "52/52 [==============================] - 38s 724ms/step - loss: 0.0184 - accuracy: 0.9939 - val_loss: 0.6636 - val_accuracy: 0.8941\n",
            "\n",
            "Epoch 00436: val_accuracy did not improve from 0.92857\n",
            "Epoch 437/500\n",
            "52/52 [==============================] - 38s 723ms/step - loss: 0.0084 - accuracy: 0.9976 - val_loss: 0.8388 - val_accuracy: 0.8596\n",
            "\n",
            "Epoch 00437: val_accuracy did not improve from 0.92857\n",
            "Epoch 438/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0071 - accuracy: 0.9982 - val_loss: 0.6311 - val_accuracy: 0.9138\n",
            "\n",
            "Epoch 00438: val_accuracy did not improve from 0.92857\n",
            "Epoch 439/500\n",
            "52/52 [==============================] - 38s 722ms/step - loss: 0.0119 - accuracy: 0.9951 - val_loss: 0.6172 - val_accuracy: 0.8941\n",
            "\n",
            "Epoch 00439: val_accuracy did not improve from 0.92857\n",
            "Epoch 440/500\n",
            "52/52 [==============================] - 38s 723ms/step - loss: 0.0102 - accuracy: 0.9951 - val_loss: 0.6110 - val_accuracy: 0.9015\n",
            "\n",
            "Epoch 00440: val_accuracy did not improve from 0.92857\n",
            "Epoch 441/500\n",
            "52/52 [==============================] - 37s 713ms/step - loss: 0.0216 - accuracy: 0.9939 - val_loss: 0.4781 - val_accuracy: 0.9187\n",
            "\n",
            "Epoch 00441: val_accuracy did not improve from 0.92857\n",
            "Epoch 442/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0323 - accuracy: 0.9878 - val_loss: 0.7370 - val_accuracy: 0.8892\n",
            "\n",
            "Epoch 00442: val_accuracy did not improve from 0.92857\n",
            "Epoch 443/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.0127 - accuracy: 0.9957 - val_loss: 0.7013 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00443: val_accuracy did not improve from 0.92857\n",
            "Epoch 444/500\n",
            "52/52 [==============================] - 37s 714ms/step - loss: 0.0070 - accuracy: 0.9988 - val_loss: 0.7067 - val_accuracy: 0.8916\n",
            "\n",
            "Epoch 00444: val_accuracy did not improve from 0.92857\n",
            "Epoch 445/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0087 - accuracy: 0.9957 - val_loss: 1.4375 - val_accuracy: 0.7906\n",
            "\n",
            "Epoch 00445: val_accuracy did not improve from 0.92857\n",
            "Epoch 446/500\n",
            "52/52 [==============================] - 38s 719ms/step - loss: 0.0192 - accuracy: 0.9939 - val_loss: 1.8437 - val_accuracy: 0.7586\n",
            "\n",
            "Epoch 00446: val_accuracy did not improve from 0.92857\n",
            "Epoch 447/500\n",
            "52/52 [==============================] - 38s 722ms/step - loss: 0.0218 - accuracy: 0.9963 - val_loss: 2.1418 - val_accuracy: 0.7463\n",
            "\n",
            "Epoch 00447: val_accuracy did not improve from 0.92857\n",
            "Epoch 448/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0219 - accuracy: 0.9945 - val_loss: 2.9881 - val_accuracy: 0.6601\n",
            "\n",
            "Epoch 00448: val_accuracy did not improve from 0.92857\n",
            "Epoch 449/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0161 - accuracy: 0.9939 - val_loss: 0.8783 - val_accuracy: 0.8916\n",
            "\n",
            "Epoch 00449: val_accuracy did not improve from 0.92857\n",
            "Epoch 450/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0096 - accuracy: 0.9970 - val_loss: 1.0439 - val_accuracy: 0.8522\n",
            "\n",
            "Epoch 00450: val_accuracy did not improve from 0.92857\n",
            "Epoch 451/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.0079 - accuracy: 0.9970 - val_loss: 0.9142 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00451: val_accuracy did not improve from 0.92857\n",
            "Epoch 452/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.6858 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00452: val_accuracy did not improve from 0.92857\n",
            "Epoch 453/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0281 - accuracy: 0.9939 - val_loss: 2.4825 - val_accuracy: 0.6847\n",
            "\n",
            "Epoch 00453: val_accuracy did not improve from 0.92857\n",
            "Epoch 454/500\n",
            "52/52 [==============================] - 37s 716ms/step - loss: 0.0258 - accuracy: 0.9939 - val_loss: 4.0114 - val_accuracy: 0.5813\n",
            "\n",
            "Epoch 00454: val_accuracy did not improve from 0.92857\n",
            "Epoch 455/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0232 - accuracy: 0.9921 - val_loss: 0.8316 - val_accuracy: 0.8719\n",
            "\n",
            "Epoch 00455: val_accuracy did not improve from 0.92857\n",
            "Epoch 456/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0192 - accuracy: 0.9939 - val_loss: 3.1983 - val_accuracy: 0.6355\n",
            "\n",
            "Epoch 00456: val_accuracy did not improve from 0.92857\n",
            "Epoch 457/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0096 - accuracy: 0.9951 - val_loss: 2.9981 - val_accuracy: 0.6404\n",
            "\n",
            "Epoch 00457: val_accuracy did not improve from 0.92857\n",
            "Epoch 458/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.1476 - accuracy: 0.9714 - val_loss: 14.7548 - val_accuracy: 0.1158\n",
            "\n",
            "Epoch 00458: val_accuracy did not improve from 0.92857\n",
            "Epoch 459/500\n",
            "52/52 [==============================] - 37s 712ms/step - loss: 0.0400 - accuracy: 0.9890 - val_loss: 12.9341 - val_accuracy: 0.1798\n",
            "\n",
            "Epoch 00459: val_accuracy did not improve from 0.92857\n",
            "Epoch 460/500\n",
            "52/52 [==============================] - 38s 721ms/step - loss: 0.0328 - accuracy: 0.9909 - val_loss: 5.5206 - val_accuracy: 0.3547\n",
            "\n",
            "Epoch 00460: val_accuracy did not improve from 0.92857\n",
            "Epoch 461/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0241 - accuracy: 0.9927 - val_loss: 1.2020 - val_accuracy: 0.7906\n",
            "\n",
            "Epoch 00461: val_accuracy did not improve from 0.92857\n",
            "Epoch 462/500\n",
            "52/52 [==============================] - 37s 715ms/step - loss: 0.0153 - accuracy: 0.9945 - val_loss: 0.5797 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00462: val_accuracy did not improve from 0.92857\n",
            "Epoch 463/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0034 - accuracy: 0.9988 - val_loss: 0.4351 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00463: val_accuracy did not improve from 0.92857\n",
            "Epoch 464/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.0055 - accuracy: 0.9982 - val_loss: 0.4854 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00464: val_accuracy did not improve from 0.92857\n",
            "Epoch 465/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.0076 - accuracy: 0.9963 - val_loss: 0.4711 - val_accuracy: 0.9212\n",
            "\n",
            "Epoch 00465: val_accuracy did not improve from 0.92857\n",
            "Epoch 466/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0128 - accuracy: 0.9957 - val_loss: 0.6445 - val_accuracy: 0.8892\n",
            "\n",
            "Epoch 00466: val_accuracy did not improve from 0.92857\n",
            "Epoch 467/500\n",
            "52/52 [==============================] - 38s 723ms/step - loss: 0.0072 - accuracy: 0.9982 - val_loss: 0.5390 - val_accuracy: 0.9039\n",
            "\n",
            "Epoch 00467: val_accuracy did not improve from 0.92857\n",
            "Epoch 468/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0121 - accuracy: 0.9963 - val_loss: 0.5950 - val_accuracy: 0.8892\n",
            "\n",
            "Epoch 00468: val_accuracy did not improve from 0.92857\n",
            "Epoch 469/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.4966 - val_accuracy: 0.9163\n",
            "\n",
            "Epoch 00469: val_accuracy did not improve from 0.92857\n",
            "Epoch 470/500\n",
            "52/52 [==============================] - 37s 714ms/step - loss: 0.0054 - accuracy: 0.9976 - val_loss: 0.4428 - val_accuracy: 0.9187\n",
            "\n",
            "Epoch 00470: val_accuracy did not improve from 0.92857\n",
            "Epoch 471/500\n",
            "52/52 [==============================] - 37s 714ms/step - loss: 0.0063 - accuracy: 0.9976 - val_loss: 0.4626 - val_accuracy: 0.9138\n",
            "\n",
            "Epoch 00471: val_accuracy did not improve from 0.92857\n",
            "Epoch 472/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0028 - accuracy: 0.9988 - val_loss: 0.5869 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00472: val_accuracy did not improve from 0.92857\n",
            "Epoch 473/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0036 - accuracy: 0.9988 - val_loss: 0.4388 - val_accuracy: 0.9163\n",
            "\n",
            "Epoch 00473: val_accuracy did not improve from 0.92857\n",
            "Epoch 474/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0095 - accuracy: 0.9970 - val_loss: 0.9439 - val_accuracy: 0.8498\n",
            "\n",
            "Epoch 00474: val_accuracy did not improve from 0.92857\n",
            "Epoch 475/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0052 - accuracy: 0.9970 - val_loss: 0.6021 - val_accuracy: 0.9064\n",
            "\n",
            "Epoch 00475: val_accuracy did not improve from 0.92857\n",
            "Epoch 476/500\n",
            "52/52 [==============================] - 38s 722ms/step - loss: 0.0062 - accuracy: 0.9963 - val_loss: 0.4415 - val_accuracy: 0.9187\n",
            "\n",
            "Epoch 00476: val_accuracy did not improve from 0.92857\n",
            "Epoch 477/500\n",
            "52/52 [==============================] - 37s 720ms/step - loss: 0.0056 - accuracy: 0.9988 - val_loss: 0.3923 - val_accuracy: 0.9187\n",
            "\n",
            "Epoch 00477: val_accuracy did not improve from 0.92857\n",
            "Epoch 478/500\n",
            "52/52 [==============================] - 37s 714ms/step - loss: 9.1368e-04 - accuracy: 1.0000 - val_loss: 0.4253 - val_accuracy: 0.9163\n",
            "\n",
            "Epoch 00478: val_accuracy did not improve from 0.92857\n",
            "Epoch 479/500\n",
            "52/52 [==============================] - 37s 715ms/step - loss: 9.6429e-04 - accuracy: 1.0000 - val_loss: 0.4799 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00479: val_accuracy did not improve from 0.92857\n",
            "Epoch 480/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.5215 - val_accuracy: 0.9138\n",
            "\n",
            "Epoch 00480: val_accuracy did not improve from 0.92857\n",
            "Epoch 481/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0031 - accuracy: 0.9988 - val_loss: 0.6008 - val_accuracy: 0.9039\n",
            "\n",
            "Epoch 00481: val_accuracy did not improve from 0.92857\n",
            "Epoch 482/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0032 - accuracy: 0.9982 - val_loss: 0.6919 - val_accuracy: 0.9138\n",
            "\n",
            "Epoch 00482: val_accuracy did not improve from 0.92857\n",
            "Epoch 483/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.0030 - accuracy: 0.9982 - val_loss: 0.5450 - val_accuracy: 0.9039\n",
            "\n",
            "Epoch 00483: val_accuracy did not improve from 0.92857\n",
            "Epoch 484/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0035 - accuracy: 0.9988 - val_loss: 0.5034 - val_accuracy: 0.9261\n",
            "\n",
            "Epoch 00484: val_accuracy did not improve from 0.92857\n",
            "Epoch 485/500\n",
            "52/52 [==============================] - 37s 719ms/step - loss: 0.0058 - accuracy: 0.9976 - val_loss: 0.5605 - val_accuracy: 0.9039\n",
            "\n",
            "Epoch 00485: val_accuracy did not improve from 0.92857\n",
            "Epoch 486/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0124 - accuracy: 0.9951 - val_loss: 0.7413 - val_accuracy: 0.8744\n",
            "\n",
            "Epoch 00486: val_accuracy did not improve from 0.92857\n",
            "Epoch 487/500\n",
            "52/52 [==============================] - 37s 716ms/step - loss: 0.0377 - accuracy: 0.9903 - val_loss: 0.8836 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00487: val_accuracy did not improve from 0.92857\n",
            "Epoch 488/500\n",
            "52/52 [==============================] - 37s 714ms/step - loss: 0.0279 - accuracy: 0.9909 - val_loss: 13.8734 - val_accuracy: 0.1502\n",
            "\n",
            "Epoch 00488: val_accuracy did not improve from 0.92857\n",
            "Epoch 489/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0356 - accuracy: 0.9933 - val_loss: 2.0143 - val_accuracy: 0.6773\n",
            "\n",
            "Epoch 00489: val_accuracy did not improve from 0.92857\n",
            "Epoch 490/500\n",
            "52/52 [==============================] - 37s 716ms/step - loss: 0.0366 - accuracy: 0.9878 - val_loss: 0.7390 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00490: val_accuracy did not improve from 0.92857\n",
            "Epoch 491/500\n",
            "52/52 [==============================] - 38s 720ms/step - loss: 0.0237 - accuracy: 0.9933 - val_loss: 0.8461 - val_accuracy: 0.8695\n",
            "\n",
            "Epoch 00491: val_accuracy did not improve from 0.92857\n",
            "Epoch 492/500\n",
            "52/52 [==============================] - 37s 718ms/step - loss: 0.0162 - accuracy: 0.9945 - val_loss: 0.5364 - val_accuracy: 0.9113\n",
            "\n",
            "Epoch 00492: val_accuracy did not improve from 0.92857\n",
            "Epoch 493/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0175 - accuracy: 0.9933 - val_loss: 0.7524 - val_accuracy: 0.8941\n",
            "\n",
            "Epoch 00493: val_accuracy did not improve from 0.92857\n",
            "Epoch 494/500\n",
            "52/52 [==============================] - 37s 715ms/step - loss: 0.0207 - accuracy: 0.9927 - val_loss: 0.5648 - val_accuracy: 0.9015\n",
            "\n",
            "Epoch 00494: val_accuracy did not improve from 0.92857\n",
            "Epoch 495/500\n",
            "52/52 [==============================] - 38s 722ms/step - loss: 0.0149 - accuracy: 0.9951 - val_loss: 2.5959 - val_accuracy: 0.7192\n",
            "\n",
            "Epoch 00495: val_accuracy did not improve from 0.92857\n",
            "Epoch 496/500\n",
            "52/52 [==============================] - 37s 720ms/step - loss: 0.0106 - accuracy: 0.9939 - val_loss: 2.6451 - val_accuracy: 0.7020\n",
            "\n",
            "Epoch 00496: val_accuracy did not improve from 0.92857\n",
            "Epoch 497/500\n",
            "52/52 [==============================] - 37s 715ms/step - loss: 0.0131 - accuracy: 0.9963 - val_loss: 4.9766 - val_accuracy: 0.5123\n",
            "\n",
            "Epoch 00497: val_accuracy did not improve from 0.92857\n",
            "Epoch 498/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0093 - accuracy: 0.9963 - val_loss: 0.4944 - val_accuracy: 0.9113\n",
            "\n",
            "Epoch 00498: val_accuracy did not improve from 0.92857\n",
            "Epoch 499/500\n",
            "52/52 [==============================] - 37s 713ms/step - loss: 0.0103 - accuracy: 0.9963 - val_loss: 4.4601 - val_accuracy: 0.5369\n",
            "\n",
            "Epoch 00499: val_accuracy did not improve from 0.92857\n",
            "Epoch 500/500\n",
            "52/52 [==============================] - 37s 717ms/step - loss: 0.0124 - accuracy: 0.9963 - val_loss: 0.9192 - val_accuracy: 0.8768\n",
            "\n",
            "Epoch 00500: val_accuracy did not improve from 0.92857\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f62e95257d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHmpkzRJyCrf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "3d4bc3f2-ad4a-445b-86f2-12bf43eb8daf"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(EfficientNetB5_model.history.history[\"accuracy\"], label='EfficientNetB5_acc')\n",
        "plt.plot(EfficientNetB5_model.history.history[\"val_accuracy\"], label='EfficientNetB5_val')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcElIu93yIQU"
      },
      "source": [
        "EfficientNetB5_model = tf.keras.models.load_model('/content/drive/MyDrive/DACON_CVLC/Checkpoint/CVLC_06_EfficientNetB5.h5', compile=False)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hR4N2pAZyiR-"
      },
      "source": [
        "!mkdir images_test/none\n",
        "!mv images_test/*.png images_test/none"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxH98QOgyu1z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fd7d29f-da0e-4dc9-d2ff-a28c184a06b9"
      },
      "source": [
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = datagen.flow_from_directory('./images_test', target_size=(224,224), color_mode='grayscale', class_mode='categorical', shuffle=False)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 20480 images belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFEcoCR-3DNH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ee0328d-a817-4440-b79a-e24537d2d1e6"
      },
      "source": [
        "EfficientNetB5_predict = EfficientNetB5_model.predict_generator(test_generator).argmax(axis=1)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2001: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYhGZuzr1AjD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "905d0450-1b03-443c-af31-e9bc7d015568"
      },
      "source": [
        "submission = pd.read_csv('/content/drive/MyDrive/DACON_CVLC/data/submission.csv')\n",
        "submission.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>digit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2049</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2050</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2051</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2052</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2053</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     id  digit\n",
              "0  2049      0\n",
              "1  2050      0\n",
              "2  2051      0\n",
              "3  2052      0\n",
              "4  2053      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWALVGA1shFz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1968befc-12b2-4e68-fc7f-733e6a2f0c9e"
      },
      "source": [
        "import numpy as np\n",
        "mylist = []\n",
        "\n",
        "for i in range(len(submission)):\n",
        "    name =  test_generator.filenames\n",
        "    id = name[i].split('/')[1].rstrip('.').split('.')[0]\n",
        "    mylist.append(id)\n",
        "print(mylist)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['10000', '10001', '10002', '10003', '10004', '10005', '10006', '10007', '10008', '10009', '10010', '10011', '10012', '10013', '10014', '10015', '10016', '10017', '10018', '10019', '10020', '10021', '10022', '10023', '10024', '10025', '10026', '10027', '10028', '10029', '10030', '10031', '10032', '10033', '10034', '10035', '10036', '10037', '10038', '10039', '10040', '10041', '10042', '10043', '10044', '10045', '10046', '10047', '10048', '10049', '10050', '10051', '10052', '10053', '10054', '10055', '10056', '10057', '10058', '10059', '10060', '10061', '10062', '10063', '10064', '10065', '10066', '10067', '10068', '10069', '10070', '10071', '10072', '10073', '10074', '10075', '10076', '10077', '10078', '10079', '10080', '10081', '10082', '10083', '10084', '10085', '10086', '10087', '10088', '10089', '10090', '10091', '10092', '10093', '10094', '10095', '10096', '10097', '10098', '10099', '10100', '10101', '10102', '10103', '10104', '10105', '10106', '10107', '10108', '10109', '10110', '10111', '10112', '10113', '10114', '10115', '10116', '10117', '10118', '10119', '10120', '10121', '10122', '10123', '10124', '10125', '10126', '10127', '10128', '10129', '10130', '10131', '10132', '10133', '10134', '10135', '10136', '10137', '10138', '10139', '10140', '10141', '10142', '10143', '10144', '10145', '10146', '10147', '10148', '10149', '10150', '10151', '10152', '10153', '10154', '10155', '10156', '10157', '10158', '10159', '10160', '10161', '10162', '10163', '10164', '10165', '10166', '10167', '10168', '10169', '10170', '10171', '10172', '10173', '10174', '10175', '10176', '10177', '10178', '10179', '10180', '10181', '10182', '10183', '10184', '10185', '10186', '10187', '10188', '10189', '10190', '10191', '10192', '10193', '10194', '10195', '10196', '10197', '10198', '10199', '10200', '10201', '10202', '10203', '10204', '10205', '10206', '10207', '10208', '10209', '10210', '10211', '10212', '10213', '10214', '10215', '10216', '10217', '10218', '10219', '10220', '10221', '10222', '10223', '10224', '10225', '10226', '10227', '10228', '10229', '10230', '10231', '10232', '10233', '10234', '10235', '10236', '10237', '10238', '10239', '10240', '10241', '10242', '10243', '10244', '10245', '10246', '10247', '10248', '10249', '10250', '10251', '10252', '10253', '10254', '10255', '10256', '10257', '10258', '10259', '10260', '10261', '10262', '10263', '10264', '10265', '10266', '10267', '10268', '10269', '10270', '10271', '10272', '10273', '10274', '10275', '10276', '10277', '10278', '10279', '10280', '10281', '10282', '10283', '10284', '10285', '10286', '10287', '10288', '10289', '10290', '10291', '10292', '10293', '10294', '10295', '10296', '10297', '10298', '10299', '10300', '10301', '10302', '10303', '10304', '10305', '10306', '10307', '10308', '10309', '10310', '10311', '10312', '10313', '10314', '10315', '10316', '10317', '10318', '10319', '10320', '10321', '10322', '10323', '10324', '10325', '10326', '10327', '10328', '10329', '10330', '10331', '10332', '10333', '10334', '10335', '10336', '10337', '10338', '10339', '10340', '10341', '10342', '10343', '10344', '10345', '10346', '10347', '10348', '10349', '10350', '10351', '10352', '10353', '10354', '10355', '10356', '10357', '10358', '10359', '10360', '10361', '10362', '10363', '10364', '10365', '10366', '10367', '10368', '10369', '10370', '10371', '10372', '10373', '10374', '10375', '10376', '10377', '10378', '10379', '10380', '10381', '10382', '10383', '10384', '10385', '10386', '10387', '10388', '10389', '10390', '10391', '10392', '10393', '10394', '10395', '10396', '10397', '10398', '10399', '10400', '10401', '10402', '10403', '10404', '10405', '10406', '10407', '10408', '10409', '10410', '10411', '10412', '10413', '10414', '10415', '10416', '10417', '10418', '10419', '10420', '10421', '10422', '10423', '10424', '10425', '10426', '10427', '10428', '10429', '10430', '10431', '10432', '10433', '10434', '10435', '10436', '10437', '10438', '10439', '10440', '10441', '10442', '10443', '10444', '10445', '10446', '10447', '10448', '10449', '10450', '10451', '10452', '10453', '10454', '10455', '10456', '10457', '10458', '10459', '10460', '10461', '10462', '10463', '10464', '10465', '10466', '10467', '10468', '10469', '10470', '10471', '10472', '10473', '10474', '10475', '10476', '10477', '10478', '10479', '10480', '10481', '10482', '10483', '10484', '10485', '10486', '10487', '10488', '10489', '10490', '10491', '10492', '10493', '10494', '10495', '10496', '10497', '10498', '10499', '10500', '10501', '10502', '10503', '10504', '10505', '10506', '10507', '10508', '10509', '10510', '10511', '10512', '10513', '10514', '10515', '10516', '10517', '10518', '10519', '10520', '10521', '10522', '10523', '10524', '10525', '10526', '10527', '10528', '10529', '10530', '10531', '10532', '10533', '10534', '10535', '10536', '10537', '10538', '10539', '10540', '10541', '10542', '10543', '10544', '10545', '10546', '10547', '10548', '10549', '10550', '10551', '10552', '10553', '10554', '10555', '10556', '10557', '10558', '10559', '10560', '10561', '10562', '10563', '10564', '10565', '10566', '10567', '10568', '10569', '10570', '10571', '10572', '10573', '10574', '10575', '10576', '10577', '10578', '10579', '10580', '10581', '10582', '10583', '10584', '10585', '10586', '10587', '10588', '10589', '10590', '10591', '10592', '10593', '10594', '10595', '10596', '10597', '10598', '10599', '10600', '10601', '10602', '10603', '10604', '10605', '10606', '10607', '10608', '10609', '10610', '10611', '10612', '10613', '10614', '10615', '10616', '10617', '10618', '10619', '10620', '10621', '10622', '10623', '10624', '10625', '10626', '10627', '10628', '10629', '10630', '10631', '10632', '10633', '10634', '10635', '10636', '10637', '10638', '10639', '10640', '10641', '10642', '10643', '10644', '10645', '10646', '10647', '10648', '10649', '10650', '10651', '10652', '10653', '10654', '10655', '10656', '10657', '10658', '10659', '10660', '10661', '10662', '10663', '10664', '10665', '10666', '10667', '10668', '10669', '10670', '10671', '10672', '10673', '10674', '10675', '10676', '10677', '10678', '10679', '10680', '10681', '10682', '10683', '10684', '10685', '10686', '10687', '10688', '10689', '10690', '10691', '10692', '10693', '10694', '10695', '10696', '10697', '10698', '10699', '10700', '10701', '10702', '10703', '10704', '10705', '10706', '10707', '10708', '10709', '10710', '10711', '10712', '10713', '10714', '10715', '10716', '10717', '10718', '10719', '10720', '10721', '10722', '10723', '10724', '10725', '10726', '10727', '10728', '10729', '10730', '10731', '10732', '10733', '10734', '10735', '10736', '10737', '10738', '10739', '10740', '10741', '10742', '10743', '10744', '10745', '10746', '10747', '10748', '10749', '10750', '10751', '10752', '10753', '10754', '10755', '10756', '10757', '10758', '10759', '10760', '10761', '10762', '10763', '10764', '10765', '10766', '10767', '10768', '10769', '10770', '10771', '10772', '10773', '10774', '10775', '10776', '10777', '10778', '10779', '10780', '10781', '10782', '10783', '10784', '10785', '10786', '10787', '10788', '10789', '10790', '10791', '10792', '10793', '10794', '10795', '10796', '10797', '10798', '10799', '10800', '10801', '10802', '10803', '10804', '10805', '10806', '10807', '10808', '10809', '10810', '10811', '10812', '10813', '10814', '10815', '10816', '10817', '10818', '10819', '10820', '10821', '10822', '10823', '10824', '10825', '10826', '10827', '10828', '10829', '10830', '10831', '10832', '10833', '10834', '10835', '10836', '10837', '10838', '10839', '10840', '10841', '10842', '10843', '10844', '10845', '10846', '10847', '10848', '10849', '10850', '10851', '10852', '10853', '10854', '10855', '10856', '10857', '10858', '10859', '10860', '10861', '10862', '10863', '10864', '10865', '10866', '10867', '10868', '10869', '10870', '10871', '10872', '10873', '10874', '10875', '10876', '10877', '10878', '10879', '10880', '10881', '10882', '10883', '10884', '10885', '10886', '10887', '10888', '10889', '10890', '10891', '10892', '10893', '10894', '10895', '10896', '10897', '10898', '10899', '10900', '10901', '10902', '10903', '10904', '10905', '10906', '10907', '10908', '10909', '10910', '10911', '10912', '10913', '10914', '10915', '10916', '10917', '10918', '10919', '10920', '10921', '10922', '10923', '10924', '10925', '10926', '10927', '10928', '10929', '10930', '10931', '10932', '10933', '10934', '10935', '10936', '10937', '10938', '10939', '10940', '10941', '10942', '10943', '10944', '10945', '10946', '10947', '10948', '10949', '10950', '10951', '10952', '10953', '10954', '10955', '10956', '10957', '10958', '10959', '10960', '10961', '10962', '10963', '10964', '10965', '10966', '10967', '10968', '10969', '10970', '10971', '10972', '10973', '10974', '10975', '10976', '10977', '10978', '10979', '10980', '10981', '10982', '10983', '10984', '10985', '10986', '10987', '10988', '10989', '10990', '10991', '10992', '10993', '10994', '10995', '10996', '10997', '10998', '10999', '11000', '11001', '11002', '11003', '11004', '11005', '11006', '11007', '11008', '11009', '11010', '11011', '11012', '11013', '11014', '11015', '11016', '11017', '11018', '11019', '11020', '11021', '11022', '11023', '11024', '11025', '11026', '11027', '11028', '11029', '11030', '11031', '11032', '11033', '11034', '11035', '11036', '11037', '11038', '11039', '11040', '11041', '11042', '11043', '11044', '11045', '11046', '11047', '11048', '11049', '11050', '11051', '11052', '11053', '11054', '11055', '11056', '11057', '11058', '11059', '11060', '11061', '11062', '11063', '11064', '11065', '11066', '11067', '11068', '11069', '11070', '11071', '11072', '11073', '11074', '11075', '11076', '11077', '11078', '11079', '11080', '11081', '11082', '11083', '11084', '11085', '11086', '11087', '11088', '11089', '11090', '11091', '11092', '11093', '11094', '11095', '11096', '11097', '11098', '11099', '11100', '11101', '11102', '11103', '11104', '11105', '11106', '11107', '11108', '11109', '11110', '11111', '11112', '11113', '11114', '11115', '11116', '11117', '11118', '11119', '11120', '11121', '11122', '11123', '11124', '11125', '11126', '11127', '11128', '11129', '11130', '11131', '11132', '11133', '11134', '11135', '11136', '11137', '11138', '11139', '11140', '11141', '11142', '11143', '11144', '11145', '11146', '11147', '11148', '11149', '11150', '11151', '11152', '11153', '11154', '11155', '11156', '11157', '11158', '11159', '11160', '11161', '11162', '11163', '11164', '11165', '11166', '11167', '11168', '11169', '11170', '11171', '11172', '11173', '11174', '11175', '11176', '11177', '11178', '11179', '11180', '11181', '11182', '11183', '11184', '11185', '11186', '11187', '11188', '11189', '11190', '11191', '11192', '11193', '11194', '11195', '11196', '11197', '11198', '11199', '11200', '11201', '11202', '11203', '11204', '11205', '11206', '11207', '11208', '11209', '11210', '11211', '11212', '11213', '11214', '11215', '11216', '11217', '11218', '11219', '11220', '11221', '11222', '11223', '11224', '11225', '11226', '11227', '11228', '11229', '11230', '11231', '11232', '11233', '11234', '11235', '11236', '11237', '11238', '11239', '11240', '11241', '11242', '11243', '11244', '11245', '11246', '11247', '11248', '11249', '11250', '11251', '11252', '11253', '11254', '11255', '11256', '11257', '11258', '11259', '11260', '11261', '11262', '11263', '11264', '11265', '11266', '11267', '11268', '11269', '11270', '11271', '11272', '11273', '11274', '11275', '11276', '11277', '11278', '11279', '11280', '11281', '11282', '11283', '11284', '11285', '11286', '11287', '11288', '11289', '11290', '11291', '11292', '11293', '11294', '11295', '11296', '11297', '11298', '11299', '11300', '11301', '11302', '11303', '11304', '11305', '11306', '11307', '11308', '11309', '11310', '11311', '11312', '11313', '11314', '11315', '11316', '11317', '11318', '11319', '11320', '11321', '11322', '11323', '11324', '11325', '11326', '11327', '11328', '11329', '11330', '11331', '11332', '11333', '11334', '11335', '11336', '11337', '11338', '11339', '11340', '11341', '11342', '11343', '11344', '11345', '11346', '11347', '11348', '11349', '11350', '11351', '11352', '11353', '11354', '11355', '11356', '11357', '11358', '11359', '11360', '11361', '11362', '11363', '11364', '11365', '11366', '11367', '11368', '11369', '11370', '11371', '11372', '11373', '11374', '11375', '11376', '11377', '11378', '11379', '11380', '11381', '11382', '11383', '11384', '11385', '11386', '11387', '11388', '11389', '11390', '11391', '11392', '11393', '11394', '11395', '11396', '11397', '11398', '11399', '11400', '11401', '11402', '11403', '11404', '11405', '11406', '11407', '11408', '11409', '11410', '11411', '11412', '11413', '11414', '11415', '11416', '11417', '11418', '11419', '11420', '11421', '11422', '11423', '11424', '11425', '11426', '11427', '11428', '11429', '11430', '11431', '11432', '11433', '11434', '11435', '11436', '11437', '11438', '11439', '11440', '11441', '11442', '11443', '11444', '11445', '11446', '11447', '11448', '11449', '11450', '11451', '11452', '11453', '11454', '11455', '11456', '11457', '11458', '11459', '11460', '11461', '11462', '11463', '11464', '11465', '11466', '11467', '11468', '11469', '11470', '11471', '11472', '11473', '11474', '11475', '11476', '11477', '11478', '11479', '11480', '11481', '11482', '11483', '11484', '11485', '11486', '11487', '11488', '11489', '11490', '11491', '11492', '11493', '11494', '11495', '11496', '11497', '11498', '11499', '11500', '11501', '11502', '11503', '11504', '11505', '11506', '11507', '11508', '11509', '11510', '11511', '11512', '11513', '11514', '11515', '11516', '11517', '11518', '11519', '11520', '11521', '11522', '11523', '11524', '11525', '11526', '11527', '11528', '11529', '11530', '11531', '11532', '11533', '11534', '11535', '11536', '11537', '11538', '11539', '11540', '11541', '11542', '11543', '11544', '11545', '11546', '11547', '11548', '11549', '11550', '11551', '11552', '11553', '11554', '11555', '11556', '11557', '11558', '11559', '11560', '11561', '11562', '11563', '11564', '11565', '11566', '11567', '11568', '11569', '11570', '11571', '11572', '11573', '11574', '11575', '11576', '11577', '11578', '11579', '11580', '11581', '11582', '11583', '11584', '11585', '11586', '11587', '11588', '11589', '11590', '11591', '11592', '11593', '11594', '11595', '11596', '11597', '11598', '11599', '11600', '11601', '11602', '11603', '11604', '11605', '11606', '11607', '11608', '11609', '11610', '11611', '11612', '11613', '11614', '11615', '11616', '11617', '11618', '11619', '11620', '11621', '11622', '11623', '11624', '11625', '11626', '11627', '11628', '11629', '11630', '11631', '11632', '11633', '11634', '11635', '11636', '11637', '11638', '11639', '11640', '11641', '11642', '11643', '11644', '11645', '11646', '11647', '11648', '11649', '11650', '11651', '11652', '11653', '11654', '11655', '11656', '11657', '11658', '11659', '11660', '11661', '11662', '11663', '11664', '11665', '11666', '11667', '11668', '11669', '11670', '11671', '11672', '11673', '11674', '11675', '11676', '11677', '11678', '11679', '11680', '11681', '11682', '11683', '11684', '11685', '11686', '11687', '11688', '11689', '11690', '11691', '11692', '11693', '11694', '11695', '11696', '11697', '11698', '11699', '11700', '11701', '11702', '11703', '11704', '11705', '11706', '11707', '11708', '11709', '11710', '11711', '11712', '11713', '11714', '11715', '11716', '11717', '11718', '11719', '11720', '11721', '11722', '11723', '11724', '11725', '11726', '11727', '11728', '11729', '11730', '11731', '11732', '11733', '11734', '11735', '11736', '11737', '11738', '11739', '11740', '11741', '11742', '11743', '11744', '11745', '11746', '11747', '11748', '11749', '11750', '11751', '11752', '11753', '11754', '11755', '11756', '11757', '11758', '11759', '11760', '11761', '11762', '11763', '11764', '11765', '11766', '11767', '11768', '11769', '11770', '11771', '11772', '11773', '11774', '11775', '11776', '11777', '11778', '11779', '11780', '11781', '11782', '11783', '11784', '11785', '11786', '11787', '11788', '11789', '11790', '11791', '11792', '11793', '11794', '11795', '11796', '11797', '11798', '11799', '11800', '11801', '11802', '11803', '11804', '11805', '11806', '11807', '11808', '11809', '11810', '11811', '11812', '11813', '11814', '11815', '11816', '11817', '11818', '11819', '11820', '11821', '11822', '11823', '11824', '11825', '11826', '11827', '11828', '11829', '11830', '11831', '11832', '11833', '11834', '11835', '11836', '11837', '11838', '11839', '11840', '11841', '11842', '11843', '11844', '11845', '11846', '11847', '11848', '11849', '11850', '11851', '11852', '11853', '11854', '11855', '11856', '11857', '11858', '11859', '11860', '11861', '11862', '11863', '11864', '11865', '11866', '11867', '11868', '11869', '11870', '11871', '11872', '11873', '11874', '11875', '11876', '11877', '11878', '11879', '11880', '11881', '11882', '11883', '11884', '11885', '11886', '11887', '11888', '11889', '11890', '11891', '11892', '11893', '11894', '11895', '11896', '11897', '11898', '11899', '11900', '11901', '11902', '11903', '11904', '11905', '11906', '11907', '11908', '11909', '11910', '11911', '11912', '11913', '11914', '11915', '11916', '11917', '11918', '11919', '11920', '11921', '11922', '11923', '11924', '11925', '11926', '11927', '11928', '11929', '11930', '11931', '11932', '11933', '11934', '11935', '11936', '11937', '11938', '11939', '11940', '11941', '11942', '11943', '11944', '11945', '11946', '11947', '11948', '11949', '11950', '11951', '11952', '11953', '11954', '11955', '11956', '11957', '11958', '11959', '11960', '11961', '11962', '11963', '11964', '11965', '11966', '11967', '11968', '11969', '11970', '11971', '11972', '11973', '11974', '11975', '11976', '11977', '11978', '11979', '11980', '11981', '11982', '11983', '11984', '11985', '11986', '11987', '11988', '11989', '11990', '11991', '11992', '11993', '11994', '11995', '11996', '11997', '11998', '11999', '12000', '12001', '12002', '12003', '12004', '12005', '12006', '12007', '12008', '12009', '12010', '12011', '12012', '12013', '12014', '12015', '12016', '12017', '12018', '12019', '12020', '12021', '12022', '12023', '12024', '12025', '12026', '12027', '12028', '12029', '12030', '12031', '12032', '12033', '12034', '12035', '12036', '12037', '12038', '12039', '12040', '12041', '12042', '12043', '12044', '12045', '12046', '12047', '12048', '12049', '12050', '12051', '12052', '12053', '12054', '12055', '12056', '12057', '12058', '12059', '12060', '12061', '12062', '12063', '12064', '12065', '12066', '12067', '12068', '12069', '12070', '12071', '12072', '12073', '12074', '12075', '12076', '12077', '12078', '12079', '12080', '12081', '12082', '12083', '12084', '12085', '12086', '12087', '12088', '12089', '12090', '12091', '12092', '12093', '12094', '12095', '12096', '12097', '12098', '12099', '12100', '12101', '12102', '12103', '12104', '12105', '12106', '12107', '12108', '12109', '12110', '12111', '12112', '12113', '12114', '12115', '12116', '12117', '12118', '12119', '12120', '12121', '12122', '12123', '12124', '12125', '12126', '12127', '12128', '12129', '12130', '12131', '12132', '12133', '12134', '12135', '12136', '12137', '12138', '12139', '12140', '12141', '12142', '12143', '12144', '12145', '12146', '12147', '12148', '12149', '12150', '12151', '12152', '12153', '12154', '12155', '12156', '12157', '12158', '12159', '12160', '12161', '12162', '12163', '12164', '12165', '12166', '12167', '12168', '12169', '12170', '12171', '12172', '12173', '12174', '12175', '12176', '12177', '12178', '12179', '12180', '12181', '12182', '12183', '12184', '12185', '12186', '12187', '12188', '12189', '12190', '12191', '12192', '12193', '12194', '12195', '12196', '12197', '12198', '12199', '12200', '12201', '12202', '12203', '12204', '12205', '12206', '12207', '12208', '12209', '12210', '12211', '12212', '12213', '12214', '12215', '12216', '12217', '12218', '12219', '12220', '12221', '12222', '12223', '12224', '12225', '12226', '12227', '12228', '12229', '12230', '12231', '12232', '12233', '12234', '12235', '12236', '12237', '12238', '12239', '12240', '12241', '12242', '12243', '12244', '12245', '12246', '12247', '12248', '12249', '12250', '12251', '12252', '12253', '12254', '12255', '12256', '12257', '12258', '12259', '12260', '12261', '12262', '12263', '12264', '12265', '12266', '12267', '12268', '12269', '12270', '12271', '12272', '12273', '12274', '12275', '12276', '12277', '12278', '12279', '12280', '12281', '12282', '12283', '12284', '12285', '12286', '12287', '12288', '12289', '12290', '12291', '12292', '12293', '12294', '12295', '12296', '12297', '12298', '12299', '12300', '12301', '12302', '12303', '12304', '12305', '12306', '12307', '12308', '12309', '12310', '12311', '12312', '12313', '12314', '12315', '12316', '12317', '12318', '12319', '12320', '12321', '12322', '12323', '12324', '12325', '12326', '12327', '12328', '12329', '12330', '12331', '12332', '12333', '12334', '12335', '12336', '12337', '12338', '12339', '12340', '12341', '12342', '12343', '12344', '12345', '12346', '12347', '12348', '12349', '12350', '12351', '12352', '12353', '12354', '12355', '12356', '12357', '12358', '12359', '12360', '12361', '12362', '12363', '12364', '12365', '12366', '12367', '12368', '12369', '12370', '12371', '12372', '12373', '12374', '12375', '12376', '12377', '12378', '12379', '12380', '12381', '12382', '12383', '12384', '12385', '12386', '12387', '12388', '12389', '12390', '12391', '12392', '12393', '12394', '12395', '12396', '12397', '12398', '12399', '12400', '12401', '12402', '12403', '12404', '12405', '12406', '12407', '12408', '12409', '12410', '12411', '12412', '12413', '12414', '12415', '12416', '12417', '12418', '12419', '12420', '12421', '12422', '12423', '12424', '12425', '12426', '12427', '12428', '12429', '12430', '12431', '12432', '12433', '12434', '12435', '12436', '12437', '12438', '12439', '12440', '12441', '12442', '12443', '12444', '12445', '12446', '12447', '12448', '12449', '12450', '12451', '12452', '12453', '12454', '12455', '12456', '12457', '12458', '12459', '12460', '12461', '12462', '12463', '12464', '12465', '12466', '12467', '12468', '12469', '12470', '12471', '12472', '12473', '12474', '12475', '12476', '12477', '12478', '12479', '12480', '12481', '12482', '12483', '12484', '12485', '12486', '12487', '12488', '12489', '12490', '12491', '12492', '12493', '12494', '12495', '12496', '12497', '12498', '12499', '12500', '12501', '12502', '12503', '12504', '12505', '12506', '12507', '12508', '12509', '12510', '12511', '12512', '12513', '12514', '12515', '12516', '12517', '12518', '12519', '12520', '12521', '12522', '12523', '12524', '12525', '12526', '12527', '12528', '12529', '12530', '12531', '12532', '12533', '12534', '12535', '12536', '12537', '12538', '12539', '12540', '12541', '12542', '12543', '12544', '12545', '12546', '12547', '12548', '12549', '12550', '12551', '12552', '12553', '12554', '12555', '12556', '12557', '12558', '12559', '12560', '12561', '12562', '12563', '12564', '12565', '12566', '12567', '12568', '12569', '12570', '12571', '12572', '12573', '12574', '12575', '12576', '12577', '12578', '12579', '12580', '12581', '12582', '12583', '12584', '12585', '12586', '12587', '12588', '12589', '12590', '12591', '12592', '12593', '12594', '12595', '12596', '12597', '12598', '12599', '12600', '12601', '12602', '12603', '12604', '12605', '12606', '12607', '12608', '12609', '12610', '12611', '12612', '12613', '12614', '12615', '12616', '12617', '12618', '12619', '12620', '12621', '12622', '12623', '12624', '12625', '12626', '12627', '12628', '12629', '12630', '12631', '12632', '12633', '12634', '12635', '12636', '12637', '12638', '12639', '12640', '12641', '12642', '12643', '12644', '12645', '12646', '12647', '12648', '12649', '12650', '12651', '12652', '12653', '12654', '12655', '12656', '12657', '12658', '12659', '12660', '12661', '12662', '12663', '12664', '12665', '12666', '12667', '12668', '12669', '12670', '12671', '12672', '12673', '12674', '12675', '12676', '12677', '12678', '12679', '12680', '12681', '12682', '12683', '12684', '12685', '12686', '12687', '12688', '12689', '12690', '12691', '12692', '12693', '12694', '12695', '12696', '12697', '12698', '12699', '12700', '12701', '12702', '12703', '12704', '12705', '12706', '12707', '12708', '12709', '12710', '12711', '12712', '12713', '12714', '12715', '12716', '12717', '12718', '12719', '12720', '12721', '12722', '12723', '12724', '12725', '12726', '12727', '12728', '12729', '12730', '12731', '12732', '12733', '12734', '12735', '12736', '12737', '12738', '12739', '12740', '12741', '12742', '12743', '12744', '12745', '12746', '12747', '12748', '12749', '12750', '12751', '12752', '12753', '12754', '12755', '12756', '12757', '12758', '12759', '12760', '12761', '12762', '12763', '12764', '12765', '12766', '12767', '12768', '12769', '12770', '12771', '12772', '12773', '12774', '12775', '12776', '12777', '12778', '12779', '12780', '12781', '12782', '12783', '12784', '12785', '12786', '12787', '12788', '12789', '12790', '12791', '12792', '12793', '12794', '12795', '12796', '12797', '12798', '12799', '12800', '12801', '12802', '12803', '12804', '12805', '12806', '12807', '12808', '12809', '12810', '12811', '12812', '12813', '12814', '12815', '12816', '12817', '12818', '12819', '12820', '12821', '12822', '12823', '12824', '12825', '12826', '12827', '12828', '12829', '12830', '12831', '12832', '12833', '12834', '12835', '12836', '12837', '12838', '12839', '12840', '12841', '12842', '12843', '12844', '12845', '12846', '12847', '12848', '12849', '12850', '12851', '12852', '12853', '12854', '12855', '12856', '12857', '12858', '12859', '12860', '12861', '12862', '12863', '12864', '12865', '12866', '12867', '12868', '12869', '12870', '12871', '12872', '12873', '12874', '12875', '12876', '12877', '12878', '12879', '12880', '12881', '12882', '12883', '12884', '12885', '12886', '12887', '12888', '12889', '12890', '12891', '12892', '12893', '12894', '12895', '12896', '12897', '12898', '12899', '12900', '12901', '12902', '12903', '12904', '12905', '12906', '12907', '12908', '12909', '12910', '12911', '12912', '12913', '12914', '12915', '12916', '12917', '12918', '12919', '12920', '12921', '12922', '12923', '12924', '12925', '12926', '12927', '12928', '12929', '12930', '12931', '12932', '12933', '12934', '12935', '12936', '12937', '12938', '12939', '12940', '12941', '12942', '12943', '12944', '12945', '12946', '12947', '12948', '12949', '12950', '12951', '12952', '12953', '12954', '12955', '12956', '12957', '12958', '12959', '12960', '12961', '12962', '12963', '12964', '12965', '12966', '12967', '12968', '12969', '12970', '12971', '12972', '12973', '12974', '12975', '12976', '12977', '12978', '12979', '12980', '12981', '12982', '12983', '12984', '12985', '12986', '12987', '12988', '12989', '12990', '12991', '12992', '12993', '12994', '12995', '12996', '12997', '12998', '12999', '13000', '13001', '13002', '13003', '13004', '13005', '13006', '13007', '13008', '13009', '13010', '13011', '13012', '13013', '13014', '13015', '13016', '13017', '13018', '13019', '13020', '13021', '13022', '13023', '13024', '13025', '13026', '13027', '13028', '13029', '13030', '13031', '13032', '13033', '13034', '13035', '13036', '13037', '13038', '13039', '13040', '13041', '13042', '13043', '13044', '13045', '13046', '13047', '13048', '13049', '13050', '13051', '13052', '13053', '13054', '13055', '13056', '13057', '13058', '13059', '13060', '13061', '13062', '13063', '13064', '13065', '13066', '13067', '13068', '13069', '13070', '13071', '13072', '13073', '13074', '13075', '13076', '13077', '13078', '13079', '13080', '13081', '13082', '13083', '13084', '13085', '13086', '13087', '13088', '13089', '13090', '13091', '13092', '13093', '13094', '13095', '13096', '13097', '13098', '13099', '13100', '13101', '13102', '13103', '13104', '13105', '13106', '13107', '13108', '13109', '13110', '13111', '13112', '13113', '13114', '13115', '13116', '13117', '13118', '13119', '13120', '13121', '13122', '13123', '13124', '13125', '13126', '13127', '13128', '13129', '13130', '13131', '13132', '13133', '13134', '13135', '13136', '13137', '13138', '13139', '13140', '13141', '13142', '13143', '13144', '13145', '13146', '13147', '13148', '13149', '13150', '13151', '13152', '13153', '13154', '13155', '13156', '13157', '13158', '13159', '13160', '13161', '13162', '13163', '13164', '13165', '13166', '13167', '13168', '13169', '13170', '13171', '13172', '13173', '13174', '13175', '13176', '13177', '13178', '13179', '13180', '13181', '13182', '13183', '13184', '13185', '13186', '13187', '13188', '13189', '13190', '13191', '13192', '13193', '13194', '13195', '13196', '13197', '13198', '13199', '13200', '13201', '13202', '13203', '13204', '13205', '13206', '13207', '13208', '13209', '13210', '13211', '13212', '13213', '13214', '13215', '13216', '13217', '13218', '13219', '13220', '13221', '13222', '13223', '13224', '13225', '13226', '13227', '13228', '13229', '13230', '13231', '13232', '13233', '13234', '13235', '13236', '13237', '13238', '13239', '13240', '13241', '13242', '13243', '13244', '13245', '13246', '13247', '13248', '13249', '13250', '13251', '13252', '13253', '13254', '13255', '13256', '13257', '13258', '13259', '13260', '13261', '13262', '13263', '13264', '13265', '13266', '13267', '13268', '13269', '13270', '13271', '13272', '13273', '13274', '13275', '13276', '13277', '13278', '13279', '13280', '13281', '13282', '13283', '13284', '13285', '13286', '13287', '13288', '13289', '13290', '13291', '13292', '13293', '13294', '13295', '13296', '13297', '13298', '13299', '13300', '13301', '13302', '13303', '13304', '13305', '13306', '13307', '13308', '13309', '13310', '13311', '13312', '13313', '13314', '13315', '13316', '13317', '13318', '13319', '13320', '13321', '13322', '13323', '13324', '13325', '13326', '13327', '13328', '13329', '13330', '13331', '13332', '13333', '13334', '13335', '13336', '13337', '13338', '13339', '13340', '13341', '13342', '13343', '13344', '13345', '13346', '13347', '13348', '13349', '13350', '13351', '13352', '13353', '13354', '13355', '13356', '13357', '13358', '13359', '13360', '13361', '13362', '13363', '13364', '13365', '13366', '13367', '13368', '13369', '13370', '13371', '13372', '13373', '13374', '13375', '13376', '13377', '13378', '13379', '13380', '13381', '13382', '13383', '13384', '13385', '13386', '13387', '13388', '13389', '13390', '13391', '13392', '13393', '13394', '13395', '13396', '13397', '13398', '13399', '13400', '13401', '13402', '13403', '13404', '13405', '13406', '13407', '13408', '13409', '13410', '13411', '13412', '13413', '13414', '13415', '13416', '13417', '13418', '13419', '13420', '13421', '13422', '13423', '13424', '13425', '13426', '13427', '13428', '13429', '13430', '13431', '13432', '13433', '13434', '13435', '13436', '13437', '13438', '13439', '13440', '13441', '13442', '13443', '13444', '13445', '13446', '13447', '13448', '13449', '13450', '13451', '13452', '13453', '13454', '13455', '13456', '13457', '13458', '13459', '13460', '13461', '13462', '13463', '13464', '13465', '13466', '13467', '13468', '13469', '13470', '13471', '13472', '13473', '13474', '13475', '13476', '13477', '13478', '13479', '13480', '13481', '13482', '13483', '13484', '13485', '13486', '13487', '13488', '13489', '13490', '13491', '13492', '13493', '13494', '13495', '13496', '13497', '13498', '13499', '13500', '13501', '13502', '13503', '13504', '13505', '13506', '13507', '13508', '13509', '13510', '13511', '13512', '13513', '13514', '13515', '13516', '13517', '13518', '13519', '13520', '13521', '13522', '13523', '13524', '13525', '13526', '13527', '13528', '13529', '13530', '13531', '13532', '13533', '13534', '13535', '13536', '13537', '13538', '13539', '13540', '13541', '13542', '13543', '13544', '13545', '13546', '13547', '13548', '13549', '13550', '13551', '13552', '13553', '13554', '13555', '13556', '13557', '13558', '13559', '13560', '13561', '13562', '13563', '13564', '13565', '13566', '13567', '13568', '13569', '13570', '13571', '13572', '13573', '13574', '13575', '13576', '13577', '13578', '13579', '13580', '13581', '13582', '13583', '13584', '13585', '13586', '13587', '13588', '13589', '13590', '13591', '13592', '13593', '13594', '13595', '13596', '13597', '13598', '13599', '13600', '13601', '13602', '13603', '13604', '13605', '13606', '13607', '13608', '13609', '13610', '13611', '13612', '13613', '13614', '13615', '13616', '13617', '13618', '13619', '13620', '13621', '13622', '13623', '13624', '13625', '13626', '13627', '13628', '13629', '13630', '13631', '13632', '13633', '13634', '13635', '13636', '13637', '13638', '13639', '13640', '13641', '13642', '13643', '13644', '13645', '13646', '13647', '13648', '13649', '13650', '13651', '13652', '13653', '13654', '13655', '13656', '13657', '13658', '13659', '13660', '13661', '13662', '13663', '13664', '13665', '13666', '13667', '13668', '13669', '13670', '13671', '13672', '13673', '13674', '13675', '13676', '13677', '13678', '13679', '13680', '13681', '13682', '13683', '13684', '13685', '13686', '13687', '13688', '13689', '13690', '13691', '13692', '13693', '13694', '13695', '13696', '13697', '13698', '13699', '13700', '13701', '13702', '13703', '13704', '13705', '13706', '13707', '13708', '13709', '13710', '13711', '13712', '13713', '13714', '13715', '13716', '13717', '13718', '13719', '13720', '13721', '13722', '13723', '13724', '13725', '13726', '13727', '13728', '13729', '13730', '13731', '13732', '13733', '13734', '13735', '13736', '13737', '13738', '13739', '13740', '13741', '13742', '13743', '13744', '13745', '13746', '13747', '13748', '13749', '13750', '13751', '13752', '13753', '13754', '13755', '13756', '13757', '13758', '13759', '13760', '13761', '13762', '13763', '13764', '13765', '13766', '13767', '13768', '13769', '13770', '13771', '13772', '13773', '13774', '13775', '13776', '13777', '13778', '13779', '13780', '13781', '13782', '13783', '13784', '13785', '13786', '13787', '13788', '13789', '13790', '13791', '13792', '13793', '13794', '13795', '13796', '13797', '13798', '13799', '13800', '13801', '13802', '13803', '13804', '13805', '13806', '13807', '13808', '13809', '13810', '13811', '13812', '13813', '13814', '13815', '13816', '13817', '13818', '13819', '13820', '13821', '13822', '13823', '13824', '13825', '13826', '13827', '13828', '13829', '13830', '13831', '13832', '13833', '13834', '13835', '13836', '13837', '13838', '13839', '13840', '13841', '13842', '13843', '13844', '13845', '13846', '13847', '13848', '13849', '13850', '13851', '13852', '13853', '13854', '13855', '13856', '13857', '13858', '13859', '13860', '13861', '13862', '13863', '13864', '13865', '13866', '13867', '13868', '13869', '13870', '13871', '13872', '13873', '13874', '13875', '13876', '13877', '13878', '13879', '13880', '13881', '13882', '13883', '13884', '13885', '13886', '13887', '13888', '13889', '13890', '13891', '13892', '13893', '13894', '13895', '13896', '13897', '13898', '13899', '13900', '13901', '13902', '13903', '13904', '13905', '13906', '13907', '13908', '13909', '13910', '13911', '13912', '13913', '13914', '13915', '13916', '13917', '13918', '13919', '13920', '13921', '13922', '13923', '13924', '13925', '13926', '13927', '13928', '13929', '13930', '13931', '13932', '13933', '13934', '13935', '13936', '13937', '13938', '13939', '13940', '13941', '13942', '13943', '13944', '13945', '13946', '13947', '13948', '13949', '13950', '13951', '13952', '13953', '13954', '13955', '13956', '13957', '13958', '13959', '13960', '13961', '13962', '13963', '13964', '13965', '13966', '13967', '13968', '13969', '13970', '13971', '13972', '13973', '13974', '13975', '13976', '13977', '13978', '13979', '13980', '13981', '13982', '13983', '13984', '13985', '13986', '13987', '13988', '13989', '13990', '13991', '13992', '13993', '13994', '13995', '13996', '13997', '13998', '13999', '14000', '14001', '14002', '14003', '14004', '14005', '14006', '14007', '14008', '14009', '14010', '14011', '14012', '14013', '14014', '14015', '14016', '14017', '14018', '14019', '14020', '14021', '14022', '14023', '14024', '14025', '14026', '14027', '14028', '14029', '14030', '14031', '14032', '14033', '14034', '14035', '14036', '14037', '14038', '14039', '14040', '14041', '14042', '14043', '14044', '14045', '14046', '14047', '14048', '14049', '14050', '14051', '14052', '14053', '14054', '14055', '14056', '14057', '14058', '14059', '14060', '14061', '14062', '14063', '14064', '14065', '14066', '14067', '14068', '14069', '14070', '14071', '14072', '14073', '14074', '14075', '14076', '14077', '14078', '14079', '14080', '14081', '14082', '14083', '14084', '14085', '14086', '14087', '14088', '14089', '14090', '14091', '14092', '14093', '14094', '14095', '14096', '14097', '14098', '14099', '14100', '14101', '14102', '14103', '14104', '14105', '14106', '14107', '14108', '14109', '14110', '14111', '14112', '14113', '14114', '14115', '14116', '14117', '14118', '14119', '14120', '14121', '14122', '14123', '14124', '14125', '14126', '14127', '14128', '14129', '14130', '14131', '14132', '14133', '14134', '14135', '14136', '14137', '14138', '14139', '14140', '14141', '14142', '14143', '14144', '14145', '14146', '14147', '14148', '14149', '14150', '14151', '14152', '14153', '14154', '14155', '14156', '14157', '14158', '14159', '14160', '14161', '14162', '14163', '14164', '14165', '14166', '14167', '14168', '14169', '14170', '14171', '14172', '14173', '14174', '14175', '14176', '14177', '14178', '14179', '14180', '14181', '14182', '14183', '14184', '14185', '14186', '14187', '14188', '14189', '14190', '14191', '14192', '14193', '14194', '14195', '14196', '14197', '14198', '14199', '14200', '14201', '14202', '14203', '14204', '14205', '14206', '14207', '14208', '14209', '14210', '14211', '14212', '14213', '14214', '14215', '14216', '14217', '14218', '14219', '14220', '14221', '14222', '14223', '14224', '14225', '14226', '14227', '14228', '14229', '14230', '14231', '14232', '14233', '14234', '14235', '14236', '14237', '14238', '14239', '14240', '14241', '14242', '14243', '14244', '14245', '14246', '14247', '14248', '14249', '14250', '14251', '14252', '14253', '14254', '14255', '14256', '14257', '14258', '14259', '14260', '14261', '14262', '14263', '14264', '14265', '14266', '14267', '14268', '14269', '14270', '14271', '14272', '14273', '14274', '14275', '14276', '14277', '14278', '14279', '14280', '14281', '14282', '14283', '14284', '14285', '14286', '14287', '14288', '14289', '14290', '14291', '14292', '14293', '14294', '14295', '14296', '14297', '14298', '14299', '14300', '14301', '14302', '14303', '14304', '14305', '14306', '14307', '14308', '14309', '14310', '14311', '14312', '14313', '14314', '14315', '14316', '14317', '14318', '14319', '14320', '14321', '14322', '14323', '14324', '14325', '14326', '14327', '14328', '14329', '14330', '14331', '14332', '14333', '14334', '14335', '14336', '14337', '14338', '14339', '14340', '14341', '14342', '14343', '14344', '14345', '14346', '14347', '14348', '14349', '14350', '14351', '14352', '14353', '14354', '14355', '14356', '14357', '14358', '14359', '14360', '14361', '14362', '14363', '14364', '14365', '14366', '14367', '14368', '14369', '14370', '14371', '14372', '14373', '14374', '14375', '14376', '14377', '14378', '14379', '14380', '14381', '14382', '14383', '14384', '14385', '14386', '14387', '14388', '14389', '14390', '14391', '14392', '14393', '14394', '14395', '14396', '14397', '14398', '14399', '14400', '14401', '14402', '14403', '14404', '14405', '14406', '14407', '14408', '14409', '14410', '14411', '14412', '14413', '14414', '14415', '14416', '14417', '14418', '14419', '14420', '14421', '14422', '14423', '14424', '14425', '14426', '14427', '14428', '14429', '14430', '14431', '14432', '14433', '14434', '14435', '14436', '14437', '14438', '14439', '14440', '14441', '14442', '14443', '14444', '14445', '14446', '14447', '14448', '14449', '14450', '14451', '14452', '14453', '14454', '14455', '14456', '14457', '14458', '14459', '14460', '14461', '14462', '14463', '14464', '14465', '14466', '14467', '14468', '14469', '14470', '14471', '14472', '14473', '14474', '14475', '14476', '14477', '14478', '14479', '14480', '14481', '14482', '14483', '14484', '14485', '14486', '14487', '14488', '14489', '14490', '14491', '14492', '14493', '14494', '14495', '14496', '14497', '14498', '14499', '14500', '14501', '14502', '14503', '14504', '14505', '14506', '14507', '14508', '14509', '14510', '14511', '14512', '14513', '14514', '14515', '14516', '14517', '14518', '14519', '14520', '14521', '14522', '14523', '14524', '14525', '14526', '14527', '14528', '14529', '14530', '14531', '14532', '14533', '14534', '14535', '14536', '14537', '14538', '14539', '14540', '14541', '14542', '14543', '14544', '14545', '14546', '14547', '14548', '14549', '14550', '14551', '14552', '14553', '14554', '14555', '14556', '14557', '14558', '14559', '14560', '14561', '14562', '14563', '14564', '14565', '14566', '14567', '14568', '14569', '14570', '14571', '14572', '14573', '14574', '14575', '14576', '14577', '14578', '14579', '14580', '14581', '14582', '14583', '14584', '14585', '14586', '14587', '14588', '14589', '14590', '14591', '14592', '14593', '14594', '14595', '14596', '14597', '14598', '14599', '14600', '14601', '14602', '14603', '14604', '14605', '14606', '14607', '14608', '14609', '14610', '14611', '14612', '14613', '14614', '14615', '14616', '14617', '14618', '14619', '14620', '14621', '14622', '14623', '14624', '14625', '14626', '14627', '14628', '14629', '14630', '14631', '14632', '14633', '14634', '14635', '14636', '14637', '14638', '14639', '14640', '14641', '14642', '14643', '14644', '14645', '14646', '14647', '14648', '14649', '14650', '14651', '14652', '14653', '14654', '14655', '14656', '14657', '14658', '14659', '14660', '14661', '14662', '14663', '14664', '14665', '14666', '14667', '14668', '14669', '14670', '14671', '14672', '14673', '14674', '14675', '14676', '14677', '14678', '14679', '14680', '14681', '14682', '14683', '14684', '14685', '14686', '14687', '14688', '14689', '14690', '14691', '14692', '14693', '14694', '14695', '14696', '14697', '14698', '14699', '14700', '14701', '14702', '14703', '14704', '14705', '14706', '14707', '14708', '14709', '14710', '14711', '14712', '14713', '14714', '14715', '14716', '14717', '14718', '14719', '14720', '14721', '14722', '14723', '14724', '14725', '14726', '14727', '14728', '14729', '14730', '14731', '14732', '14733', '14734', '14735', '14736', '14737', '14738', '14739', '14740', '14741', '14742', '14743', '14744', '14745', '14746', '14747', '14748', '14749', '14750', '14751', '14752', '14753', '14754', '14755', '14756', '14757', '14758', '14759', '14760', '14761', '14762', '14763', '14764', '14765', '14766', '14767', '14768', '14769', '14770', '14771', '14772', '14773', '14774', '14775', '14776', '14777', '14778', '14779', '14780', '14781', '14782', '14783', '14784', '14785', '14786', '14787', '14788', '14789', '14790', '14791', '14792', '14793', '14794', '14795', '14796', '14797', '14798', '14799', '14800', '14801', '14802', '14803', '14804', '14805', '14806', '14807', '14808', '14809', '14810', '14811', '14812', '14813', '14814', '14815', '14816', '14817', '14818', '14819', '14820', '14821', '14822', '14823', '14824', '14825', '14826', '14827', '14828', '14829', '14830', '14831', '14832', '14833', '14834', '14835', '14836', '14837', '14838', '14839', '14840', '14841', '14842', '14843', '14844', '14845', '14846', '14847', '14848', '14849', '14850', '14851', '14852', '14853', '14854', '14855', '14856', '14857', '14858', '14859', '14860', '14861', '14862', '14863', '14864', '14865', '14866', '14867', '14868', '14869', '14870', '14871', '14872', '14873', '14874', '14875', '14876', '14877', '14878', '14879', '14880', '14881', '14882', '14883', '14884', '14885', '14886', '14887', '14888', '14889', '14890', '14891', '14892', '14893', '14894', '14895', '14896', '14897', '14898', '14899', '14900', '14901', '14902', '14903', '14904', '14905', '14906', '14907', '14908', '14909', '14910', '14911', '14912', '14913', '14914', '14915', '14916', '14917', '14918', '14919', '14920', '14921', '14922', '14923', '14924', '14925', '14926', '14927', '14928', '14929', '14930', '14931', '14932', '14933', '14934', '14935', '14936', '14937', '14938', '14939', '14940', '14941', '14942', '14943', '14944', '14945', '14946', '14947', '14948', '14949', '14950', '14951', '14952', '14953', '14954', '14955', '14956', '14957', '14958', '14959', '14960', '14961', '14962', '14963', '14964', '14965', '14966', '14967', '14968', '14969', '14970', '14971', '14972', '14973', '14974', '14975', '14976', '14977', '14978', '14979', '14980', '14981', '14982', '14983', '14984', '14985', '14986', '14987', '14988', '14989', '14990', '14991', '14992', '14993', '14994', '14995', '14996', '14997', '14998', '14999', '15000', '15001', '15002', '15003', '15004', '15005', '15006', '15007', '15008', '15009', '15010', '15011', '15012', '15013', '15014', '15015', '15016', '15017', '15018', '15019', '15020', '15021', '15022', '15023', '15024', '15025', '15026', '15027', '15028', '15029', '15030', '15031', '15032', '15033', '15034', '15035', '15036', '15037', '15038', '15039', '15040', '15041', '15042', '15043', '15044', '15045', '15046', '15047', '15048', '15049', '15050', '15051', '15052', '15053', '15054', '15055', '15056', '15057', '15058', '15059', '15060', '15061', '15062', '15063', '15064', '15065', '15066', '15067', '15068', '15069', '15070', '15071', '15072', '15073', '15074', '15075', '15076', '15077', '15078', '15079', '15080', '15081', '15082', '15083', '15084', '15085', '15086', '15087', '15088', '15089', '15090', '15091', '15092', '15093', '15094', '15095', '15096', '15097', '15098', '15099', '15100', '15101', '15102', '15103', '15104', '15105', '15106', '15107', '15108', '15109', '15110', '15111', '15112', '15113', '15114', '15115', '15116', '15117', '15118', '15119', '15120', '15121', '15122', '15123', '15124', '15125', '15126', '15127', '15128', '15129', '15130', '15131', '15132', '15133', '15134', '15135', '15136', '15137', '15138', '15139', '15140', '15141', '15142', '15143', '15144', '15145', '15146', '15147', '15148', '15149', '15150', '15151', '15152', '15153', '15154', '15155', '15156', '15157', '15158', '15159', '15160', '15161', '15162', '15163', '15164', '15165', '15166', '15167', '15168', '15169', '15170', '15171', '15172', '15173', '15174', '15175', '15176', '15177', '15178', '15179', '15180', '15181', '15182', '15183', '15184', '15185', '15186', '15187', '15188', '15189', '15190', '15191', '15192', '15193', '15194', '15195', '15196', '15197', '15198', '15199', '15200', '15201', '15202', '15203', '15204', '15205', '15206', '15207', '15208', '15209', '15210', '15211', '15212', '15213', '15214', '15215', '15216', '15217', '15218', '15219', '15220', '15221', '15222', '15223', '15224', '15225', '15226', '15227', '15228', '15229', '15230', '15231', '15232', '15233', '15234', '15235', '15236', '15237', '15238', '15239', '15240', '15241', '15242', '15243', '15244', '15245', '15246', '15247', '15248', '15249', '15250', '15251', '15252', '15253', '15254', '15255', '15256', '15257', '15258', '15259', '15260', '15261', '15262', '15263', '15264', '15265', '15266', '15267', '15268', '15269', '15270', '15271', '15272', '15273', '15274', '15275', '15276', '15277', '15278', '15279', '15280', '15281', '15282', '15283', '15284', '15285', '15286', '15287', '15288', '15289', '15290', '15291', '15292', '15293', '15294', '15295', '15296', '15297', '15298', '15299', '15300', '15301', '15302', '15303', '15304', '15305', '15306', '15307', '15308', '15309', '15310', '15311', '15312', '15313', '15314', '15315', '15316', '15317', '15318', '15319', '15320', '15321', '15322', '15323', '15324', '15325', '15326', '15327', '15328', '15329', '15330', '15331', '15332', '15333', '15334', '15335', '15336', '15337', '15338', '15339', '15340', '15341', '15342', '15343', '15344', '15345', '15346', '15347', '15348', '15349', '15350', '15351', '15352', '15353', '15354', '15355', '15356', '15357', '15358', '15359', '15360', '15361', '15362', '15363', '15364', '15365', '15366', '15367', '15368', '15369', '15370', '15371', '15372', '15373', '15374', '15375', '15376', '15377', '15378', '15379', '15380', '15381', '15382', '15383', '15384', '15385', '15386', '15387', '15388', '15389', '15390', '15391', '15392', '15393', '15394', '15395', '15396', '15397', '15398', '15399', '15400', '15401', '15402', '15403', '15404', '15405', '15406', '15407', '15408', '15409', '15410', '15411', '15412', '15413', '15414', '15415', '15416', '15417', '15418', '15419', '15420', '15421', '15422', '15423', '15424', '15425', '15426', '15427', '15428', '15429', '15430', '15431', '15432', '15433', '15434', '15435', '15436', '15437', '15438', '15439', '15440', '15441', '15442', '15443', '15444', '15445', '15446', '15447', '15448', '15449', '15450', '15451', '15452', '15453', '15454', '15455', '15456', '15457', '15458', '15459', '15460', '15461', '15462', '15463', '15464', '15465', '15466', '15467', '15468', '15469', '15470', '15471', '15472', '15473', '15474', '15475', '15476', '15477', '15478', '15479', '15480', '15481', '15482', '15483', '15484', '15485', '15486', '15487', '15488', '15489', '15490', '15491', '15492', '15493', '15494', '15495', '15496', '15497', '15498', '15499', '15500', '15501', '15502', '15503', '15504', '15505', '15506', '15507', '15508', '15509', '15510', '15511', '15512', '15513', '15514', '15515', '15516', '15517', '15518', '15519', '15520', '15521', '15522', '15523', '15524', '15525', '15526', '15527', '15528', '15529', '15530', '15531', '15532', '15533', '15534', '15535', '15536', '15537', '15538', '15539', '15540', '15541', '15542', '15543', '15544', '15545', '15546', '15547', '15548', '15549', '15550', '15551', '15552', '15553', '15554', '15555', '15556', '15557', '15558', '15559', '15560', '15561', '15562', '15563', '15564', '15565', '15566', '15567', '15568', '15569', '15570', '15571', '15572', '15573', '15574', '15575', '15576', '15577', '15578', '15579', '15580', '15581', '15582', '15583', '15584', '15585', '15586', '15587', '15588', '15589', '15590', '15591', '15592', '15593', '15594', '15595', '15596', '15597', '15598', '15599', '15600', '15601', '15602', '15603', '15604', '15605', '15606', '15607', '15608', '15609', '15610', '15611', '15612', '15613', '15614', '15615', '15616', '15617', '15618', '15619', '15620', '15621', '15622', '15623', '15624', '15625', '15626', '15627', '15628', '15629', '15630', '15631', '15632', '15633', '15634', '15635', '15636', '15637', '15638', '15639', '15640', '15641', '15642', '15643', '15644', '15645', '15646', '15647', '15648', '15649', '15650', '15651', '15652', '15653', '15654', '15655', '15656', '15657', '15658', '15659', '15660', '15661', '15662', '15663', '15664', '15665', '15666', '15667', '15668', '15669', '15670', '15671', '15672', '15673', '15674', '15675', '15676', '15677', '15678', '15679', '15680', '15681', '15682', '15683', '15684', '15685', '15686', '15687', '15688', '15689', '15690', '15691', '15692', '15693', '15694', '15695', '15696', '15697', '15698', '15699', '15700', '15701', '15702', '15703', '15704', '15705', '15706', '15707', '15708', '15709', '15710', '15711', '15712', '15713', '15714', '15715', '15716', '15717', '15718', '15719', '15720', '15721', '15722', '15723', '15724', '15725', '15726', '15727', '15728', '15729', '15730', '15731', '15732', '15733', '15734', '15735', '15736', '15737', '15738', '15739', '15740', '15741', '15742', '15743', '15744', '15745', '15746', '15747', '15748', '15749', '15750', '15751', '15752', '15753', '15754', '15755', '15756', '15757', '15758', '15759', '15760', '15761', '15762', '15763', '15764', '15765', '15766', '15767', '15768', '15769', '15770', '15771', '15772', '15773', '15774', '15775', '15776', '15777', '15778', '15779', '15780', '15781', '15782', '15783', '15784', '15785', '15786', '15787', '15788', '15789', '15790', '15791', '15792', '15793', '15794', '15795', '15796', '15797', '15798', '15799', '15800', '15801', '15802', '15803', '15804', '15805', '15806', '15807', '15808', '15809', '15810', '15811', '15812', '15813', '15814', '15815', '15816', '15817', '15818', '15819', '15820', '15821', '15822', '15823', '15824', '15825', '15826', '15827', '15828', '15829', '15830', '15831', '15832', '15833', '15834', '15835', '15836', '15837', '15838', '15839', '15840', '15841', '15842', '15843', '15844', '15845', '15846', '15847', '15848', '15849', '15850', '15851', '15852', '15853', '15854', '15855', '15856', '15857', '15858', '15859', '15860', '15861', '15862', '15863', '15864', '15865', '15866', '15867', '15868', '15869', '15870', '15871', '15872', '15873', '15874', '15875', '15876', '15877', '15878', '15879', '15880', '15881', '15882', '15883', '15884', '15885', '15886', '15887', '15888', '15889', '15890', '15891', '15892', '15893', '15894', '15895', '15896', '15897', '15898', '15899', '15900', '15901', '15902', '15903', '15904', '15905', '15906', '15907', '15908', '15909', '15910', '15911', '15912', '15913', '15914', '15915', '15916', '15917', '15918', '15919', '15920', '15921', '15922', '15923', '15924', '15925', '15926', '15927', '15928', '15929', '15930', '15931', '15932', '15933', '15934', '15935', '15936', '15937', '15938', '15939', '15940', '15941', '15942', '15943', '15944', '15945', '15946', '15947', '15948', '15949', '15950', '15951', '15952', '15953', '15954', '15955', '15956', '15957', '15958', '15959', '15960', '15961', '15962', '15963', '15964', '15965', '15966', '15967', '15968', '15969', '15970', '15971', '15972', '15973', '15974', '15975', '15976', '15977', '15978', '15979', '15980', '15981', '15982', '15983', '15984', '15985', '15986', '15987', '15988', '15989', '15990', '15991', '15992', '15993', '15994', '15995', '15996', '15997', '15998', '15999', '16000', '16001', '16002', '16003', '16004', '16005', '16006', '16007', '16008', '16009', '16010', '16011', '16012', '16013', '16014', '16015', '16016', '16017', '16018', '16019', '16020', '16021', '16022', '16023', '16024', '16025', '16026', '16027', '16028', '16029', '16030', '16031', '16032', '16033', '16034', '16035', '16036', '16037', '16038', '16039', '16040', '16041', '16042', '16043', '16044', '16045', '16046', '16047', '16048', '16049', '16050', '16051', '16052', '16053', '16054', '16055', '16056', '16057', '16058', '16059', '16060', '16061', '16062', '16063', '16064', '16065', '16066', '16067', '16068', '16069', '16070', '16071', '16072', '16073', '16074', '16075', '16076', '16077', '16078', '16079', '16080', '16081', '16082', '16083', '16084', '16085', '16086', '16087', '16088', '16089', '16090', '16091', '16092', '16093', '16094', '16095', '16096', '16097', '16098', '16099', '16100', '16101', '16102', '16103', '16104', '16105', '16106', '16107', '16108', '16109', '16110', '16111', '16112', '16113', '16114', '16115', '16116', '16117', '16118', '16119', '16120', '16121', '16122', '16123', '16124', '16125', '16126', '16127', '16128', '16129', '16130', '16131', '16132', '16133', '16134', '16135', '16136', '16137', '16138', '16139', '16140', '16141', '16142', '16143', '16144', '16145', '16146', '16147', '16148', '16149', '16150', '16151', '16152', '16153', '16154', '16155', '16156', '16157', '16158', '16159', '16160', '16161', '16162', '16163', '16164', '16165', '16166', '16167', '16168', '16169', '16170', '16171', '16172', '16173', '16174', '16175', '16176', '16177', '16178', '16179', '16180', '16181', '16182', '16183', '16184', '16185', '16186', '16187', '16188', '16189', '16190', '16191', '16192', '16193', '16194', '16195', '16196', '16197', '16198', '16199', '16200', '16201', '16202', '16203', '16204', '16205', '16206', '16207', '16208', '16209', '16210', '16211', '16212', '16213', '16214', '16215', '16216', '16217', '16218', '16219', '16220', '16221', '16222', '16223', '16224', '16225', '16226', '16227', '16228', '16229', '16230', '16231', '16232', '16233', '16234', '16235', '16236', '16237', '16238', '16239', '16240', '16241', '16242', '16243', '16244', '16245', '16246', '16247', '16248', '16249', '16250', '16251', '16252', '16253', '16254', '16255', '16256', '16257', '16258', '16259', '16260', '16261', '16262', '16263', '16264', '16265', '16266', '16267', '16268', '16269', '16270', '16271', '16272', '16273', '16274', '16275', '16276', '16277', '16278', '16279', '16280', '16281', '16282', '16283', '16284', '16285', '16286', '16287', '16288', '16289', '16290', '16291', '16292', '16293', '16294', '16295', '16296', '16297', '16298', '16299', '16300', '16301', '16302', '16303', '16304', '16305', '16306', '16307', '16308', '16309', '16310', '16311', '16312', '16313', '16314', '16315', '16316', '16317', '16318', '16319', '16320', '16321', '16322', '16323', '16324', '16325', '16326', '16327', '16328', '16329', '16330', '16331', '16332', '16333', '16334', '16335', '16336', '16337', '16338', '16339', '16340', '16341', '16342', '16343', '16344', '16345', '16346', '16347', '16348', '16349', '16350', '16351', '16352', '16353', '16354', '16355', '16356', '16357', '16358', '16359', '16360', '16361', '16362', '16363', '16364', '16365', '16366', '16367', '16368', '16369', '16370', '16371', '16372', '16373', '16374', '16375', '16376', '16377', '16378', '16379', '16380', '16381', '16382', '16383', '16384', '16385', '16386', '16387', '16388', '16389', '16390', '16391', '16392', '16393', '16394', '16395', '16396', '16397', '16398', '16399', '16400', '16401', '16402', '16403', '16404', '16405', '16406', '16407', '16408', '16409', '16410', '16411', '16412', '16413', '16414', '16415', '16416', '16417', '16418', '16419', '16420', '16421', '16422', '16423', '16424', '16425', '16426', '16427', '16428', '16429', '16430', '16431', '16432', '16433', '16434', '16435', '16436', '16437', '16438', '16439', '16440', '16441', '16442', '16443', '16444', '16445', '16446', '16447', '16448', '16449', '16450', '16451', '16452', '16453', '16454', '16455', '16456', '16457', '16458', '16459', '16460', '16461', '16462', '16463', '16464', '16465', '16466', '16467', '16468', '16469', '16470', '16471', '16472', '16473', '16474', '16475', '16476', '16477', '16478', '16479', '16480', '16481', '16482', '16483', '16484', '16485', '16486', '16487', '16488', '16489', '16490', '16491', '16492', '16493', '16494', '16495', '16496', '16497', '16498', '16499', '16500', '16501', '16502', '16503', '16504', '16505', '16506', '16507', '16508', '16509', '16510', '16511', '16512', '16513', '16514', '16515', '16516', '16517', '16518', '16519', '16520', '16521', '16522', '16523', '16524', '16525', '16526', '16527', '16528', '16529', '16530', '16531', '16532', '16533', '16534', '16535', '16536', '16537', '16538', '16539', '16540', '16541', '16542', '16543', '16544', '16545', '16546', '16547', '16548', '16549', '16550', '16551', '16552', '16553', '16554', '16555', '16556', '16557', '16558', '16559', '16560', '16561', '16562', '16563', '16564', '16565', '16566', '16567', '16568', '16569', '16570', '16571', '16572', '16573', '16574', '16575', '16576', '16577', '16578', '16579', '16580', '16581', '16582', '16583', '16584', '16585', '16586', '16587', '16588', '16589', '16590', '16591', '16592', '16593', '16594', '16595', '16596', '16597', '16598', '16599', '16600', '16601', '16602', '16603', '16604', '16605', '16606', '16607', '16608', '16609', '16610', '16611', '16612', '16613', '16614', '16615', '16616', '16617', '16618', '16619', '16620', '16621', '16622', '16623', '16624', '16625', '16626', '16627', '16628', '16629', '16630', '16631', '16632', '16633', '16634', '16635', '16636', '16637', '16638', '16639', '16640', '16641', '16642', '16643', '16644', '16645', '16646', '16647', '16648', '16649', '16650', '16651', '16652', '16653', '16654', '16655', '16656', '16657', '16658', '16659', '16660', '16661', '16662', '16663', '16664', '16665', '16666', '16667', '16668', '16669', '16670', '16671', '16672', '16673', '16674', '16675', '16676', '16677', '16678', '16679', '16680', '16681', '16682', '16683', '16684', '16685', '16686', '16687', '16688', '16689', '16690', '16691', '16692', '16693', '16694', '16695', '16696', '16697', '16698', '16699', '16700', '16701', '16702', '16703', '16704', '16705', '16706', '16707', '16708', '16709', '16710', '16711', '16712', '16713', '16714', '16715', '16716', '16717', '16718', '16719', '16720', '16721', '16722', '16723', '16724', '16725', '16726', '16727', '16728', '16729', '16730', '16731', '16732', '16733', '16734', '16735', '16736', '16737', '16738', '16739', '16740', '16741', '16742', '16743', '16744', '16745', '16746', '16747', '16748', '16749', '16750', '16751', '16752', '16753', '16754', '16755', '16756', '16757', '16758', '16759', '16760', '16761', '16762', '16763', '16764', '16765', '16766', '16767', '16768', '16769', '16770', '16771', '16772', '16773', '16774', '16775', '16776', '16777', '16778', '16779', '16780', '16781', '16782', '16783', '16784', '16785', '16786', '16787', '16788', '16789', '16790', '16791', '16792', '16793', '16794', '16795', '16796', '16797', '16798', '16799', '16800', '16801', '16802', '16803', '16804', '16805', '16806', '16807', '16808', '16809', '16810', '16811', '16812', '16813', '16814', '16815', '16816', '16817', '16818', '16819', '16820', '16821', '16822', '16823', '16824', '16825', '16826', '16827', '16828', '16829', '16830', '16831', '16832', '16833', '16834', '16835', '16836', '16837', '16838', '16839', '16840', '16841', '16842', '16843', '16844', '16845', '16846', '16847', '16848', '16849', '16850', '16851', '16852', '16853', '16854', '16855', '16856', '16857', '16858', '16859', '16860', '16861', '16862', '16863', '16864', '16865', '16866', '16867', '16868', '16869', '16870', '16871', '16872', '16873', '16874', '16875', '16876', '16877', '16878', '16879', '16880', '16881', '16882', '16883', '16884', '16885', '16886', '16887', '16888', '16889', '16890', '16891', '16892', '16893', '16894', '16895', '16896', '16897', '16898', '16899', '16900', '16901', '16902', '16903', '16904', '16905', '16906', '16907', '16908', '16909', '16910', '16911', '16912', '16913', '16914', '16915', '16916', '16917', '16918', '16919', '16920', '16921', '16922', '16923', '16924', '16925', '16926', '16927', '16928', '16929', '16930', '16931', '16932', '16933', '16934', '16935', '16936', '16937', '16938', '16939', '16940', '16941', '16942', '16943', '16944', '16945', '16946', '16947', '16948', '16949', '16950', '16951', '16952', '16953', '16954', '16955', '16956', '16957', '16958', '16959', '16960', '16961', '16962', '16963', '16964', '16965', '16966', '16967', '16968', '16969', '16970', '16971', '16972', '16973', '16974', '16975', '16976', '16977', '16978', '16979', '16980', '16981', '16982', '16983', '16984', '16985', '16986', '16987', '16988', '16989', '16990', '16991', '16992', '16993', '16994', '16995', '16996', '16997', '16998', '16999', '17000', '17001', '17002', '17003', '17004', '17005', '17006', '17007', '17008', '17009', '17010', '17011', '17012', '17013', '17014', '17015', '17016', '17017', '17018', '17019', '17020', '17021', '17022', '17023', '17024', '17025', '17026', '17027', '17028', '17029', '17030', '17031', '17032', '17033', '17034', '17035', '17036', '17037', '17038', '17039', '17040', '17041', '17042', '17043', '17044', '17045', '17046', '17047', '17048', '17049', '17050', '17051', '17052', '17053', '17054', '17055', '17056', '17057', '17058', '17059', '17060', '17061', '17062', '17063', '17064', '17065', '17066', '17067', '17068', '17069', '17070', '17071', '17072', '17073', '17074', '17075', '17076', '17077', '17078', '17079', '17080', '17081', '17082', '17083', '17084', '17085', '17086', '17087', '17088', '17089', '17090', '17091', '17092', '17093', '17094', '17095', '17096', '17097', '17098', '17099', '17100', '17101', '17102', '17103', '17104', '17105', '17106', '17107', '17108', '17109', '17110', '17111', '17112', '17113', '17114', '17115', '17116', '17117', '17118', '17119', '17120', '17121', '17122', '17123', '17124', '17125', '17126', '17127', '17128', '17129', '17130', '17131', '17132', '17133', '17134', '17135', '17136', '17137', '17138', '17139', '17140', '17141', '17142', '17143', '17144', '17145', '17146', '17147', '17148', '17149', '17150', '17151', '17152', '17153', '17154', '17155', '17156', '17157', '17158', '17159', '17160', '17161', '17162', '17163', '17164', '17165', '17166', '17167', '17168', '17169', '17170', '17171', '17172', '17173', '17174', '17175', '17176', '17177', '17178', '17179', '17180', '17181', '17182', '17183', '17184', '17185', '17186', '17187', '17188', '17189', '17190', '17191', '17192', '17193', '17194', '17195', '17196', '17197', '17198', '17199', '17200', '17201', '17202', '17203', '17204', '17205', '17206', '17207', '17208', '17209', '17210', '17211', '17212', '17213', '17214', '17215', '17216', '17217', '17218', '17219', '17220', '17221', '17222', '17223', '17224', '17225', '17226', '17227', '17228', '17229', '17230', '17231', '17232', '17233', '17234', '17235', '17236', '17237', '17238', '17239', '17240', '17241', '17242', '17243', '17244', '17245', '17246', '17247', '17248', '17249', '17250', '17251', '17252', '17253', '17254', '17255', '17256', '17257', '17258', '17259', '17260', '17261', '17262', '17263', '17264', '17265', '17266', '17267', '17268', '17269', '17270', '17271', '17272', '17273', '17274', '17275', '17276', '17277', '17278', '17279', '17280', '17281', '17282', '17283', '17284', '17285', '17286', '17287', '17288', '17289', '17290', '17291', '17292', '17293', '17294', '17295', '17296', '17297', '17298', '17299', '17300', '17301', '17302', '17303', '17304', '17305', '17306', '17307', '17308', '17309', '17310', '17311', '17312', '17313', '17314', '17315', '17316', '17317', '17318', '17319', '17320', '17321', '17322', '17323', '17324', '17325', '17326', '17327', '17328', '17329', '17330', '17331', '17332', '17333', '17334', '17335', '17336', '17337', '17338', '17339', '17340', '17341', '17342', '17343', '17344', '17345', '17346', '17347', '17348', '17349', '17350', '17351', '17352', '17353', '17354', '17355', '17356', '17357', '17358', '17359', '17360', '17361', '17362', '17363', '17364', '17365', '17366', '17367', '17368', '17369', '17370', '17371', '17372', '17373', '17374', '17375', '17376', '17377', '17378', '17379', '17380', '17381', '17382', '17383', '17384', '17385', '17386', '17387', '17388', '17389', '17390', '17391', '17392', '17393', '17394', '17395', '17396', '17397', '17398', '17399', '17400', '17401', '17402', '17403', '17404', '17405', '17406', '17407', '17408', '17409', '17410', '17411', '17412', '17413', '17414', '17415', '17416', '17417', '17418', '17419', '17420', '17421', '17422', '17423', '17424', '17425', '17426', '17427', '17428', '17429', '17430', '17431', '17432', '17433', '17434', '17435', '17436', '17437', '17438', '17439', '17440', '17441', '17442', '17443', '17444', '17445', '17446', '17447', '17448', '17449', '17450', '17451', '17452', '17453', '17454', '17455', '17456', '17457', '17458', '17459', '17460', '17461', '17462', '17463', '17464', '17465', '17466', '17467', '17468', '17469', '17470', '17471', '17472', '17473', '17474', '17475', '17476', '17477', '17478', '17479', '17480', '17481', '17482', '17483', '17484', '17485', '17486', '17487', '17488', '17489', '17490', '17491', '17492', '17493', '17494', '17495', '17496', '17497', '17498', '17499', '17500', '17501', '17502', '17503', '17504', '17505', '17506', '17507', '17508', '17509', '17510', '17511', '17512', '17513', '17514', '17515', '17516', '17517', '17518', '17519', '17520', '17521', '17522', '17523', '17524', '17525', '17526', '17527', '17528', '17529', '17530', '17531', '17532', '17533', '17534', '17535', '17536', '17537', '17538', '17539', '17540', '17541', '17542', '17543', '17544', '17545', '17546', '17547', '17548', '17549', '17550', '17551', '17552', '17553', '17554', '17555', '17556', '17557', '17558', '17559', '17560', '17561', '17562', '17563', '17564', '17565', '17566', '17567', '17568', '17569', '17570', '17571', '17572', '17573', '17574', '17575', '17576', '17577', '17578', '17579', '17580', '17581', '17582', '17583', '17584', '17585', '17586', '17587', '17588', '17589', '17590', '17591', '17592', '17593', '17594', '17595', '17596', '17597', '17598', '17599', '17600', '17601', '17602', '17603', '17604', '17605', '17606', '17607', '17608', '17609', '17610', '17611', '17612', '17613', '17614', '17615', '17616', '17617', '17618', '17619', '17620', '17621', '17622', '17623', '17624', '17625', '17626', '17627', '17628', '17629', '17630', '17631', '17632', '17633', '17634', '17635', '17636', '17637', '17638', '17639', '17640', '17641', '17642', '17643', '17644', '17645', '17646', '17647', '17648', '17649', '17650', '17651', '17652', '17653', '17654', '17655', '17656', '17657', '17658', '17659', '17660', '17661', '17662', '17663', '17664', '17665', '17666', '17667', '17668', '17669', '17670', '17671', '17672', '17673', '17674', '17675', '17676', '17677', '17678', '17679', '17680', '17681', '17682', '17683', '17684', '17685', '17686', '17687', '17688', '17689', '17690', '17691', '17692', '17693', '17694', '17695', '17696', '17697', '17698', '17699', '17700', '17701', '17702', '17703', '17704', '17705', '17706', '17707', '17708', '17709', '17710', '17711', '17712', '17713', '17714', '17715', '17716', '17717', '17718', '17719', '17720', '17721', '17722', '17723', '17724', '17725', '17726', '17727', '17728', '17729', '17730', '17731', '17732', '17733', '17734', '17735', '17736', '17737', '17738', '17739', '17740', '17741', '17742', '17743', '17744', '17745', '17746', '17747', '17748', '17749', '17750', '17751', '17752', '17753', '17754', '17755', '17756', '17757', '17758', '17759', '17760', '17761', '17762', '17763', '17764', '17765', '17766', '17767', '17768', '17769', '17770', '17771', '17772', '17773', '17774', '17775', '17776', '17777', '17778', '17779', '17780', '17781', '17782', '17783', '17784', '17785', '17786', '17787', '17788', '17789', '17790', '17791', '17792', '17793', '17794', '17795', '17796', '17797', '17798', '17799', '17800', '17801', '17802', '17803', '17804', '17805', '17806', '17807', '17808', '17809', '17810', '17811', '17812', '17813', '17814', '17815', '17816', '17817', '17818', '17819', '17820', '17821', '17822', '17823', '17824', '17825', '17826', '17827', '17828', '17829', '17830', '17831', '17832', '17833', '17834', '17835', '17836', '17837', '17838', '17839', '17840', '17841', '17842', '17843', '17844', '17845', '17846', '17847', '17848', '17849', '17850', '17851', '17852', '17853', '17854', '17855', '17856', '17857', '17858', '17859', '17860', '17861', '17862', '17863', '17864', '17865', '17866', '17867', '17868', '17869', '17870', '17871', '17872', '17873', '17874', '17875', '17876', '17877', '17878', '17879', '17880', '17881', '17882', '17883', '17884', '17885', '17886', '17887', '17888', '17889', '17890', '17891', '17892', '17893', '17894', '17895', '17896', '17897', '17898', '17899', '17900', '17901', '17902', '17903', '17904', '17905', '17906', '17907', '17908', '17909', '17910', '17911', '17912', '17913', '17914', '17915', '17916', '17917', '17918', '17919', '17920', '17921', '17922', '17923', '17924', '17925', '17926', '17927', '17928', '17929', '17930', '17931', '17932', '17933', '17934', '17935', '17936', '17937', '17938', '17939', '17940', '17941', '17942', '17943', '17944', '17945', '17946', '17947', '17948', '17949', '17950', '17951', '17952', '17953', '17954', '17955', '17956', '17957', '17958', '17959', '17960', '17961', '17962', '17963', '17964', '17965', '17966', '17967', '17968', '17969', '17970', '17971', '17972', '17973', '17974', '17975', '17976', '17977', '17978', '17979', '17980', '17981', '17982', '17983', '17984', '17985', '17986', '17987', '17988', '17989', '17990', '17991', '17992', '17993', '17994', '17995', '17996', '17997', '17998', '17999', '18000', '18001', '18002', '18003', '18004', '18005', '18006', '18007', '18008', '18009', '18010', '18011', '18012', '18013', '18014', '18015', '18016', '18017', '18018', '18019', '18020', '18021', '18022', '18023', '18024', '18025', '18026', '18027', '18028', '18029', '18030', '18031', '18032', '18033', '18034', '18035', '18036', '18037', '18038', '18039', '18040', '18041', '18042', '18043', '18044', '18045', '18046', '18047', '18048', '18049', '18050', '18051', '18052', '18053', '18054', '18055', '18056', '18057', '18058', '18059', '18060', '18061', '18062', '18063', '18064', '18065', '18066', '18067', '18068', '18069', '18070', '18071', '18072', '18073', '18074', '18075', '18076', '18077', '18078', '18079', '18080', '18081', '18082', '18083', '18084', '18085', '18086', '18087', '18088', '18089', '18090', '18091', '18092', '18093', '18094', '18095', '18096', '18097', '18098', '18099', '18100', '18101', '18102', '18103', '18104', '18105', '18106', '18107', '18108', '18109', '18110', '18111', '18112', '18113', '18114', '18115', '18116', '18117', '18118', '18119', '18120', '18121', '18122', '18123', '18124', '18125', '18126', '18127', '18128', '18129', '18130', '18131', '18132', '18133', '18134', '18135', '18136', '18137', '18138', '18139', '18140', '18141', '18142', '18143', '18144', '18145', '18146', '18147', '18148', '18149', '18150', '18151', '18152', '18153', '18154', '18155', '18156', '18157', '18158', '18159', '18160', '18161', '18162', '18163', '18164', '18165', '18166', '18167', '18168', '18169', '18170', '18171', '18172', '18173', '18174', '18175', '18176', '18177', '18178', '18179', '18180', '18181', '18182', '18183', '18184', '18185', '18186', '18187', '18188', '18189', '18190', '18191', '18192', '18193', '18194', '18195', '18196', '18197', '18198', '18199', '18200', '18201', '18202', '18203', '18204', '18205', '18206', '18207', '18208', '18209', '18210', '18211', '18212', '18213', '18214', '18215', '18216', '18217', '18218', '18219', '18220', '18221', '18222', '18223', '18224', '18225', '18226', '18227', '18228', '18229', '18230', '18231', '18232', '18233', '18234', '18235', '18236', '18237', '18238', '18239', '18240', '18241', '18242', '18243', '18244', '18245', '18246', '18247', '18248', '18249', '18250', '18251', '18252', '18253', '18254', '18255', '18256', '18257', '18258', '18259', '18260', '18261', '18262', '18263', '18264', '18265', '18266', '18267', '18268', '18269', '18270', '18271', '18272', '18273', '18274', '18275', '18276', '18277', '18278', '18279', '18280', '18281', '18282', '18283', '18284', '18285', '18286', '18287', '18288', '18289', '18290', '18291', '18292', '18293', '18294', '18295', '18296', '18297', '18298', '18299', '18300', '18301', '18302', '18303', '18304', '18305', '18306', '18307', '18308', '18309', '18310', '18311', '18312', '18313', '18314', '18315', '18316', '18317', '18318', '18319', '18320', '18321', '18322', '18323', '18324', '18325', '18326', '18327', '18328', '18329', '18330', '18331', '18332', '18333', '18334', '18335', '18336', '18337', '18338', '18339', '18340', '18341', '18342', '18343', '18344', '18345', '18346', '18347', '18348', '18349', '18350', '18351', '18352', '18353', '18354', '18355', '18356', '18357', '18358', '18359', '18360', '18361', '18362', '18363', '18364', '18365', '18366', '18367', '18368', '18369', '18370', '18371', '18372', '18373', '18374', '18375', '18376', '18377', '18378', '18379', '18380', '18381', '18382', '18383', '18384', '18385', '18386', '18387', '18388', '18389', '18390', '18391', '18392', '18393', '18394', '18395', '18396', '18397', '18398', '18399', '18400', '18401', '18402', '18403', '18404', '18405', '18406', '18407', '18408', '18409', '18410', '18411', '18412', '18413', '18414', '18415', '18416', '18417', '18418', '18419', '18420', '18421', '18422', '18423', '18424', '18425', '18426', '18427', '18428', '18429', '18430', '18431', '18432', '18433', '18434', '18435', '18436', '18437', '18438', '18439', '18440', '18441', '18442', '18443', '18444', '18445', '18446', '18447', '18448', '18449', '18450', '18451', '18452', '18453', '18454', '18455', '18456', '18457', '18458', '18459', '18460', '18461', '18462', '18463', '18464', '18465', '18466', '18467', '18468', '18469', '18470', '18471', '18472', '18473', '18474', '18475', '18476', '18477', '18478', '18479', '18480', '18481', '18482', '18483', '18484', '18485', '18486', '18487', '18488', '18489', '18490', '18491', '18492', '18493', '18494', '18495', '18496', '18497', '18498', '18499', '18500', '18501', '18502', '18503', '18504', '18505', '18506', '18507', '18508', '18509', '18510', '18511', '18512', '18513', '18514', '18515', '18516', '18517', '18518', '18519', '18520', '18521', '18522', '18523', '18524', '18525', '18526', '18527', '18528', '18529', '18530', '18531', '18532', '18533', '18534', '18535', '18536', '18537', '18538', '18539', '18540', '18541', '18542', '18543', '18544', '18545', '18546', '18547', '18548', '18549', '18550', '18551', '18552', '18553', '18554', '18555', '18556', '18557', '18558', '18559', '18560', '18561', '18562', '18563', '18564', '18565', '18566', '18567', '18568', '18569', '18570', '18571', '18572', '18573', '18574', '18575', '18576', '18577', '18578', '18579', '18580', '18581', '18582', '18583', '18584', '18585', '18586', '18587', '18588', '18589', '18590', '18591', '18592', '18593', '18594', '18595', '18596', '18597', '18598', '18599', '18600', '18601', '18602', '18603', '18604', '18605', '18606', '18607', '18608', '18609', '18610', '18611', '18612', '18613', '18614', '18615', '18616', '18617', '18618', '18619', '18620', '18621', '18622', '18623', '18624', '18625', '18626', '18627', '18628', '18629', '18630', '18631', '18632', '18633', '18634', '18635', '18636', '18637', '18638', '18639', '18640', '18641', '18642', '18643', '18644', '18645', '18646', '18647', '18648', '18649', '18650', '18651', '18652', '18653', '18654', '18655', '18656', '18657', '18658', '18659', '18660', '18661', '18662', '18663', '18664', '18665', '18666', '18667', '18668', '18669', '18670', '18671', '18672', '18673', '18674', '18675', '18676', '18677', '18678', '18679', '18680', '18681', '18682', '18683', '18684', '18685', '18686', '18687', '18688', '18689', '18690', '18691', '18692', '18693', '18694', '18695', '18696', '18697', '18698', '18699', '18700', '18701', '18702', '18703', '18704', '18705', '18706', '18707', '18708', '18709', '18710', '18711', '18712', '18713', '18714', '18715', '18716', '18717', '18718', '18719', '18720', '18721', '18722', '18723', '18724', '18725', '18726', '18727', '18728', '18729', '18730', '18731', '18732', '18733', '18734', '18735', '18736', '18737', '18738', '18739', '18740', '18741', '18742', '18743', '18744', '18745', '18746', '18747', '18748', '18749', '18750', '18751', '18752', '18753', '18754', '18755', '18756', '18757', '18758', '18759', '18760', '18761', '18762', '18763', '18764', '18765', '18766', '18767', '18768', '18769', '18770', '18771', '18772', '18773', '18774', '18775', '18776', '18777', '18778', '18779', '18780', '18781', '18782', '18783', '18784', '18785', '18786', '18787', '18788', '18789', '18790', '18791', '18792', '18793', '18794', '18795', '18796', '18797', '18798', '18799', '18800', '18801', '18802', '18803', '18804', '18805', '18806', '18807', '18808', '18809', '18810', '18811', '18812', '18813', '18814', '18815', '18816', '18817', '18818', '18819', '18820', '18821', '18822', '18823', '18824', '18825', '18826', '18827', '18828', '18829', '18830', '18831', '18832', '18833', '18834', '18835', '18836', '18837', '18838', '18839', '18840', '18841', '18842', '18843', '18844', '18845', '18846', '18847', '18848', '18849', '18850', '18851', '18852', '18853', '18854', '18855', '18856', '18857', '18858', '18859', '18860', '18861', '18862', '18863', '18864', '18865', '18866', '18867', '18868', '18869', '18870', '18871', '18872', '18873', '18874', '18875', '18876', '18877', '18878', '18879', '18880', '18881', '18882', '18883', '18884', '18885', '18886', '18887', '18888', '18889', '18890', '18891', '18892', '18893', '18894', '18895', '18896', '18897', '18898', '18899', '18900', '18901', '18902', '18903', '18904', '18905', '18906', '18907', '18908', '18909', '18910', '18911', '18912', '18913', '18914', '18915', '18916', '18917', '18918', '18919', '18920', '18921', '18922', '18923', '18924', '18925', '18926', '18927', '18928', '18929', '18930', '18931', '18932', '18933', '18934', '18935', '18936', '18937', '18938', '18939', '18940', '18941', '18942', '18943', '18944', '18945', '18946', '18947', '18948', '18949', '18950', '18951', '18952', '18953', '18954', '18955', '18956', '18957', '18958', '18959', '18960', '18961', '18962', '18963', '18964', '18965', '18966', '18967', '18968', '18969', '18970', '18971', '18972', '18973', '18974', '18975', '18976', '18977', '18978', '18979', '18980', '18981', '18982', '18983', '18984', '18985', '18986', '18987', '18988', '18989', '18990', '18991', '18992', '18993', '18994', '18995', '18996', '18997', '18998', '18999', '19000', '19001', '19002', '19003', '19004', '19005', '19006', '19007', '19008', '19009', '19010', '19011', '19012', '19013', '19014', '19015', '19016', '19017', '19018', '19019', '19020', '19021', '19022', '19023', '19024', '19025', '19026', '19027', '19028', '19029', '19030', '19031', '19032', '19033', '19034', '19035', '19036', '19037', '19038', '19039', '19040', '19041', '19042', '19043', '19044', '19045', '19046', '19047', '19048', '19049', '19050', '19051', '19052', '19053', '19054', '19055', '19056', '19057', '19058', '19059', '19060', '19061', '19062', '19063', '19064', '19065', '19066', '19067', '19068', '19069', '19070', '19071', '19072', '19073', '19074', '19075', '19076', '19077', '19078', '19079', '19080', '19081', '19082', '19083', '19084', '19085', '19086', '19087', '19088', '19089', '19090', '19091', '19092', '19093', '19094', '19095', '19096', '19097', '19098', '19099', '19100', '19101', '19102', '19103', '19104', '19105', '19106', '19107', '19108', '19109', '19110', '19111', '19112', '19113', '19114', '19115', '19116', '19117', '19118', '19119', '19120', '19121', '19122', '19123', '19124', '19125', '19126', '19127', '19128', '19129', '19130', '19131', '19132', '19133', '19134', '19135', '19136', '19137', '19138', '19139', '19140', '19141', '19142', '19143', '19144', '19145', '19146', '19147', '19148', '19149', '19150', '19151', '19152', '19153', '19154', '19155', '19156', '19157', '19158', '19159', '19160', '19161', '19162', '19163', '19164', '19165', '19166', '19167', '19168', '19169', '19170', '19171', '19172', '19173', '19174', '19175', '19176', '19177', '19178', '19179', '19180', '19181', '19182', '19183', '19184', '19185', '19186', '19187', '19188', '19189', '19190', '19191', '19192', '19193', '19194', '19195', '19196', '19197', '19198', '19199', '19200', '19201', '19202', '19203', '19204', '19205', '19206', '19207', '19208', '19209', '19210', '19211', '19212', '19213', '19214', '19215', '19216', '19217', '19218', '19219', '19220', '19221', '19222', '19223', '19224', '19225', '19226', '19227', '19228', '19229', '19230', '19231', '19232', '19233', '19234', '19235', '19236', '19237', '19238', '19239', '19240', '19241', '19242', '19243', '19244', '19245', '19246', '19247', '19248', '19249', '19250', '19251', '19252', '19253', '19254', '19255', '19256', '19257', '19258', '19259', '19260', '19261', '19262', '19263', '19264', '19265', '19266', '19267', '19268', '19269', '19270', '19271', '19272', '19273', '19274', '19275', '19276', '19277', '19278', '19279', '19280', '19281', '19282', '19283', '19284', '19285', '19286', '19287', '19288', '19289', '19290', '19291', '19292', '19293', '19294', '19295', '19296', '19297', '19298', '19299', '19300', '19301', '19302', '19303', '19304', '19305', '19306', '19307', '19308', '19309', '19310', '19311', '19312', '19313', '19314', '19315', '19316', '19317', '19318', '19319', '19320', '19321', '19322', '19323', '19324', '19325', '19326', '19327', '19328', '19329', '19330', '19331', '19332', '19333', '19334', '19335', '19336', '19337', '19338', '19339', '19340', '19341', '19342', '19343', '19344', '19345', '19346', '19347', '19348', '19349', '19350', '19351', '19352', '19353', '19354', '19355', '19356', '19357', '19358', '19359', '19360', '19361', '19362', '19363', '19364', '19365', '19366', '19367', '19368', '19369', '19370', '19371', '19372', '19373', '19374', '19375', '19376', '19377', '19378', '19379', '19380', '19381', '19382', '19383', '19384', '19385', '19386', '19387', '19388', '19389', '19390', '19391', '19392', '19393', '19394', '19395', '19396', '19397', '19398', '19399', '19400', '19401', '19402', '19403', '19404', '19405', '19406', '19407', '19408', '19409', '19410', '19411', '19412', '19413', '19414', '19415', '19416', '19417', '19418', '19419', '19420', '19421', '19422', '19423', '19424', '19425', '19426', '19427', '19428', '19429', '19430', '19431', '19432', '19433', '19434', '19435', '19436', '19437', '19438', '19439', '19440', '19441', '19442', '19443', '19444', '19445', '19446', '19447', '19448', '19449', '19450', '19451', '19452', '19453', '19454', '19455', '19456', '19457', '19458', '19459', '19460', '19461', '19462', '19463', '19464', '19465', '19466', '19467', '19468', '19469', '19470', '19471', '19472', '19473', '19474', '19475', '19476', '19477', '19478', '19479', '19480', '19481', '19482', '19483', '19484', '19485', '19486', '19487', '19488', '19489', '19490', '19491', '19492', '19493', '19494', '19495', '19496', '19497', '19498', '19499', '19500', '19501', '19502', '19503', '19504', '19505', '19506', '19507', '19508', '19509', '19510', '19511', '19512', '19513', '19514', '19515', '19516', '19517', '19518', '19519', '19520', '19521', '19522', '19523', '19524', '19525', '19526', '19527', '19528', '19529', '19530', '19531', '19532', '19533', '19534', '19535', '19536', '19537', '19538', '19539', '19540', '19541', '19542', '19543', '19544', '19545', '19546', '19547', '19548', '19549', '19550', '19551', '19552', '19553', '19554', '19555', '19556', '19557', '19558', '19559', '19560', '19561', '19562', '19563', '19564', '19565', '19566', '19567', '19568', '19569', '19570', '19571', '19572', '19573', '19574', '19575', '19576', '19577', '19578', '19579', '19580', '19581', '19582', '19583', '19584', '19585', '19586', '19587', '19588', '19589', '19590', '19591', '19592', '19593', '19594', '19595', '19596', '19597', '19598', '19599', '19600', '19601', '19602', '19603', '19604', '19605', '19606', '19607', '19608', '19609', '19610', '19611', '19612', '19613', '19614', '19615', '19616', '19617', '19618', '19619', '19620', '19621', '19622', '19623', '19624', '19625', '19626', '19627', '19628', '19629', '19630', '19631', '19632', '19633', '19634', '19635', '19636', '19637', '19638', '19639', '19640', '19641', '19642', '19643', '19644', '19645', '19646', '19647', '19648', '19649', '19650', '19651', '19652', '19653', '19654', '19655', '19656', '19657', '19658', '19659', '19660', '19661', '19662', '19663', '19664', '19665', '19666', '19667', '19668', '19669', '19670', '19671', '19672', '19673', '19674', '19675', '19676', '19677', '19678', '19679', '19680', '19681', '19682', '19683', '19684', '19685', '19686', '19687', '19688', '19689', '19690', '19691', '19692', '19693', '19694', '19695', '19696', '19697', '19698', '19699', '19700', '19701', '19702', '19703', '19704', '19705', '19706', '19707', '19708', '19709', '19710', '19711', '19712', '19713', '19714', '19715', '19716', '19717', '19718', '19719', '19720', '19721', '19722', '19723', '19724', '19725', '19726', '19727', '19728', '19729', '19730', '19731', '19732', '19733', '19734', '19735', '19736', '19737', '19738', '19739', '19740', '19741', '19742', '19743', '19744', '19745', '19746', '19747', '19748', '19749', '19750', '19751', '19752', '19753', '19754', '19755', '19756', '19757', '19758', '19759', '19760', '19761', '19762', '19763', '19764', '19765', '19766', '19767', '19768', '19769', '19770', '19771', '19772', '19773', '19774', '19775', '19776', '19777', '19778', '19779', '19780', '19781', '19782', '19783', '19784', '19785', '19786', '19787', '19788', '19789', '19790', '19791', '19792', '19793', '19794', '19795', '19796', '19797', '19798', '19799', '19800', '19801', '19802', '19803', '19804', '19805', '19806', '19807', '19808', '19809', '19810', '19811', '19812', '19813', '19814', '19815', '19816', '19817', '19818', '19819', '19820', '19821', '19822', '19823', '19824', '19825', '19826', '19827', '19828', '19829', '19830', '19831', '19832', '19833', '19834', '19835', '19836', '19837', '19838', '19839', '19840', '19841', '19842', '19843', '19844', '19845', '19846', '19847', '19848', '19849', '19850', '19851', '19852', '19853', '19854', '19855', '19856', '19857', '19858', '19859', '19860', '19861', '19862', '19863', '19864', '19865', '19866', '19867', '19868', '19869', '19870', '19871', '19872', '19873', '19874', '19875', '19876', '19877', '19878', '19879', '19880', '19881', '19882', '19883', '19884', '19885', '19886', '19887', '19888', '19889', '19890', '19891', '19892', '19893', '19894', '19895', '19896', '19897', '19898', '19899', '19900', '19901', '19902', '19903', '19904', '19905', '19906', '19907', '19908', '19909', '19910', '19911', '19912', '19913', '19914', '19915', '19916', '19917', '19918', '19919', '19920', '19921', '19922', '19923', '19924', '19925', '19926', '19927', '19928', '19929', '19930', '19931', '19932', '19933', '19934', '19935', '19936', '19937', '19938', '19939', '19940', '19941', '19942', '19943', '19944', '19945', '19946', '19947', '19948', '19949', '19950', '19951', '19952', '19953', '19954', '19955', '19956', '19957', '19958', '19959', '19960', '19961', '19962', '19963', '19964', '19965', '19966', '19967', '19968', '19969', '19970', '19971', '19972', '19973', '19974', '19975', '19976', '19977', '19978', '19979', '19980', '19981', '19982', '19983', '19984', '19985', '19986', '19987', '19988', '19989', '19990', '19991', '19992', '19993', '19994', '19995', '19996', '19997', '19998', '19999', '20000', '20001', '20002', '20003', '20004', '20005', '20006', '20007', '20008', '20009', '20010', '20011', '20012', '20013', '20014', '20015', '20016', '20017', '20018', '20019', '20020', '20021', '20022', '20023', '20024', '20025', '20026', '20027', '20028', '20029', '20030', '20031', '20032', '20033', '20034', '20035', '20036', '20037', '20038', '20039', '20040', '20041', '20042', '20043', '20044', '20045', '20046', '20047', '20048', '20049', '20050', '20051', '20052', '20053', '20054', '20055', '20056', '20057', '20058', '20059', '20060', '20061', '20062', '20063', '20064', '20065', '20066', '20067', '20068', '20069', '20070', '20071', '20072', '20073', '20074', '20075', '20076', '20077', '20078', '20079', '20080', '20081', '20082', '20083', '20084', '20085', '20086', '20087', '20088', '20089', '20090', '20091', '20092', '20093', '20094', '20095', '20096', '20097', '20098', '20099', '20100', '20101', '20102', '20103', '20104', '20105', '20106', '20107', '20108', '20109', '20110', '20111', '20112', '20113', '20114', '20115', '20116', '20117', '20118', '20119', '20120', '20121', '20122', '20123', '20124', '20125', '20126', '20127', '20128', '20129', '20130', '20131', '20132', '20133', '20134', '20135', '20136', '20137', '20138', '20139', '20140', '20141', '20142', '20143', '20144', '20145', '20146', '20147', '20148', '20149', '20150', '20151', '20152', '20153', '20154', '20155', '20156', '20157', '20158', '20159', '20160', '20161', '20162', '20163', '20164', '20165', '20166', '20167', '20168', '20169', '20170', '20171', '20172', '20173', '20174', '20175', '20176', '20177', '20178', '20179', '20180', '20181', '20182', '20183', '20184', '20185', '20186', '20187', '20188', '20189', '20190', '20191', '20192', '20193', '20194', '20195', '20196', '20197', '20198', '20199', '20200', '20201', '20202', '20203', '20204', '20205', '20206', '20207', '20208', '20209', '20210', '20211', '20212', '20213', '20214', '20215', '20216', '20217', '20218', '20219', '20220', '20221', '20222', '20223', '20224', '20225', '20226', '20227', '20228', '20229', '20230', '20231', '20232', '20233', '20234', '20235', '20236', '20237', '20238', '20239', '20240', '20241', '20242', '20243', '20244', '20245', '20246', '20247', '20248', '20249', '20250', '20251', '20252', '20253', '20254', '20255', '20256', '20257', '20258', '20259', '20260', '20261', '20262', '20263', '20264', '20265', '20266', '20267', '20268', '20269', '20270', '20271', '20272', '20273', '20274', '20275', '20276', '20277', '20278', '20279', '20280', '20281', '20282', '20283', '20284', '20285', '20286', '20287', '20288', '20289', '20290', '20291', '20292', '20293', '20294', '20295', '20296', '20297', '20298', '20299', '20300', '20301', '20302', '20303', '20304', '20305', '20306', '20307', '20308', '20309', '20310', '20311', '20312', '20313', '20314', '20315', '20316', '20317', '20318', '20319', '20320', '20321', '20322', '20323', '20324', '20325', '20326', '20327', '20328', '20329', '20330', '20331', '20332', '20333', '20334', '20335', '20336', '20337', '20338', '20339', '20340', '20341', '20342', '20343', '20344', '20345', '20346', '20347', '20348', '20349', '20350', '20351', '20352', '20353', '20354', '20355', '20356', '20357', '20358', '20359', '20360', '20361', '20362', '20363', '20364', '20365', '20366', '20367', '20368', '20369', '20370', '20371', '20372', '20373', '20374', '20375', '20376', '20377', '20378', '20379', '20380', '20381', '20382', '20383', '20384', '20385', '20386', '20387', '20388', '20389', '20390', '20391', '20392', '20393', '20394', '20395', '20396', '20397', '20398', '20399', '20400', '20401', '20402', '20403', '20404', '20405', '20406', '20407', '20408', '20409', '20410', '20411', '20412', '20413', '20414', '20415', '20416', '20417', '20418', '20419', '20420', '20421', '20422', '20423', '20424', '20425', '20426', '20427', '20428', '20429', '20430', '20431', '20432', '20433', '20434', '20435', '20436', '20437', '20438', '20439', '20440', '20441', '20442', '20443', '20444', '20445', '20446', '20447', '20448', '20449', '20450', '20451', '20452', '20453', '20454', '20455', '20456', '20457', '20458', '20459', '20460', '20461', '20462', '20463', '20464', '20465', '20466', '20467', '20468', '20469', '20470', '20471', '20472', '20473', '20474', '20475', '20476', '20477', '20478', '20479', '20480', '20481', '20482', '20483', '20484', '20485', '20486', '20487', '20488', '20489', '2049', '20490', '20491', '20492', '20493', '20494', '20495', '20496', '20497', '20498', '20499', '2050', '20500', '20501', '20502', '20503', '20504', '20505', '20506', '20507', '20508', '20509', '2051', '20510', '20511', '20512', '20513', '20514', '20515', '20516', '20517', '20518', '20519', '2052', '20520', '20521', '20522', '20523', '20524', '20525', '20526', '20527', '20528', '20529', '2053', '20530', '20531', '20532', '20533', '20534', '20535', '20536', '20537', '20538', '20539', '2054', '20540', '20541', '20542', '20543', '20544', '20545', '20546', '20547', '20548', '20549', '2055', '20550', '20551', '20552', '20553', '20554', '20555', '20556', '20557', '20558', '20559', '2056', '20560', '20561', '20562', '20563', '20564', '20565', '20566', '20567', '20568', '20569', '2057', '20570', '20571', '20572', '20573', '20574', '20575', '20576', '20577', '20578', '20579', '2058', '20580', '20581', '20582', '20583', '20584', '20585', '20586', '20587', '20588', '20589', '2059', '20590', '20591', '20592', '20593', '20594', '20595', '20596', '20597', '20598', '20599', '2060', '20600', '20601', '20602', '20603', '20604', '20605', '20606', '20607', '20608', '20609', '2061', '20610', '20611', '20612', '20613', '20614', '20615', '20616', '20617', '20618', '20619', '2062', '20620', '20621', '20622', '20623', '20624', '20625', '20626', '20627', '20628', '20629', '2063', '20630', '20631', '20632', '20633', '20634', '20635', '20636', '20637', '20638', '20639', '2064', '20640', '20641', '20642', '20643', '20644', '20645', '20646', '20647', '20648', '20649', '2065', '20650', '20651', '20652', '20653', '20654', '20655', '20656', '20657', '20658', '20659', '2066', '20660', '20661', '20662', '20663', '20664', '20665', '20666', '20667', '20668', '20669', '2067', '20670', '20671', '20672', '20673', '20674', '20675', '20676', '20677', '20678', '20679', '2068', '20680', '20681', '20682', '20683', '20684', '20685', '20686', '20687', '20688', '20689', '2069', '20690', '20691', '20692', '20693', '20694', '20695', '20696', '20697', '20698', '20699', '2070', '20700', '20701', '20702', '20703', '20704', '20705', '20706', '20707', '20708', '20709', '2071', '20710', '20711', '20712', '20713', '20714', '20715', '20716', '20717', '20718', '20719', '2072', '20720', '20721', '20722', '20723', '20724', '20725', '20726', '20727', '20728', '20729', '2073', '20730', '20731', '20732', '20733', '20734', '20735', '20736', '20737', '20738', '20739', '2074', '20740', '20741', '20742', '20743', '20744', '20745', '20746', '20747', '20748', '20749', '2075', '20750', '20751', '20752', '20753', '20754', '20755', '20756', '20757', '20758', '20759', '2076', '20760', '20761', '20762', '20763', '20764', '20765', '20766', '20767', '20768', '20769', '2077', '20770', '20771', '20772', '20773', '20774', '20775', '20776', '20777', '20778', '20779', '2078', '20780', '20781', '20782', '20783', '20784', '20785', '20786', '20787', '20788', '20789', '2079', '20790', '20791', '20792', '20793', '20794', '20795', '20796', '20797', '20798', '20799', '2080', '20800', '20801', '20802', '20803', '20804', '20805', '20806', '20807', '20808', '20809', '2081', '20810', '20811', '20812', '20813', '20814', '20815', '20816', '20817', '20818', '20819', '2082', '20820', '20821', '20822', '20823', '20824', '20825', '20826', '20827', '20828', '20829', '2083', '20830', '20831', '20832', '20833', '20834', '20835', '20836', '20837', '20838', '20839', '2084', '20840', '20841', '20842', '20843', '20844', '20845', '20846', '20847', '20848', '20849', '2085', '20850', '20851', '20852', '20853', '20854', '20855', '20856', '20857', '20858', '20859', '2086', '20860', '20861', '20862', '20863', '20864', '20865', '20866', '20867', '20868', '20869', '2087', '20870', '20871', '20872', '20873', '20874', '20875', '20876', '20877', '20878', '20879', '2088', '20880', '20881', '20882', '20883', '20884', '20885', '20886', '20887', '20888', '20889', '2089', '20890', '20891', '20892', '20893', '20894', '20895', '20896', '20897', '20898', '20899', '2090', '20900', '20901', '20902', '20903', '20904', '20905', '20906', '20907', '20908', '20909', '2091', '20910', '20911', '20912', '20913', '20914', '20915', '20916', '20917', '20918', '20919', '2092', '20920', '20921', '20922', '20923', '20924', '20925', '20926', '20927', '20928', '20929', '2093', '20930', '20931', '20932', '20933', '20934', '20935', '20936', '20937', '20938', '20939', '2094', '20940', '20941', '20942', '20943', '20944', '20945', '20946', '20947', '20948', '20949', '2095', '20950', '20951', '20952', '20953', '20954', '20955', '20956', '20957', '20958', '20959', '2096', '20960', '20961', '20962', '20963', '20964', '20965', '20966', '20967', '20968', '20969', '2097', '20970', '20971', '20972', '20973', '20974', '20975', '20976', '20977', '20978', '20979', '2098', '20980', '20981', '20982', '20983', '20984', '20985', '20986', '20987', '20988', '20989', '2099', '20990', '20991', '20992', '20993', '20994', '20995', '20996', '20997', '20998', '20999', '2100', '21000', '21001', '21002', '21003', '21004', '21005', '21006', '21007', '21008', '21009', '2101', '21010', '21011', '21012', '21013', '21014', '21015', '21016', '21017', '21018', '21019', '2102', '21020', '21021', '21022', '21023', '21024', '21025', '21026', '21027', '21028', '21029', '2103', '21030', '21031', '21032', '21033', '21034', '21035', '21036', '21037', '21038', '21039', '2104', '21040', '21041', '21042', '21043', '21044', '21045', '21046', '21047', '21048', '21049', '2105', '21050', '21051', '21052', '21053', '21054', '21055', '21056', '21057', '21058', '21059', '2106', '21060', '21061', '21062', '21063', '21064', '21065', '21066', '21067', '21068', '21069', '2107', '21070', '21071', '21072', '21073', '21074', '21075', '21076', '21077', '21078', '21079', '2108', '21080', '21081', '21082', '21083', '21084', '21085', '21086', '21087', '21088', '21089', '2109', '21090', '21091', '21092', '21093', '21094', '21095', '21096', '21097', '21098', '21099', '2110', '21100', '21101', '21102', '21103', '21104', '21105', '21106', '21107', '21108', '21109', '2111', '21110', '21111', '21112', '21113', '21114', '21115', '21116', '21117', '21118', '21119', '2112', '21120', '21121', '21122', '21123', '21124', '21125', '21126', '21127', '21128', '21129', '2113', '21130', '21131', '21132', '21133', '21134', '21135', '21136', '21137', '21138', '21139', '2114', '21140', '21141', '21142', '21143', '21144', '21145', '21146', '21147', '21148', '21149', '2115', '21150', '21151', '21152', '21153', '21154', '21155', '21156', '21157', '21158', '21159', '2116', '21160', '21161', '21162', '21163', '21164', '21165', '21166', '21167', '21168', '21169', '2117', '21170', '21171', '21172', '21173', '21174', '21175', '21176', '21177', '21178', '21179', '2118', '21180', '21181', '21182', '21183', '21184', '21185', '21186', '21187', '21188', '21189', '2119', '21190', '21191', '21192', '21193', '21194', '21195', '21196', '21197', '21198', '21199', '2120', '21200', '21201', '21202', '21203', '21204', '21205', '21206', '21207', '21208', '21209', '2121', '21210', '21211', '21212', '21213', '21214', '21215', '21216', '21217', '21218', '21219', '2122', '21220', '21221', '21222', '21223', '21224', '21225', '21226', '21227', '21228', '21229', '2123', '21230', '21231', '21232', '21233', '21234', '21235', '21236', '21237', '21238', '21239', '2124', '21240', '21241', '21242', '21243', '21244', '21245', '21246', '21247', '21248', '21249', '2125', '21250', '21251', '21252', '21253', '21254', '21255', '21256', '21257', '21258', '21259', '2126', '21260', '21261', '21262', '21263', '21264', '21265', '21266', '21267', '21268', '21269', '2127', '21270', '21271', '21272', '21273', '21274', '21275', '21276', '21277', '21278', '21279', '2128', '21280', '21281', '21282', '21283', '21284', '21285', '21286', '21287', '21288', '21289', '2129', '21290', '21291', '21292', '21293', '21294', '21295', '21296', '21297', '21298', '21299', '2130', '21300', '21301', '21302', '21303', '21304', '21305', '21306', '21307', '21308', '21309', '2131', '21310', '21311', '21312', '21313', '21314', '21315', '21316', '21317', '21318', '21319', '2132', '21320', '21321', '21322', '21323', '21324', '21325', '21326', '21327', '21328', '21329', '2133', '21330', '21331', '21332', '21333', '21334', '21335', '21336', '21337', '21338', '21339', '2134', '21340', '21341', '21342', '21343', '21344', '21345', '21346', '21347', '21348', '21349', '2135', '21350', '21351', '21352', '21353', '21354', '21355', '21356', '21357', '21358', '21359', '2136', '21360', '21361', '21362', '21363', '21364', '21365', '21366', '21367', '21368', '21369', '2137', '21370', '21371', '21372', '21373', '21374', '21375', '21376', '21377', '21378', '21379', '2138', '21380', '21381', '21382', '21383', '21384', '21385', '21386', '21387', '21388', '21389', '2139', '21390', '21391', '21392', '21393', '21394', '21395', '21396', '21397', '21398', '21399', '2140', '21400', '21401', '21402', '21403', '21404', '21405', '21406', '21407', '21408', '21409', '2141', '21410', '21411', '21412', '21413', '21414', '21415', '21416', '21417', '21418', '21419', '2142', '21420', '21421', '21422', '21423', '21424', '21425', '21426', '21427', '21428', '21429', '2143', '21430', '21431', '21432', '21433', '21434', '21435', '21436', '21437', '21438', '21439', '2144', '21440', '21441', '21442', '21443', '21444', '21445', '21446', '21447', '21448', '21449', '2145', '21450', '21451', '21452', '21453', '21454', '21455', '21456', '21457', '21458', '21459', '2146', '21460', '21461', '21462', '21463', '21464', '21465', '21466', '21467', '21468', '21469', '2147', '21470', '21471', '21472', '21473', '21474', '21475', '21476', '21477', '21478', '21479', '2148', '21480', '21481', '21482', '21483', '21484', '21485', '21486', '21487', '21488', '21489', '2149', '21490', '21491', '21492', '21493', '21494', '21495', '21496', '21497', '21498', '21499', '2150', '21500', '21501', '21502', '21503', '21504', '21505', '21506', '21507', '21508', '21509', '2151', '21510', '21511', '21512', '21513', '21514', '21515', '21516', '21517', '21518', '21519', '2152', '21520', '21521', '21522', '21523', '21524', '21525', '21526', '21527', '21528', '21529', '2153', '21530', '21531', '21532', '21533', '21534', '21535', '21536', '21537', '21538', '21539', '2154', '21540', '21541', '21542', '21543', '21544', '21545', '21546', '21547', '21548', '21549', '2155', '21550', '21551', '21552', '21553', '21554', '21555', '21556', '21557', '21558', '21559', '2156', '21560', '21561', '21562', '21563', '21564', '21565', '21566', '21567', '21568', '21569', '2157', '21570', '21571', '21572', '21573', '21574', '21575', '21576', '21577', '21578', '21579', '2158', '21580', '21581', '21582', '21583', '21584', '21585', '21586', '21587', '21588', '21589', '2159', '21590', '21591', '21592', '21593', '21594', '21595', '21596', '21597', '21598', '21599', '2160', '21600', '21601', '21602', '21603', '21604', '21605', '21606', '21607', '21608', '21609', '2161', '21610', '21611', '21612', '21613', '21614', '21615', '21616', '21617', '21618', '21619', '2162', '21620', '21621', '21622', '21623', '21624', '21625', '21626', '21627', '21628', '21629', '2163', '21630', '21631', '21632', '21633', '21634', '21635', '21636', '21637', '21638', '21639', '2164', '21640', '21641', '21642', '21643', '21644', '21645', '21646', '21647', '21648', '21649', '2165', '21650', '21651', '21652', '21653', '21654', '21655', '21656', '21657', '21658', '21659', '2166', '21660', '21661', '21662', '21663', '21664', '21665', '21666', '21667', '21668', '21669', '2167', '21670', '21671', '21672', '21673', '21674', '21675', '21676', '21677', '21678', '21679', '2168', '21680', '21681', '21682', '21683', '21684', '21685', '21686', '21687', '21688', '21689', '2169', '21690', '21691', '21692', '21693', '21694', '21695', '21696', '21697', '21698', '21699', '2170', '21700', '21701', '21702', '21703', '21704', '21705', '21706', '21707', '21708', '21709', '2171', '21710', '21711', '21712', '21713', '21714', '21715', '21716', '21717', '21718', '21719', '2172', '21720', '21721', '21722', '21723', '21724', '21725', '21726', '21727', '21728', '21729', '2173', '21730', '21731', '21732', '21733', '21734', '21735', '21736', '21737', '21738', '21739', '2174', '21740', '21741', '21742', '21743', '21744', '21745', '21746', '21747', '21748', '21749', '2175', '21750', '21751', '21752', '21753', '21754', '21755', '21756', '21757', '21758', '21759', '2176', '21760', '21761', '21762', '21763', '21764', '21765', '21766', '21767', '21768', '21769', '2177', '21770', '21771', '21772', '21773', '21774', '21775', '21776', '21777', '21778', '21779', '2178', '21780', '21781', '21782', '21783', '21784', '21785', '21786', '21787', '21788', '21789', '2179', '21790', '21791', '21792', '21793', '21794', '21795', '21796', '21797', '21798', '21799', '2180', '21800', '21801', '21802', '21803', '21804', '21805', '21806', '21807', '21808', '21809', '2181', '21810', '21811', '21812', '21813', '21814', '21815', '21816', '21817', '21818', '21819', '2182', '21820', '21821', '21822', '21823', '21824', '21825', '21826', '21827', '21828', '21829', '2183', '21830', '21831', '21832', '21833', '21834', '21835', '21836', '21837', '21838', '21839', '2184', '21840', '21841', '21842', '21843', '21844', '21845', '21846', '21847', '21848', '21849', '2185', '21850', '21851', '21852', '21853', '21854', '21855', '21856', '21857', '21858', '21859', '2186', '21860', '21861', '21862', '21863', '21864', '21865', '21866', '21867', '21868', '21869', '2187', '21870', '21871', '21872', '21873', '21874', '21875', '21876', '21877', '21878', '21879', '2188', '21880', '21881', '21882', '21883', '21884', '21885', '21886', '21887', '21888', '21889', '2189', '21890', '21891', '21892', '21893', '21894', '21895', '21896', '21897', '21898', '21899', '2190', '21900', '21901', '21902', '21903', '21904', '21905', '21906', '21907', '21908', '21909', '2191', '21910', '21911', '21912', '21913', '21914', '21915', '21916', '21917', '21918', '21919', '2192', '21920', '21921', '21922', '21923', '21924', '21925', '21926', '21927', '21928', '21929', '2193', '21930', '21931', '21932', '21933', '21934', '21935', '21936', '21937', '21938', '21939', '2194', '21940', '21941', '21942', '21943', '21944', '21945', '21946', '21947', '21948', '21949', '2195', '21950', '21951', '21952', '21953', '21954', '21955', '21956', '21957', '21958', '21959', '2196', '21960', '21961', '21962', '21963', '21964', '21965', '21966', '21967', '21968', '21969', '2197', '21970', '21971', '21972', '21973', '21974', '21975', '21976', '21977', '21978', '21979', '2198', '21980', '21981', '21982', '21983', '21984', '21985', '21986', '21987', '21988', '21989', '2199', '21990', '21991', '21992', '21993', '21994', '21995', '21996', '21997', '21998', '21999', '2200', '22000', '22001', '22002', '22003', '22004', '22005', '22006', '22007', '22008', '22009', '2201', '22010', '22011', '22012', '22013', '22014', '22015', '22016', '22017', '22018', '22019', '2202', '22020', '22021', '22022', '22023', '22024', '22025', '22026', '22027', '22028', '22029', '2203', '22030', '22031', '22032', '22033', '22034', '22035', '22036', '22037', '22038', '22039', '2204', '22040', '22041', '22042', '22043', '22044', '22045', '22046', '22047', '22048', '22049', '2205', '22050', '22051', '22052', '22053', '22054', '22055', '22056', '22057', '22058', '22059', '2206', '22060', '22061', '22062', '22063', '22064', '22065', '22066', '22067', '22068', '22069', '2207', '22070', '22071', '22072', '22073', '22074', '22075', '22076', '22077', '22078', '22079', '2208', '22080', '22081', '22082', '22083', '22084', '22085', '22086', '22087', '22088', '22089', '2209', '22090', '22091', '22092', '22093', '22094', '22095', '22096', '22097', '22098', '22099', '2210', '22100', '22101', '22102', '22103', '22104', '22105', '22106', '22107', '22108', '22109', '2211', '22110', '22111', '22112', '22113', '22114', '22115', '22116', '22117', '22118', '22119', '2212', '22120', '22121', '22122', '22123', '22124', '22125', '22126', '22127', '22128', '22129', '2213', '22130', '22131', '22132', '22133', '22134', '22135', '22136', '22137', '22138', '22139', '2214', '22140', '22141', '22142', '22143', '22144', '22145', '22146', '22147', '22148', '22149', '2215', '22150', '22151', '22152', '22153', '22154', '22155', '22156', '22157', '22158', '22159', '2216', '22160', '22161', '22162', '22163', '22164', '22165', '22166', '22167', '22168', '22169', '2217', '22170', '22171', '22172', '22173', '22174', '22175', '22176', '22177', '22178', '22179', '2218', '22180', '22181', '22182', '22183', '22184', '22185', '22186', '22187', '22188', '22189', '2219', '22190', '22191', '22192', '22193', '22194', '22195', '22196', '22197', '22198', '22199', '2220', '22200', '22201', '22202', '22203', '22204', '22205', '22206', '22207', '22208', '22209', '2221', '22210', '22211', '22212', '22213', '22214', '22215', '22216', '22217', '22218', '22219', '2222', '22220', '22221', '22222', '22223', '22224', '22225', '22226', '22227', '22228', '22229', '2223', '22230', '22231', '22232', '22233', '22234', '22235', '22236', '22237', '22238', '22239', '2224', '22240', '22241', '22242', '22243', '22244', '22245', '22246', '22247', '22248', '22249', '2225', '22250', '22251', '22252', '22253', '22254', '22255', '22256', '22257', '22258', '22259', '2226', '22260', '22261', '22262', '22263', '22264', '22265', '22266', '22267', '22268', '22269', '2227', '22270', '22271', '22272', '22273', '22274', '22275', '22276', '22277', '22278', '22279', '2228', '22280', '22281', '22282', '22283', '22284', '22285', '22286', '22287', '22288', '22289', '2229', '22290', '22291', '22292', '22293', '22294', '22295', '22296', '22297', '22298', '22299', '2230', '22300', '22301', '22302', '22303', '22304', '22305', '22306', '22307', '22308', '22309', '2231', '22310', '22311', '22312', '22313', '22314', '22315', '22316', '22317', '22318', '22319', '2232', '22320', '22321', '22322', '22323', '22324', '22325', '22326', '22327', '22328', '22329', '2233', '22330', '22331', '22332', '22333', '22334', '22335', '22336', '22337', '22338', '22339', '2234', '22340', '22341', '22342', '22343', '22344', '22345', '22346', '22347', '22348', '22349', '2235', '22350', '22351', '22352', '22353', '22354', '22355', '22356', '22357', '22358', '22359', '2236', '22360', '22361', '22362', '22363', '22364', '22365', '22366', '22367', '22368', '22369', '2237', '22370', '22371', '22372', '22373', '22374', '22375', '22376', '22377', '22378', '22379', '2238', '22380', '22381', '22382', '22383', '22384', '22385', '22386', '22387', '22388', '22389', '2239', '22390', '22391', '22392', '22393', '22394', '22395', '22396', '22397', '22398', '22399', '2240', '22400', '22401', '22402', '22403', '22404', '22405', '22406', '22407', '22408', '22409', '2241', '22410', '22411', '22412', '22413', '22414', '22415', '22416', '22417', '22418', '22419', '2242', '22420', '22421', '22422', '22423', '22424', '22425', '22426', '22427', '22428', '22429', '2243', '22430', '22431', '22432', '22433', '22434', '22435', '22436', '22437', '22438', '22439', '2244', '22440', '22441', '22442', '22443', '22444', '22445', '22446', '22447', '22448', '22449', '2245', '22450', '22451', '22452', '22453', '22454', '22455', '22456', '22457', '22458', '22459', '2246', '22460', '22461', '22462', '22463', '22464', '22465', '22466', '22467', '22468', '22469', '2247', '22470', '22471', '22472', '22473', '22474', '22475', '22476', '22477', '22478', '22479', '2248', '22480', '22481', '22482', '22483', '22484', '22485', '22486', '22487', '22488', '22489', '2249', '22490', '22491', '22492', '22493', '22494', '22495', '22496', '22497', '22498', '22499', '2250', '22500', '22501', '22502', '22503', '22504', '22505', '22506', '22507', '22508', '22509', '2251', '22510', '22511', '22512', '22513', '22514', '22515', '22516', '22517', '22518', '22519', '2252', '22520', '22521', '22522', '22523', '22524', '22525', '22526', '22527', '22528', '2253', '2254', '2255', '2256', '2257', '2258', '2259', '2260', '2261', '2262', '2263', '2264', '2265', '2266', '2267', '2268', '2269', '2270', '2271', '2272', '2273', '2274', '2275', '2276', '2277', '2278', '2279', '2280', '2281', '2282', '2283', '2284', '2285', '2286', '2287', '2288', '2289', '2290', '2291', '2292', '2293', '2294', '2295', '2296', '2297', '2298', '2299', '2300', '2301', '2302', '2303', '2304', '2305', '2306', '2307', '2308', '2309', '2310', '2311', '2312', '2313', '2314', '2315', '2316', '2317', '2318', '2319', '2320', '2321', '2322', '2323', '2324', '2325', '2326', '2327', '2328', '2329', '2330', '2331', '2332', '2333', '2334', '2335', '2336', '2337', '2338', '2339', '2340', '2341', '2342', '2343', '2344', '2345', '2346', '2347', '2348', '2349', '2350', '2351', '2352', '2353', '2354', '2355', '2356', '2357', '2358', '2359', '2360', '2361', '2362', '2363', '2364', '2365', '2366', '2367', '2368', '2369', '2370', '2371', '2372', '2373', '2374', '2375', '2376', '2377', '2378', '2379', '2380', '2381', '2382', '2383', '2384', '2385', '2386', '2387', '2388', '2389', '2390', '2391', '2392', '2393', '2394', '2395', '2396', '2397', '2398', '2399', '2400', '2401', '2402', '2403', '2404', '2405', '2406', '2407', '2408', '2409', '2410', '2411', '2412', '2413', '2414', '2415', '2416', '2417', '2418', '2419', '2420', '2421', '2422', '2423', '2424', '2425', '2426', '2427', '2428', '2429', '2430', '2431', '2432', '2433', '2434', '2435', '2436', '2437', '2438', '2439', '2440', '2441', '2442', '2443', '2444', '2445', '2446', '2447', '2448', '2449', '2450', '2451', '2452', '2453', '2454', '2455', '2456', '2457', '2458', '2459', '2460', '2461', '2462', '2463', '2464', '2465', '2466', '2467', '2468', '2469', '2470', '2471', '2472', '2473', '2474', '2475', '2476', '2477', '2478', '2479', '2480', '2481', '2482', '2483', '2484', '2485', '2486', '2487', '2488', '2489', '2490', '2491', '2492', '2493', '2494', '2495', '2496', '2497', '2498', '2499', '2500', '2501', '2502', '2503', '2504', '2505', '2506', '2507', '2508', '2509', '2510', '2511', '2512', '2513', '2514', '2515', '2516', '2517', '2518', '2519', '2520', '2521', '2522', '2523', '2524', '2525', '2526', '2527', '2528', '2529', '2530', '2531', '2532', '2533', '2534', '2535', '2536', '2537', '2538', '2539', '2540', '2541', '2542', '2543', '2544', '2545', '2546', '2547', '2548', '2549', '2550', '2551', '2552', '2553', '2554', '2555', '2556', '2557', '2558', '2559', '2560', '2561', '2562', '2563', '2564', '2565', '2566', '2567', '2568', '2569', '2570', '2571', '2572', '2573', '2574', '2575', '2576', '2577', '2578', '2579', '2580', '2581', '2582', '2583', '2584', '2585', '2586', '2587', '2588', '2589', '2590', '2591', '2592', '2593', '2594', '2595', '2596', '2597', '2598', '2599', '2600', '2601', '2602', '2603', '2604', '2605', '2606', '2607', '2608', '2609', '2610', '2611', '2612', '2613', '2614', '2615', '2616', '2617', '2618', '2619', '2620', '2621', '2622', '2623', '2624', '2625', '2626', '2627', '2628', '2629', '2630', '2631', '2632', '2633', '2634', '2635', '2636', '2637', '2638', '2639', '2640', '2641', '2642', '2643', '2644', '2645', '2646', '2647', '2648', '2649', '2650', '2651', '2652', '2653', '2654', '2655', '2656', '2657', '2658', '2659', '2660', '2661', '2662', '2663', '2664', '2665', '2666', '2667', '2668', '2669', '2670', '2671', '2672', '2673', '2674', '2675', '2676', '2677', '2678', '2679', '2680', '2681', '2682', '2683', '2684', '2685', '2686', '2687', '2688', '2689', '2690', '2691', '2692', '2693', '2694', '2695', '2696', '2697', '2698', '2699', '2700', '2701', '2702', '2703', '2704', '2705', '2706', '2707', '2708', '2709', '2710', '2711', '2712', '2713', '2714', '2715', '2716', '2717', '2718', '2719', '2720', '2721', '2722', '2723', '2724', '2725', '2726', '2727', '2728', '2729', '2730', '2731', '2732', '2733', '2734', '2735', '2736', '2737', '2738', '2739', '2740', '2741', '2742', '2743', '2744', '2745', '2746', '2747', '2748', '2749', '2750', '2751', '2752', '2753', '2754', '2755', '2756', '2757', '2758', '2759', '2760', '2761', '2762', '2763', '2764', '2765', '2766', '2767', '2768', '2769', '2770', '2771', '2772', '2773', '2774', '2775', '2776', '2777', '2778', '2779', '2780', '2781', '2782', '2783', '2784', '2785', '2786', '2787', '2788', '2789', '2790', '2791', '2792', '2793', '2794', '2795', '2796', '2797', '2798', '2799', '2800', '2801', '2802', '2803', '2804', '2805', '2806', '2807', '2808', '2809', '2810', '2811', '2812', '2813', '2814', '2815', '2816', '2817', '2818', '2819', '2820', '2821', '2822', '2823', '2824', '2825', '2826', '2827', '2828', '2829', '2830', '2831', '2832', '2833', '2834', '2835', '2836', '2837', '2838', '2839', '2840', '2841', '2842', '2843', '2844', '2845', '2846', '2847', '2848', '2849', '2850', '2851', '2852', '2853', '2854', '2855', '2856', '2857', '2858', '2859', '2860', '2861', '2862', '2863', '2864', '2865', '2866', '2867', '2868', '2869', '2870', '2871', '2872', '2873', '2874', '2875', '2876', '2877', '2878', '2879', '2880', '2881', '2882', '2883', '2884', '2885', '2886', '2887', '2888', '2889', '2890', '2891', '2892', '2893', '2894', '2895', '2896', '2897', '2898', '2899', '2900', '2901', '2902', '2903', '2904', '2905', '2906', '2907', '2908', '2909', '2910', '2911', '2912', '2913', '2914', '2915', '2916', '2917', '2918', '2919', '2920', '2921', '2922', '2923', '2924', '2925', '2926', '2927', '2928', '2929', '2930', '2931', '2932', '2933', '2934', '2935', '2936', '2937', '2938', '2939', '2940', '2941', '2942', '2943', '2944', '2945', '2946', '2947', '2948', '2949', '2950', '2951', '2952', '2953', '2954', '2955', '2956', '2957', '2958', '2959', '2960', '2961', '2962', '2963', '2964', '2965', '2966', '2967', '2968', '2969', '2970', '2971', '2972', '2973', '2974', '2975', '2976', '2977', '2978', '2979', '2980', '2981', '2982', '2983', '2984', '2985', '2986', '2987', '2988', '2989', '2990', '2991', '2992', '2993', '2994', '2995', '2996', '2997', '2998', '2999', '3000', '3001', '3002', '3003', '3004', '3005', '3006', '3007', '3008', '3009', '3010', '3011', '3012', '3013', '3014', '3015', '3016', '3017', '3018', '3019', '3020', '3021', '3022', '3023', '3024', '3025', '3026', '3027', '3028', '3029', '3030', '3031', '3032', '3033', '3034', '3035', '3036', '3037', '3038', '3039', '3040', '3041', '3042', '3043', '3044', '3045', '3046', '3047', '3048', '3049', '3050', '3051', '3052', '3053', '3054', '3055', '3056', '3057', '3058', '3059', '3060', '3061', '3062', '3063', '3064', '3065', '3066', '3067', '3068', '3069', '3070', '3071', '3072', '3073', '3074', '3075', '3076', '3077', '3078', '3079', '3080', '3081', '3082', '3083', '3084', '3085', '3086', '3087', '3088', '3089', '3090', '3091', '3092', '3093', '3094', '3095', '3096', '3097', '3098', '3099', '3100', '3101', '3102', '3103', '3104', '3105', '3106', '3107', '3108', '3109', '3110', '3111', '3112', '3113', '3114', '3115', '3116', '3117', '3118', '3119', '3120', '3121', '3122', '3123', '3124', '3125', '3126', '3127', '3128', '3129', '3130', '3131', '3132', '3133', '3134', '3135', '3136', '3137', '3138', '3139', '3140', '3141', '3142', '3143', '3144', '3145', '3146', '3147', '3148', '3149', '3150', '3151', '3152', '3153', '3154', '3155', '3156', '3157', '3158', '3159', '3160', '3161', '3162', '3163', '3164', '3165', '3166', '3167', '3168', '3169', '3170', '3171', '3172', '3173', '3174', '3175', '3176', '3177', '3178', '3179', '3180', '3181', '3182', '3183', '3184', '3185', '3186', '3187', '3188', '3189', '3190', '3191', '3192', '3193', '3194', '3195', '3196', '3197', '3198', '3199', '3200', '3201', '3202', '3203', '3204', '3205', '3206', '3207', '3208', '3209', '3210', '3211', '3212', '3213', '3214', '3215', '3216', '3217', '3218', '3219', '3220', '3221', '3222', '3223', '3224', '3225', '3226', '3227', '3228', '3229', '3230', '3231', '3232', '3233', '3234', '3235', '3236', '3237', '3238', '3239', '3240', '3241', '3242', '3243', '3244', '3245', '3246', '3247', '3248', '3249', '3250', '3251', '3252', '3253', '3254', '3255', '3256', '3257', '3258', '3259', '3260', '3261', '3262', '3263', '3264', '3265', '3266', '3267', '3268', '3269', '3270', '3271', '3272', '3273', '3274', '3275', '3276', '3277', '3278', '3279', '3280', '3281', '3282', '3283', '3284', '3285', '3286', '3287', '3288', '3289', '3290', '3291', '3292', '3293', '3294', '3295', '3296', '3297', '3298', '3299', '3300', '3301', '3302', '3303', '3304', '3305', '3306', '3307', '3308', '3309', '3310', '3311', '3312', '3313', '3314', '3315', '3316', '3317', '3318', '3319', '3320', '3321', '3322', '3323', '3324', '3325', '3326', '3327', '3328', '3329', '3330', '3331', '3332', '3333', '3334', '3335', '3336', '3337', '3338', '3339', '3340', '3341', '3342', '3343', '3344', '3345', '3346', '3347', '3348', '3349', '3350', '3351', '3352', '3353', '3354', '3355', '3356', '3357', '3358', '3359', '3360', '3361', '3362', '3363', '3364', '3365', '3366', '3367', '3368', '3369', '3370', '3371', '3372', '3373', '3374', '3375', '3376', '3377', '3378', '3379', '3380', '3381', '3382', '3383', '3384', '3385', '3386', '3387', '3388', '3389', '3390', '3391', '3392', '3393', '3394', '3395', '3396', '3397', '3398', '3399', '3400', '3401', '3402', '3403', '3404', '3405', '3406', '3407', '3408', '3409', '3410', '3411', '3412', '3413', '3414', '3415', '3416', '3417', '3418', '3419', '3420', '3421', '3422', '3423', '3424', '3425', '3426', '3427', '3428', '3429', '3430', '3431', '3432', '3433', '3434', '3435', '3436', '3437', '3438', '3439', '3440', '3441', '3442', '3443', '3444', '3445', '3446', '3447', '3448', '3449', '3450', '3451', '3452', '3453', '3454', '3455', '3456', '3457', '3458', '3459', '3460', '3461', '3462', '3463', '3464', '3465', '3466', '3467', '3468', '3469', '3470', '3471', '3472', '3473', '3474', '3475', '3476', '3477', '3478', '3479', '3480', '3481', '3482', '3483', '3484', '3485', '3486', '3487', '3488', '3489', '3490', '3491', '3492', '3493', '3494', '3495', '3496', '3497', '3498', '3499', '3500', '3501', '3502', '3503', '3504', '3505', '3506', '3507', '3508', '3509', '3510', '3511', '3512', '3513', '3514', '3515', '3516', '3517', '3518', '3519', '3520', '3521', '3522', '3523', '3524', '3525', '3526', '3527', '3528', '3529', '3530', '3531', '3532', '3533', '3534', '3535', '3536', '3537', '3538', '3539', '3540', '3541', '3542', '3543', '3544', '3545', '3546', '3547', '3548', '3549', '3550', '3551', '3552', '3553', '3554', '3555', '3556', '3557', '3558', '3559', '3560', '3561', '3562', '3563', '3564', '3565', '3566', '3567', '3568', '3569', '3570', '3571', '3572', '3573', '3574', '3575', '3576', '3577', '3578', '3579', '3580', '3581', '3582', '3583', '3584', '3585', '3586', '3587', '3588', '3589', '3590', '3591', '3592', '3593', '3594', '3595', '3596', '3597', '3598', '3599', '3600', '3601', '3602', '3603', '3604', '3605', '3606', '3607', '3608', '3609', '3610', '3611', '3612', '3613', '3614', '3615', '3616', '3617', '3618', '3619', '3620', '3621', '3622', '3623', '3624', '3625', '3626', '3627', '3628', '3629', '3630', '3631', '3632', '3633', '3634', '3635', '3636', '3637', '3638', '3639', '3640', '3641', '3642', '3643', '3644', '3645', '3646', '3647', '3648', '3649', '3650', '3651', '3652', '3653', '3654', '3655', '3656', '3657', '3658', '3659', '3660', '3661', '3662', '3663', '3664', '3665', '3666', '3667', '3668', '3669', '3670', '3671', '3672', '3673', '3674', '3675', '3676', '3677', '3678', '3679', '3680', '3681', '3682', '3683', '3684', '3685', '3686', '3687', '3688', '3689', '3690', '3691', '3692', '3693', '3694', '3695', '3696', '3697', '3698', '3699', '3700', '3701', '3702', '3703', '3704', '3705', '3706', '3707', '3708', '3709', '3710', '3711', '3712', '3713', '3714', '3715', '3716', '3717', '3718', '3719', '3720', '3721', '3722', '3723', '3724', '3725', '3726', '3727', '3728', '3729', '3730', '3731', '3732', '3733', '3734', '3735', '3736', '3737', '3738', '3739', '3740', '3741', '3742', '3743', '3744', '3745', '3746', '3747', '3748', '3749', '3750', '3751', '3752', '3753', '3754', '3755', '3756', '3757', '3758', '3759', '3760', '3761', '3762', '3763', '3764', '3765', '3766', '3767', '3768', '3769', '3770', '3771', '3772', '3773', '3774', '3775', '3776', '3777', '3778', '3779', '3780', '3781', '3782', '3783', '3784', '3785', '3786', '3787', '3788', '3789', '3790', '3791', '3792', '3793', '3794', '3795', '3796', '3797', '3798', '3799', '3800', '3801', '3802', '3803', '3804', '3805', '3806', '3807', '3808', '3809', '3810', '3811', '3812', '3813', '3814', '3815', '3816', '3817', '3818', '3819', '3820', '3821', '3822', '3823', '3824', '3825', '3826', '3827', '3828', '3829', '3830', '3831', '3832', '3833', '3834', '3835', '3836', '3837', '3838', '3839', '3840', '3841', '3842', '3843', '3844', '3845', '3846', '3847', '3848', '3849', '3850', '3851', '3852', '3853', '3854', '3855', '3856', '3857', '3858', '3859', '3860', '3861', '3862', '3863', '3864', '3865', '3866', '3867', '3868', '3869', '3870', '3871', '3872', '3873', '3874', '3875', '3876', '3877', '3878', '3879', '3880', '3881', '3882', '3883', '3884', '3885', '3886', '3887', '3888', '3889', '3890', '3891', '3892', '3893', '3894', '3895', '3896', '3897', '3898', '3899', '3900', '3901', '3902', '3903', '3904', '3905', '3906', '3907', '3908', '3909', '3910', '3911', '3912', '3913', '3914', '3915', '3916', '3917', '3918', '3919', '3920', '3921', '3922', '3923', '3924', '3925', '3926', '3927', '3928', '3929', '3930', '3931', '3932', '3933', '3934', '3935', '3936', '3937', '3938', '3939', '3940', '3941', '3942', '3943', '3944', '3945', '3946', '3947', '3948', '3949', '3950', '3951', '3952', '3953', '3954', '3955', '3956', '3957', '3958', '3959', '3960', '3961', '3962', '3963', '3964', '3965', '3966', '3967', '3968', '3969', '3970', '3971', '3972', '3973', '3974', '3975', '3976', '3977', '3978', '3979', '3980', '3981', '3982', '3983', '3984', '3985', '3986', '3987', '3988', '3989', '3990', '3991', '3992', '3993', '3994', '3995', '3996', '3997', '3998', '3999', '4000', '4001', '4002', '4003', '4004', '4005', '4006', '4007', '4008', '4009', '4010', '4011', '4012', '4013', '4014', '4015', '4016', '4017', '4018', '4019', '4020', '4021', '4022', '4023', '4024', '4025', '4026', '4027', '4028', '4029', '4030', '4031', '4032', '4033', '4034', '4035', '4036', '4037', '4038', '4039', '4040', '4041', '4042', '4043', '4044', '4045', '4046', '4047', '4048', '4049', '4050', '4051', '4052', '4053', '4054', '4055', '4056', '4057', '4058', '4059', '4060', '4061', '4062', '4063', '4064', '4065', '4066', '4067', '4068', '4069', '4070', '4071', '4072', '4073', '4074', '4075', '4076', '4077', '4078', '4079', '4080', '4081', '4082', '4083', '4084', '4085', '4086', '4087', '4088', '4089', '4090', '4091', '4092', '4093', '4094', '4095', '4096', '4097', '4098', '4099', '4100', '4101', '4102', '4103', '4104', '4105', '4106', '4107', '4108', '4109', '4110', '4111', '4112', '4113', '4114', '4115', '4116', '4117', '4118', '4119', '4120', '4121', '4122', '4123', '4124', '4125', '4126', '4127', '4128', '4129', '4130', '4131', '4132', '4133', '4134', '4135', '4136', '4137', '4138', '4139', '4140', '4141', '4142', '4143', '4144', '4145', '4146', '4147', '4148', '4149', '4150', '4151', '4152', '4153', '4154', '4155', '4156', '4157', '4158', '4159', '4160', '4161', '4162', '4163', '4164', '4165', '4166', '4167', '4168', '4169', '4170', '4171', '4172', '4173', '4174', '4175', '4176', '4177', '4178', '4179', '4180', '4181', '4182', '4183', '4184', '4185', '4186', '4187', '4188', '4189', '4190', '4191', '4192', '4193', '4194', '4195', '4196', '4197', '4198', '4199', '4200', '4201', '4202', '4203', '4204', '4205', '4206', '4207', '4208', '4209', '4210', '4211', '4212', '4213', '4214', '4215', '4216', '4217', '4218', '4219', '4220', '4221', '4222', '4223', '4224', '4225', '4226', '4227', '4228', '4229', '4230', '4231', '4232', '4233', '4234', '4235', '4236', '4237', '4238', '4239', '4240', '4241', '4242', '4243', '4244', '4245', '4246', '4247', '4248', '4249', '4250', '4251', '4252', '4253', '4254', '4255', '4256', '4257', '4258', '4259', '4260', '4261', '4262', '4263', '4264', '4265', '4266', '4267', '4268', '4269', '4270', '4271', '4272', '4273', '4274', '4275', '4276', '4277', '4278', '4279', '4280', '4281', '4282', '4283', '4284', '4285', '4286', '4287', '4288', '4289', '4290', '4291', '4292', '4293', '4294', '4295', '4296', '4297', '4298', '4299', '4300', '4301', '4302', '4303', '4304', '4305', '4306', '4307', '4308', '4309', '4310', '4311', '4312', '4313', '4314', '4315', '4316', '4317', '4318', '4319', '4320', '4321', '4322', '4323', '4324', '4325', '4326', '4327', '4328', '4329', '4330', '4331', '4332', '4333', '4334', '4335', '4336', '4337', '4338', '4339', '4340', '4341', '4342', '4343', '4344', '4345', '4346', '4347', '4348', '4349', '4350', '4351', '4352', '4353', '4354', '4355', '4356', '4357', '4358', '4359', '4360', '4361', '4362', '4363', '4364', '4365', '4366', '4367', '4368', '4369', '4370', '4371', '4372', '4373', '4374', '4375', '4376', '4377', '4378', '4379', '4380', '4381', '4382', '4383', '4384', '4385', '4386', '4387', '4388', '4389', '4390', '4391', '4392', '4393', '4394', '4395', '4396', '4397', '4398', '4399', '4400', '4401', '4402', '4403', '4404', '4405', '4406', '4407', '4408', '4409', '4410', '4411', '4412', '4413', '4414', '4415', '4416', '4417', '4418', '4419', '4420', '4421', '4422', '4423', '4424', '4425', '4426', '4427', '4428', '4429', '4430', '4431', '4432', '4433', '4434', '4435', '4436', '4437', '4438', '4439', '4440', '4441', '4442', '4443', '4444', '4445', '4446', '4447', '4448', '4449', '4450', '4451', '4452', '4453', '4454', '4455', '4456', '4457', '4458', '4459', '4460', '4461', '4462', '4463', '4464', '4465', '4466', '4467', '4468', '4469', '4470', '4471', '4472', '4473', '4474', '4475', '4476', '4477', '4478', '4479', '4480', '4481', '4482', '4483', '4484', '4485', '4486', '4487', '4488', '4489', '4490', '4491', '4492', '4493', '4494', '4495', '4496', '4497', '4498', '4499', '4500', '4501', '4502', '4503', '4504', '4505', '4506', '4507', '4508', '4509', '4510', '4511', '4512', '4513', '4514', '4515', '4516', '4517', '4518', '4519', '4520', '4521', '4522', '4523', '4524', '4525', '4526', '4527', '4528', '4529', '4530', '4531', '4532', '4533', '4534', '4535', '4536', '4537', '4538', '4539', '4540', '4541', '4542', '4543', '4544', '4545', '4546', '4547', '4548', '4549', '4550', '4551', '4552', '4553', '4554', '4555', '4556', '4557', '4558', '4559', '4560', '4561', '4562', '4563', '4564', '4565', '4566', '4567', '4568', '4569', '4570', '4571', '4572', '4573', '4574', '4575', '4576', '4577', '4578', '4579', '4580', '4581', '4582', '4583', '4584', '4585', '4586', '4587', '4588', '4589', '4590', '4591', '4592', '4593', '4594', '4595', '4596', '4597', '4598', '4599', '4600', '4601', '4602', '4603', '4604', '4605', '4606', '4607', '4608', '4609', '4610', '4611', '4612', '4613', '4614', '4615', '4616', '4617', '4618', '4619', '4620', '4621', '4622', '4623', '4624', '4625', '4626', '4627', '4628', '4629', '4630', '4631', '4632', '4633', '4634', '4635', '4636', '4637', '4638', '4639', '4640', '4641', '4642', '4643', '4644', '4645', '4646', '4647', '4648', '4649', '4650', '4651', '4652', '4653', '4654', '4655', '4656', '4657', '4658', '4659', '4660', '4661', '4662', '4663', '4664', '4665', '4666', '4667', '4668', '4669', '4670', '4671', '4672', '4673', '4674', '4675', '4676', '4677', '4678', '4679', '4680', '4681', '4682', '4683', '4684', '4685', '4686', '4687', '4688', '4689', '4690', '4691', '4692', '4693', '4694', '4695', '4696', '4697', '4698', '4699', '4700', '4701', '4702', '4703', '4704', '4705', '4706', '4707', '4708', '4709', '4710', '4711', '4712', '4713', '4714', '4715', '4716', '4717', '4718', '4719', '4720', '4721', '4722', '4723', '4724', '4725', '4726', '4727', '4728', '4729', '4730', '4731', '4732', '4733', '4734', '4735', '4736', '4737', '4738', '4739', '4740', '4741', '4742', '4743', '4744', '4745', '4746', '4747', '4748', '4749', '4750', '4751', '4752', '4753', '4754', '4755', '4756', '4757', '4758', '4759', '4760', '4761', '4762', '4763', '4764', '4765', '4766', '4767', '4768', '4769', '4770', '4771', '4772', '4773', '4774', '4775', '4776', '4777', '4778', '4779', '4780', '4781', '4782', '4783', '4784', '4785', '4786', '4787', '4788', '4789', '4790', '4791', '4792', '4793', '4794', '4795', '4796', '4797', '4798', '4799', '4800', '4801', '4802', '4803', '4804', '4805', '4806', '4807', '4808', '4809', '4810', '4811', '4812', '4813', '4814', '4815', '4816', '4817', '4818', '4819', '4820', '4821', '4822', '4823', '4824', '4825', '4826', '4827', '4828', '4829', '4830', '4831', '4832', '4833', '4834', '4835', '4836', '4837', '4838', '4839', '4840', '4841', '4842', '4843', '4844', '4845', '4846', '4847', '4848', '4849', '4850', '4851', '4852', '4853', '4854', '4855', '4856', '4857', '4858', '4859', '4860', '4861', '4862', '4863', '4864', '4865', '4866', '4867', '4868', '4869', '4870', '4871', '4872', '4873', '4874', '4875', '4876', '4877', '4878', '4879', '4880', '4881', '4882', '4883', '4884', '4885', '4886', '4887', '4888', '4889', '4890', '4891', '4892', '4893', '4894', '4895', '4896', '4897', '4898', '4899', '4900', '4901', '4902', '4903', '4904', '4905', '4906', '4907', '4908', '4909', '4910', '4911', '4912', '4913', '4914', '4915', '4916', '4917', '4918', '4919', '4920', '4921', '4922', '4923', '4924', '4925', '4926', '4927', '4928', '4929', '4930', '4931', '4932', '4933', '4934', '4935', '4936', '4937', '4938', '4939', '4940', '4941', '4942', '4943', '4944', '4945', '4946', '4947', '4948', '4949', '4950', '4951', '4952', '4953', '4954', '4955', '4956', '4957', '4958', '4959', '4960', '4961', '4962', '4963', '4964', '4965', '4966', '4967', '4968', '4969', '4970', '4971', '4972', '4973', '4974', '4975', '4976', '4977', '4978', '4979', '4980', '4981', '4982', '4983', '4984', '4985', '4986', '4987', '4988', '4989', '4990', '4991', '4992', '4993', '4994', '4995', '4996', '4997', '4998', '4999', '5000', '5001', '5002', '5003', '5004', '5005', '5006', '5007', '5008', '5009', '5010', '5011', '5012', '5013', '5014', '5015', '5016', '5017', '5018', '5019', '5020', '5021', '5022', '5023', '5024', '5025', '5026', '5027', '5028', '5029', '5030', '5031', '5032', '5033', '5034', '5035', '5036', '5037', '5038', '5039', '5040', '5041', '5042', '5043', '5044', '5045', '5046', '5047', '5048', '5049', '5050', '5051', '5052', '5053', '5054', '5055', '5056', '5057', '5058', '5059', '5060', '5061', '5062', '5063', '5064', '5065', '5066', '5067', '5068', '5069', '5070', '5071', '5072', '5073', '5074', '5075', '5076', '5077', '5078', '5079', '5080', '5081', '5082', '5083', '5084', '5085', '5086', '5087', '5088', '5089', '5090', '5091', '5092', '5093', '5094', '5095', '5096', '5097', '5098', '5099', '5100', '5101', '5102', '5103', '5104', '5105', '5106', '5107', '5108', '5109', '5110', '5111', '5112', '5113', '5114', '5115', '5116', '5117', '5118', '5119', '5120', '5121', '5122', '5123', '5124', '5125', '5126', '5127', '5128', '5129', '5130', '5131', '5132', '5133', '5134', '5135', '5136', '5137', '5138', '5139', '5140', '5141', '5142', '5143', '5144', '5145', '5146', '5147', '5148', '5149', '5150', '5151', '5152', '5153', '5154', '5155', '5156', '5157', '5158', '5159', '5160', '5161', '5162', '5163', '5164', '5165', '5166', '5167', '5168', '5169', '5170', '5171', '5172', '5173', '5174', '5175', '5176', '5177', '5178', '5179', '5180', '5181', '5182', '5183', '5184', '5185', '5186', '5187', '5188', '5189', '5190', '5191', '5192', '5193', '5194', '5195', '5196', '5197', '5198', '5199', '5200', '5201', '5202', '5203', '5204', '5205', '5206', '5207', '5208', '5209', '5210', '5211', '5212', '5213', '5214', '5215', '5216', '5217', '5218', '5219', '5220', '5221', '5222', '5223', '5224', '5225', '5226', '5227', '5228', '5229', '5230', '5231', '5232', '5233', '5234', '5235', '5236', '5237', '5238', '5239', '5240', '5241', '5242', '5243', '5244', '5245', '5246', '5247', '5248', '5249', '5250', '5251', '5252', '5253', '5254', '5255', '5256', '5257', '5258', '5259', '5260', '5261', '5262', '5263', '5264', '5265', '5266', '5267', '5268', '5269', '5270', '5271', '5272', '5273', '5274', '5275', '5276', '5277', '5278', '5279', '5280', '5281', '5282', '5283', '5284', '5285', '5286', '5287', '5288', '5289', '5290', '5291', '5292', '5293', '5294', '5295', '5296', '5297', '5298', '5299', '5300', '5301', '5302', '5303', '5304', '5305', '5306', '5307', '5308', '5309', '5310', '5311', '5312', '5313', '5314', '5315', '5316', '5317', '5318', '5319', '5320', '5321', '5322', '5323', '5324', '5325', '5326', '5327', '5328', '5329', '5330', '5331', '5332', '5333', '5334', '5335', '5336', '5337', '5338', '5339', '5340', '5341', '5342', '5343', '5344', '5345', '5346', '5347', '5348', '5349', '5350', '5351', '5352', '5353', '5354', '5355', '5356', '5357', '5358', '5359', '5360', '5361', '5362', '5363', '5364', '5365', '5366', '5367', '5368', '5369', '5370', '5371', '5372', '5373', '5374', '5375', '5376', '5377', '5378', '5379', '5380', '5381', '5382', '5383', '5384', '5385', '5386', '5387', '5388', '5389', '5390', '5391', '5392', '5393', '5394', '5395', '5396', '5397', '5398', '5399', '5400', '5401', '5402', '5403', '5404', '5405', '5406', '5407', '5408', '5409', '5410', '5411', '5412', '5413', '5414', '5415', '5416', '5417', '5418', '5419', '5420', '5421', '5422', '5423', '5424', '5425', '5426', '5427', '5428', '5429', '5430', '5431', '5432', '5433', '5434', '5435', '5436', '5437', '5438', '5439', '5440', '5441', '5442', '5443', '5444', '5445', '5446', '5447', '5448', '5449', '5450', '5451', '5452', '5453', '5454', '5455', '5456', '5457', '5458', '5459', '5460', '5461', '5462', '5463', '5464', '5465', '5466', '5467', '5468', '5469', '5470', '5471', '5472', '5473', '5474', '5475', '5476', '5477', '5478', '5479', '5480', '5481', '5482', '5483', '5484', '5485', '5486', '5487', '5488', '5489', '5490', '5491', '5492', '5493', '5494', '5495', '5496', '5497', '5498', '5499', '5500', '5501', '5502', '5503', '5504', '5505', '5506', '5507', '5508', '5509', '5510', '5511', '5512', '5513', '5514', '5515', '5516', '5517', '5518', '5519', '5520', '5521', '5522', '5523', '5524', '5525', '5526', '5527', '5528', '5529', '5530', '5531', '5532', '5533', '5534', '5535', '5536', '5537', '5538', '5539', '5540', '5541', '5542', '5543', '5544', '5545', '5546', '5547', '5548', '5549', '5550', '5551', '5552', '5553', '5554', '5555', '5556', '5557', '5558', '5559', '5560', '5561', '5562', '5563', '5564', '5565', '5566', '5567', '5568', '5569', '5570', '5571', '5572', '5573', '5574', '5575', '5576', '5577', '5578', '5579', '5580', '5581', '5582', '5583', '5584', '5585', '5586', '5587', '5588', '5589', '5590', '5591', '5592', '5593', '5594', '5595', '5596', '5597', '5598', '5599', '5600', '5601', '5602', '5603', '5604', '5605', '5606', '5607', '5608', '5609', '5610', '5611', '5612', '5613', '5614', '5615', '5616', '5617', '5618', '5619', '5620', '5621', '5622', '5623', '5624', '5625', '5626', '5627', '5628', '5629', '5630', '5631', '5632', '5633', '5634', '5635', '5636', '5637', '5638', '5639', '5640', '5641', '5642', '5643', '5644', '5645', '5646', '5647', '5648', '5649', '5650', '5651', '5652', '5653', '5654', '5655', '5656', '5657', '5658', '5659', '5660', '5661', '5662', '5663', '5664', '5665', '5666', '5667', '5668', '5669', '5670', '5671', '5672', '5673', '5674', '5675', '5676', '5677', '5678', '5679', '5680', '5681', '5682', '5683', '5684', '5685', '5686', '5687', '5688', '5689', '5690', '5691', '5692', '5693', '5694', '5695', '5696', '5697', '5698', '5699', '5700', '5701', '5702', '5703', '5704', '5705', '5706', '5707', '5708', '5709', '5710', '5711', '5712', '5713', '5714', '5715', '5716', '5717', '5718', '5719', '5720', '5721', '5722', '5723', '5724', '5725', '5726', '5727', '5728', '5729', '5730', '5731', '5732', '5733', '5734', '5735', '5736', '5737', '5738', '5739', '5740', '5741', '5742', '5743', '5744', '5745', '5746', '5747', '5748', '5749', '5750', '5751', '5752', '5753', '5754', '5755', '5756', '5757', '5758', '5759', '5760', '5761', '5762', '5763', '5764', '5765', '5766', '5767', '5768', '5769', '5770', '5771', '5772', '5773', '5774', '5775', '5776', '5777', '5778', '5779', '5780', '5781', '5782', '5783', '5784', '5785', '5786', '5787', '5788', '5789', '5790', '5791', '5792', '5793', '5794', '5795', '5796', '5797', '5798', '5799', '5800', '5801', '5802', '5803', '5804', '5805', '5806', '5807', '5808', '5809', '5810', '5811', '5812', '5813', '5814', '5815', '5816', '5817', '5818', '5819', '5820', '5821', '5822', '5823', '5824', '5825', '5826', '5827', '5828', '5829', '5830', '5831', '5832', '5833', '5834', '5835', '5836', '5837', '5838', '5839', '5840', '5841', '5842', '5843', '5844', '5845', '5846', '5847', '5848', '5849', '5850', '5851', '5852', '5853', '5854', '5855', '5856', '5857', '5858', '5859', '5860', '5861', '5862', '5863', '5864', '5865', '5866', '5867', '5868', '5869', '5870', '5871', '5872', '5873', '5874', '5875', '5876', '5877', '5878', '5879', '5880', '5881', '5882', '5883', '5884', '5885', '5886', '5887', '5888', '5889', '5890', '5891', '5892', '5893', '5894', '5895', '5896', '5897', '5898', '5899', '5900', '5901', '5902', '5903', '5904', '5905', '5906', '5907', '5908', '5909', '5910', '5911', '5912', '5913', '5914', '5915', '5916', '5917', '5918', '5919', '5920', '5921', '5922', '5923', '5924', '5925', '5926', '5927', '5928', '5929', '5930', '5931', '5932', '5933', '5934', '5935', '5936', '5937', '5938', '5939', '5940', '5941', '5942', '5943', '5944', '5945', '5946', '5947', '5948', '5949', '5950', '5951', '5952', '5953', '5954', '5955', '5956', '5957', '5958', '5959', '5960', '5961', '5962', '5963', '5964', '5965', '5966', '5967', '5968', '5969', '5970', '5971', '5972', '5973', '5974', '5975', '5976', '5977', '5978', '5979', '5980', '5981', '5982', '5983', '5984', '5985', '5986', '5987', '5988', '5989', '5990', '5991', '5992', '5993', '5994', '5995', '5996', '5997', '5998', '5999', '6000', '6001', '6002', '6003', '6004', '6005', '6006', '6007', '6008', '6009', '6010', '6011', '6012', '6013', '6014', '6015', '6016', '6017', '6018', '6019', '6020', '6021', '6022', '6023', '6024', '6025', '6026', '6027', '6028', '6029', '6030', '6031', '6032', '6033', '6034', '6035', '6036', '6037', '6038', '6039', '6040', '6041', '6042', '6043', '6044', '6045', '6046', '6047', '6048', '6049', '6050', '6051', '6052', '6053', '6054', '6055', '6056', '6057', '6058', '6059', '6060', '6061', '6062', '6063', '6064', '6065', '6066', '6067', '6068', '6069', '6070', '6071', '6072', '6073', '6074', '6075', '6076', '6077', '6078', '6079', '6080', '6081', '6082', '6083', '6084', '6085', '6086', '6087', '6088', '6089', '6090', '6091', '6092', '6093', '6094', '6095', '6096', '6097', '6098', '6099', '6100', '6101', '6102', '6103', '6104', '6105', '6106', '6107', '6108', '6109', '6110', '6111', '6112', '6113', '6114', '6115', '6116', '6117', '6118', '6119', '6120', '6121', '6122', '6123', '6124', '6125', '6126', '6127', '6128', '6129', '6130', '6131', '6132', '6133', '6134', '6135', '6136', '6137', '6138', '6139', '6140', '6141', '6142', '6143', '6144', '6145', '6146', '6147', '6148', '6149', '6150', '6151', '6152', '6153', '6154', '6155', '6156', '6157', '6158', '6159', '6160', '6161', '6162', '6163', '6164', '6165', '6166', '6167', '6168', '6169', '6170', '6171', '6172', '6173', '6174', '6175', '6176', '6177', '6178', '6179', '6180', '6181', '6182', '6183', '6184', '6185', '6186', '6187', '6188', '6189', '6190', '6191', '6192', '6193', '6194', '6195', '6196', '6197', '6198', '6199', '6200', '6201', '6202', '6203', '6204', '6205', '6206', '6207', '6208', '6209', '6210', '6211', '6212', '6213', '6214', '6215', '6216', '6217', '6218', '6219', '6220', '6221', '6222', '6223', '6224', '6225', '6226', '6227', '6228', '6229', '6230', '6231', '6232', '6233', '6234', '6235', '6236', '6237', '6238', '6239', '6240', '6241', '6242', '6243', '6244', '6245', '6246', '6247', '6248', '6249', '6250', '6251', '6252', '6253', '6254', '6255', '6256', '6257', '6258', '6259', '6260', '6261', '6262', '6263', '6264', '6265', '6266', '6267', '6268', '6269', '6270', '6271', '6272', '6273', '6274', '6275', '6276', '6277', '6278', '6279', '6280', '6281', '6282', '6283', '6284', '6285', '6286', '6287', '6288', '6289', '6290', '6291', '6292', '6293', '6294', '6295', '6296', '6297', '6298', '6299', '6300', '6301', '6302', '6303', '6304', '6305', '6306', '6307', '6308', '6309', '6310', '6311', '6312', '6313', '6314', '6315', '6316', '6317', '6318', '6319', '6320', '6321', '6322', '6323', '6324', '6325', '6326', '6327', '6328', '6329', '6330', '6331', '6332', '6333', '6334', '6335', '6336', '6337', '6338', '6339', '6340', '6341', '6342', '6343', '6344', '6345', '6346', '6347', '6348', '6349', '6350', '6351', '6352', '6353', '6354', '6355', '6356', '6357', '6358', '6359', '6360', '6361', '6362', '6363', '6364', '6365', '6366', '6367', '6368', '6369', '6370', '6371', '6372', '6373', '6374', '6375', '6376', '6377', '6378', '6379', '6380', '6381', '6382', '6383', '6384', '6385', '6386', '6387', '6388', '6389', '6390', '6391', '6392', '6393', '6394', '6395', '6396', '6397', '6398', '6399', '6400', '6401', '6402', '6403', '6404', '6405', '6406', '6407', '6408', '6409', '6410', '6411', '6412', '6413', '6414', '6415', '6416', '6417', '6418', '6419', '6420', '6421', '6422', '6423', '6424', '6425', '6426', '6427', '6428', '6429', '6430', '6431', '6432', '6433', '6434', '6435', '6436', '6437', '6438', '6439', '6440', '6441', '6442', '6443', '6444', '6445', '6446', '6447', '6448', '6449', '6450', '6451', '6452', '6453', '6454', '6455', '6456', '6457', '6458', '6459', '6460', '6461', '6462', '6463', '6464', '6465', '6466', '6467', '6468', '6469', '6470', '6471', '6472', '6473', '6474', '6475', '6476', '6477', '6478', '6479', '6480', '6481', '6482', '6483', '6484', '6485', '6486', '6487', '6488', '6489', '6490', '6491', '6492', '6493', '6494', '6495', '6496', '6497', '6498', '6499', '6500', '6501', '6502', '6503', '6504', '6505', '6506', '6507', '6508', '6509', '6510', '6511', '6512', '6513', '6514', '6515', '6516', '6517', '6518', '6519', '6520', '6521', '6522', '6523', '6524', '6525', '6526', '6527', '6528', '6529', '6530', '6531', '6532', '6533', '6534', '6535', '6536', '6537', '6538', '6539', '6540', '6541', '6542', '6543', '6544', '6545', '6546', '6547', '6548', '6549', '6550', '6551', '6552', '6553', '6554', '6555', '6556', '6557', '6558', '6559', '6560', '6561', '6562', '6563', '6564', '6565', '6566', '6567', '6568', '6569', '6570', '6571', '6572', '6573', '6574', '6575', '6576', '6577', '6578', '6579', '6580', '6581', '6582', '6583', '6584', '6585', '6586', '6587', '6588', '6589', '6590', '6591', '6592', '6593', '6594', '6595', '6596', '6597', '6598', '6599', '6600', '6601', '6602', '6603', '6604', '6605', '6606', '6607', '6608', '6609', '6610', '6611', '6612', '6613', '6614', '6615', '6616', '6617', '6618', '6619', '6620', '6621', '6622', '6623', '6624', '6625', '6626', '6627', '6628', '6629', '6630', '6631', '6632', '6633', '6634', '6635', '6636', '6637', '6638', '6639', '6640', '6641', '6642', '6643', '6644', '6645', '6646', '6647', '6648', '6649', '6650', '6651', '6652', '6653', '6654', '6655', '6656', '6657', '6658', '6659', '6660', '6661', '6662', '6663', '6664', '6665', '6666', '6667', '6668', '6669', '6670', '6671', '6672', '6673', '6674', '6675', '6676', '6677', '6678', '6679', '6680', '6681', '6682', '6683', '6684', '6685', '6686', '6687', '6688', '6689', '6690', '6691', '6692', '6693', '6694', '6695', '6696', '6697', '6698', '6699', '6700', '6701', '6702', '6703', '6704', '6705', '6706', '6707', '6708', '6709', '6710', '6711', '6712', '6713', '6714', '6715', '6716', '6717', '6718', '6719', '6720', '6721', '6722', '6723', '6724', '6725', '6726', '6727', '6728', '6729', '6730', '6731', '6732', '6733', '6734', '6735', '6736', '6737', '6738', '6739', '6740', '6741', '6742', '6743', '6744', '6745', '6746', '6747', '6748', '6749', '6750', '6751', '6752', '6753', '6754', '6755', '6756', '6757', '6758', '6759', '6760', '6761', '6762', '6763', '6764', '6765', '6766', '6767', '6768', '6769', '6770', '6771', '6772', '6773', '6774', '6775', '6776', '6777', '6778', '6779', '6780', '6781', '6782', '6783', '6784', '6785', '6786', '6787', '6788', '6789', '6790', '6791', '6792', '6793', '6794', '6795', '6796', '6797', '6798', '6799', '6800', '6801', '6802', '6803', '6804', '6805', '6806', '6807', '6808', '6809', '6810', '6811', '6812', '6813', '6814', '6815', '6816', '6817', '6818', '6819', '6820', '6821', '6822', '6823', '6824', '6825', '6826', '6827', '6828', '6829', '6830', '6831', '6832', '6833', '6834', '6835', '6836', '6837', '6838', '6839', '6840', '6841', '6842', '6843', '6844', '6845', '6846', '6847', '6848', '6849', '6850', '6851', '6852', '6853', '6854', '6855', '6856', '6857', '6858', '6859', '6860', '6861', '6862', '6863', '6864', '6865', '6866', '6867', '6868', '6869', '6870', '6871', '6872', '6873', '6874', '6875', '6876', '6877', '6878', '6879', '6880', '6881', '6882', '6883', '6884', '6885', '6886', '6887', '6888', '6889', '6890', '6891', '6892', '6893', '6894', '6895', '6896', '6897', '6898', '6899', '6900', '6901', '6902', '6903', '6904', '6905', '6906', '6907', '6908', '6909', '6910', '6911', '6912', '6913', '6914', '6915', '6916', '6917', '6918', '6919', '6920', '6921', '6922', '6923', '6924', '6925', '6926', '6927', '6928', '6929', '6930', '6931', '6932', '6933', '6934', '6935', '6936', '6937', '6938', '6939', '6940', '6941', '6942', '6943', '6944', '6945', '6946', '6947', '6948', '6949', '6950', '6951', '6952', '6953', '6954', '6955', '6956', '6957', '6958', '6959', '6960', '6961', '6962', '6963', '6964', '6965', '6966', '6967', '6968', '6969', '6970', '6971', '6972', '6973', '6974', '6975', '6976', '6977', '6978', '6979', '6980', '6981', '6982', '6983', '6984', '6985', '6986', '6987', '6988', '6989', '6990', '6991', '6992', '6993', '6994', '6995', '6996', '6997', '6998', '6999', '7000', '7001', '7002', '7003', '7004', '7005', '7006', '7007', '7008', '7009', '7010', '7011', '7012', '7013', '7014', '7015', '7016', '7017', '7018', '7019', '7020', '7021', '7022', '7023', '7024', '7025', '7026', '7027', '7028', '7029', '7030', '7031', '7032', '7033', '7034', '7035', '7036', '7037', '7038', '7039', '7040', '7041', '7042', '7043', '7044', '7045', '7046', '7047', '7048', '7049', '7050', '7051', '7052', '7053', '7054', '7055', '7056', '7057', '7058', '7059', '7060', '7061', '7062', '7063', '7064', '7065', '7066', '7067', '7068', '7069', '7070', '7071', '7072', '7073', '7074', '7075', '7076', '7077', '7078', '7079', '7080', '7081', '7082', '7083', '7084', '7085', '7086', '7087', '7088', '7089', '7090', '7091', '7092', '7093', '7094', '7095', '7096', '7097', '7098', '7099', '7100', '7101', '7102', '7103', '7104', '7105', '7106', '7107', '7108', '7109', '7110', '7111', '7112', '7113', '7114', '7115', '7116', '7117', '7118', '7119', '7120', '7121', '7122', '7123', '7124', '7125', '7126', '7127', '7128', '7129', '7130', '7131', '7132', '7133', '7134', '7135', '7136', '7137', '7138', '7139', '7140', '7141', '7142', '7143', '7144', '7145', '7146', '7147', '7148', '7149', '7150', '7151', '7152', '7153', '7154', '7155', '7156', '7157', '7158', '7159', '7160', '7161', '7162', '7163', '7164', '7165', '7166', '7167', '7168', '7169', '7170', '7171', '7172', '7173', '7174', '7175', '7176', '7177', '7178', '7179', '7180', '7181', '7182', '7183', '7184', '7185', '7186', '7187', '7188', '7189', '7190', '7191', '7192', '7193', '7194', '7195', '7196', '7197', '7198', '7199', '7200', '7201', '7202', '7203', '7204', '7205', '7206', '7207', '7208', '7209', '7210', '7211', '7212', '7213', '7214', '7215', '7216', '7217', '7218', '7219', '7220', '7221', '7222', '7223', '7224', '7225', '7226', '7227', '7228', '7229', '7230', '7231', '7232', '7233', '7234', '7235', '7236', '7237', '7238', '7239', '7240', '7241', '7242', '7243', '7244', '7245', '7246', '7247', '7248', '7249', '7250', '7251', '7252', '7253', '7254', '7255', '7256', '7257', '7258', '7259', '7260', '7261', '7262', '7263', '7264', '7265', '7266', '7267', '7268', '7269', '7270', '7271', '7272', '7273', '7274', '7275', '7276', '7277', '7278', '7279', '7280', '7281', '7282', '7283', '7284', '7285', '7286', '7287', '7288', '7289', '7290', '7291', '7292', '7293', '7294', '7295', '7296', '7297', '7298', '7299', '7300', '7301', '7302', '7303', '7304', '7305', '7306', '7307', '7308', '7309', '7310', '7311', '7312', '7313', '7314', '7315', '7316', '7317', '7318', '7319', '7320', '7321', '7322', '7323', '7324', '7325', '7326', '7327', '7328', '7329', '7330', '7331', '7332', '7333', '7334', '7335', '7336', '7337', '7338', '7339', '7340', '7341', '7342', '7343', '7344', '7345', '7346', '7347', '7348', '7349', '7350', '7351', '7352', '7353', '7354', '7355', '7356', '7357', '7358', '7359', '7360', '7361', '7362', '7363', '7364', '7365', '7366', '7367', '7368', '7369', '7370', '7371', '7372', '7373', '7374', '7375', '7376', '7377', '7378', '7379', '7380', '7381', '7382', '7383', '7384', '7385', '7386', '7387', '7388', '7389', '7390', '7391', '7392', '7393', '7394', '7395', '7396', '7397', '7398', '7399', '7400', '7401', '7402', '7403', '7404', '7405', '7406', '7407', '7408', '7409', '7410', '7411', '7412', '7413', '7414', '7415', '7416', '7417', '7418', '7419', '7420', '7421', '7422', '7423', '7424', '7425', '7426', '7427', '7428', '7429', '7430', '7431', '7432', '7433', '7434', '7435', '7436', '7437', '7438', '7439', '7440', '7441', '7442', '7443', '7444', '7445', '7446', '7447', '7448', '7449', '7450', '7451', '7452', '7453', '7454', '7455', '7456', '7457', '7458', '7459', '7460', '7461', '7462', '7463', '7464', '7465', '7466', '7467', '7468', '7469', '7470', '7471', '7472', '7473', '7474', '7475', '7476', '7477', '7478', '7479', '7480', '7481', '7482', '7483', '7484', '7485', '7486', '7487', '7488', '7489', '7490', '7491', '7492', '7493', '7494', '7495', '7496', '7497', '7498', '7499', '7500', '7501', '7502', '7503', '7504', '7505', '7506', '7507', '7508', '7509', '7510', '7511', '7512', '7513', '7514', '7515', '7516', '7517', '7518', '7519', '7520', '7521', '7522', '7523', '7524', '7525', '7526', '7527', '7528', '7529', '7530', '7531', '7532', '7533', '7534', '7535', '7536', '7537', '7538', '7539', '7540', '7541', '7542', '7543', '7544', '7545', '7546', '7547', '7548', '7549', '7550', '7551', '7552', '7553', '7554', '7555', '7556', '7557', '7558', '7559', '7560', '7561', '7562', '7563', '7564', '7565', '7566', '7567', '7568', '7569', '7570', '7571', '7572', '7573', '7574', '7575', '7576', '7577', '7578', '7579', '7580', '7581', '7582', '7583', '7584', '7585', '7586', '7587', '7588', '7589', '7590', '7591', '7592', '7593', '7594', '7595', '7596', '7597', '7598', '7599', '7600', '7601', '7602', '7603', '7604', '7605', '7606', '7607', '7608', '7609', '7610', '7611', '7612', '7613', '7614', '7615', '7616', '7617', '7618', '7619', '7620', '7621', '7622', '7623', '7624', '7625', '7626', '7627', '7628', '7629', '7630', '7631', '7632', '7633', '7634', '7635', '7636', '7637', '7638', '7639', '7640', '7641', '7642', '7643', '7644', '7645', '7646', '7647', '7648', '7649', '7650', '7651', '7652', '7653', '7654', '7655', '7656', '7657', '7658', '7659', '7660', '7661', '7662', '7663', '7664', '7665', '7666', '7667', '7668', '7669', '7670', '7671', '7672', '7673', '7674', '7675', '7676', '7677', '7678', '7679', '7680', '7681', '7682', '7683', '7684', '7685', '7686', '7687', '7688', '7689', '7690', '7691', '7692', '7693', '7694', '7695', '7696', '7697', '7698', '7699', '7700', '7701', '7702', '7703', '7704', '7705', '7706', '7707', '7708', '7709', '7710', '7711', '7712', '7713', '7714', '7715', '7716', '7717', '7718', '7719', '7720', '7721', '7722', '7723', '7724', '7725', '7726', '7727', '7728', '7729', '7730', '7731', '7732', '7733', '7734', '7735', '7736', '7737', '7738', '7739', '7740', '7741', '7742', '7743', '7744', '7745', '7746', '7747', '7748', '7749', '7750', '7751', '7752', '7753', '7754', '7755', '7756', '7757', '7758', '7759', '7760', '7761', '7762', '7763', '7764', '7765', '7766', '7767', '7768', '7769', '7770', '7771', '7772', '7773', '7774', '7775', '7776', '7777', '7778', '7779', '7780', '7781', '7782', '7783', '7784', '7785', '7786', '7787', '7788', '7789', '7790', '7791', '7792', '7793', '7794', '7795', '7796', '7797', '7798', '7799', '7800', '7801', '7802', '7803', '7804', '7805', '7806', '7807', '7808', '7809', '7810', '7811', '7812', '7813', '7814', '7815', '7816', '7817', '7818', '7819', '7820', '7821', '7822', '7823', '7824', '7825', '7826', '7827', '7828', '7829', '7830', '7831', '7832', '7833', '7834', '7835', '7836', '7837', '7838', '7839', '7840', '7841', '7842', '7843', '7844', '7845', '7846', '7847', '7848', '7849', '7850', '7851', '7852', '7853', '7854', '7855', '7856', '7857', '7858', '7859', '7860', '7861', '7862', '7863', '7864', '7865', '7866', '7867', '7868', '7869', '7870', '7871', '7872', '7873', '7874', '7875', '7876', '7877', '7878', '7879', '7880', '7881', '7882', '7883', '7884', '7885', '7886', '7887', '7888', '7889', '7890', '7891', '7892', '7893', '7894', '7895', '7896', '7897', '7898', '7899', '7900', '7901', '7902', '7903', '7904', '7905', '7906', '7907', '7908', '7909', '7910', '7911', '7912', '7913', '7914', '7915', '7916', '7917', '7918', '7919', '7920', '7921', '7922', '7923', '7924', '7925', '7926', '7927', '7928', '7929', '7930', '7931', '7932', '7933', '7934', '7935', '7936', '7937', '7938', '7939', '7940', '7941', '7942', '7943', '7944', '7945', '7946', '7947', '7948', '7949', '7950', '7951', '7952', '7953', '7954', '7955', '7956', '7957', '7958', '7959', '7960', '7961', '7962', '7963', '7964', '7965', '7966', '7967', '7968', '7969', '7970', '7971', '7972', '7973', '7974', '7975', '7976', '7977', '7978', '7979', '7980', '7981', '7982', '7983', '7984', '7985', '7986', '7987', '7988', '7989', '7990', '7991', '7992', '7993', '7994', '7995', '7996', '7997', '7998', '7999', '8000', '8001', '8002', '8003', '8004', '8005', '8006', '8007', '8008', '8009', '8010', '8011', '8012', '8013', '8014', '8015', '8016', '8017', '8018', '8019', '8020', '8021', '8022', '8023', '8024', '8025', '8026', '8027', '8028', '8029', '8030', '8031', '8032', '8033', '8034', '8035', '8036', '8037', '8038', '8039', '8040', '8041', '8042', '8043', '8044', '8045', '8046', '8047', '8048', '8049', '8050', '8051', '8052', '8053', '8054', '8055', '8056', '8057', '8058', '8059', '8060', '8061', '8062', '8063', '8064', '8065', '8066', '8067', '8068', '8069', '8070', '8071', '8072', '8073', '8074', '8075', '8076', '8077', '8078', '8079', '8080', '8081', '8082', '8083', '8084', '8085', '8086', '8087', '8088', '8089', '8090', '8091', '8092', '8093', '8094', '8095', '8096', '8097', '8098', '8099', '8100', '8101', '8102', '8103', '8104', '8105', '8106', '8107', '8108', '8109', '8110', '8111', '8112', '8113', '8114', '8115', '8116', '8117', '8118', '8119', '8120', '8121', '8122', '8123', '8124', '8125', '8126', '8127', '8128', '8129', '8130', '8131', '8132', '8133', '8134', '8135', '8136', '8137', '8138', '8139', '8140', '8141', '8142', '8143', '8144', '8145', '8146', '8147', '8148', '8149', '8150', '8151', '8152', '8153', '8154', '8155', '8156', '8157', '8158', '8159', '8160', '8161', '8162', '8163', '8164', '8165', '8166', '8167', '8168', '8169', '8170', '8171', '8172', '8173', '8174', '8175', '8176', '8177', '8178', '8179', '8180', '8181', '8182', '8183', '8184', '8185', '8186', '8187', '8188', '8189', '8190', '8191', '8192', '8193', '8194', '8195', '8196', '8197', '8198', '8199', '8200', '8201', '8202', '8203', '8204', '8205', '8206', '8207', '8208', '8209', '8210', '8211', '8212', '8213', '8214', '8215', '8216', '8217', '8218', '8219', '8220', '8221', '8222', '8223', '8224', '8225', '8226', '8227', '8228', '8229', '8230', '8231', '8232', '8233', '8234', '8235', '8236', '8237', '8238', '8239', '8240', '8241', '8242', '8243', '8244', '8245', '8246', '8247', '8248', '8249', '8250', '8251', '8252', '8253', '8254', '8255', '8256', '8257', '8258', '8259', '8260', '8261', '8262', '8263', '8264', '8265', '8266', '8267', '8268', '8269', '8270', '8271', '8272', '8273', '8274', '8275', '8276', '8277', '8278', '8279', '8280', '8281', '8282', '8283', '8284', '8285', '8286', '8287', '8288', '8289', '8290', '8291', '8292', '8293', '8294', '8295', '8296', '8297', '8298', '8299', '8300', '8301', '8302', '8303', '8304', '8305', '8306', '8307', '8308', '8309', '8310', '8311', '8312', '8313', '8314', '8315', '8316', '8317', '8318', '8319', '8320', '8321', '8322', '8323', '8324', '8325', '8326', '8327', '8328', '8329', '8330', '8331', '8332', '8333', '8334', '8335', '8336', '8337', '8338', '8339', '8340', '8341', '8342', '8343', '8344', '8345', '8346', '8347', '8348', '8349', '8350', '8351', '8352', '8353', '8354', '8355', '8356', '8357', '8358', '8359', '8360', '8361', '8362', '8363', '8364', '8365', '8366', '8367', '8368', '8369', '8370', '8371', '8372', '8373', '8374', '8375', '8376', '8377', '8378', '8379', '8380', '8381', '8382', '8383', '8384', '8385', '8386', '8387', '8388', '8389', '8390', '8391', '8392', '8393', '8394', '8395', '8396', '8397', '8398', '8399', '8400', '8401', '8402', '8403', '8404', '8405', '8406', '8407', '8408', '8409', '8410', '8411', '8412', '8413', '8414', '8415', '8416', '8417', '8418', '8419', '8420', '8421', '8422', '8423', '8424', '8425', '8426', '8427', '8428', '8429', '8430', '8431', '8432', '8433', '8434', '8435', '8436', '8437', '8438', '8439', '8440', '8441', '8442', '8443', '8444', '8445', '8446', '8447', '8448', '8449', '8450', '8451', '8452', '8453', '8454', '8455', '8456', '8457', '8458', '8459', '8460', '8461', '8462', '8463', '8464', '8465', '8466', '8467', '8468', '8469', '8470', '8471', '8472', '8473', '8474', '8475', '8476', '8477', '8478', '8479', '8480', '8481', '8482', '8483', '8484', '8485', '8486', '8487', '8488', '8489', '8490', '8491', '8492', '8493', '8494', '8495', '8496', '8497', '8498', '8499', '8500', '8501', '8502', '8503', '8504', '8505', '8506', '8507', '8508', '8509', '8510', '8511', '8512', '8513', '8514', '8515', '8516', '8517', '8518', '8519', '8520', '8521', '8522', '8523', '8524', '8525', '8526', '8527', '8528', '8529', '8530', '8531', '8532', '8533', '8534', '8535', '8536', '8537', '8538', '8539', '8540', '8541', '8542', '8543', '8544', '8545', '8546', '8547', '8548', '8549', '8550', '8551', '8552', '8553', '8554', '8555', '8556', '8557', '8558', '8559', '8560', '8561', '8562', '8563', '8564', '8565', '8566', '8567', '8568', '8569', '8570', '8571', '8572', '8573', '8574', '8575', '8576', '8577', '8578', '8579', '8580', '8581', '8582', '8583', '8584', '8585', '8586', '8587', '8588', '8589', '8590', '8591', '8592', '8593', '8594', '8595', '8596', '8597', '8598', '8599', '8600', '8601', '8602', '8603', '8604', '8605', '8606', '8607', '8608', '8609', '8610', '8611', '8612', '8613', '8614', '8615', '8616', '8617', '8618', '8619', '8620', '8621', '8622', '8623', '8624', '8625', '8626', '8627', '8628', '8629', '8630', '8631', '8632', '8633', '8634', '8635', '8636', '8637', '8638', '8639', '8640', '8641', '8642', '8643', '8644', '8645', '8646', '8647', '8648', '8649', '8650', '8651', '8652', '8653', '8654', '8655', '8656', '8657', '8658', '8659', '8660', '8661', '8662', '8663', '8664', '8665', '8666', '8667', '8668', '8669', '8670', '8671', '8672', '8673', '8674', '8675', '8676', '8677', '8678', '8679', '8680', '8681', '8682', '8683', '8684', '8685', '8686', '8687', '8688', '8689', '8690', '8691', '8692', '8693', '8694', '8695', '8696', '8697', '8698', '8699', '8700', '8701', '8702', '8703', '8704', '8705', '8706', '8707', '8708', '8709', '8710', '8711', '8712', '8713', '8714', '8715', '8716', '8717', '8718', '8719', '8720', '8721', '8722', '8723', '8724', '8725', '8726', '8727', '8728', '8729', '8730', '8731', '8732', '8733', '8734', '8735', '8736', '8737', '8738', '8739', '8740', '8741', '8742', '8743', '8744', '8745', '8746', '8747', '8748', '8749', '8750', '8751', '8752', '8753', '8754', '8755', '8756', '8757', '8758', '8759', '8760', '8761', '8762', '8763', '8764', '8765', '8766', '8767', '8768', '8769', '8770', '8771', '8772', '8773', '8774', '8775', '8776', '8777', '8778', '8779', '8780', '8781', '8782', '8783', '8784', '8785', '8786', '8787', '8788', '8789', '8790', '8791', '8792', '8793', '8794', '8795', '8796', '8797', '8798', '8799', '8800', '8801', '8802', '8803', '8804', '8805', '8806', '8807', '8808', '8809', '8810', '8811', '8812', '8813', '8814', '8815', '8816', '8817', '8818', '8819', '8820', '8821', '8822', '8823', '8824', '8825', '8826', '8827', '8828', '8829', '8830', '8831', '8832', '8833', '8834', '8835', '8836', '8837', '8838', '8839', '8840', '8841', '8842', '8843', '8844', '8845', '8846', '8847', '8848', '8849', '8850', '8851', '8852', '8853', '8854', '8855', '8856', '8857', '8858', '8859', '8860', '8861', '8862', '8863', '8864', '8865', '8866', '8867', '8868', '8869', '8870', '8871', '8872', '8873', '8874', '8875', '8876', '8877', '8878', '8879', '8880', '8881', '8882', '8883', '8884', '8885', '8886', '8887', '8888', '8889', '8890', '8891', '8892', '8893', '8894', '8895', '8896', '8897', '8898', '8899', '8900', '8901', '8902', '8903', '8904', '8905', '8906', '8907', '8908', '8909', '8910', '8911', '8912', '8913', '8914', '8915', '8916', '8917', '8918', '8919', '8920', '8921', '8922', '8923', '8924', '8925', '8926', '8927', '8928', '8929', '8930', '8931', '8932', '8933', '8934', '8935', '8936', '8937', '8938', '8939', '8940', '8941', '8942', '8943', '8944', '8945', '8946', '8947', '8948', '8949', '8950', '8951', '8952', '8953', '8954', '8955', '8956', '8957', '8958', '8959', '8960', '8961', '8962', '8963', '8964', '8965', '8966', '8967', '8968', '8969', '8970', '8971', '8972', '8973', '8974', '8975', '8976', '8977', '8978', '8979', '8980', '8981', '8982', '8983', '8984', '8985', '8986', '8987', '8988', '8989', '8990', '8991', '8992', '8993', '8994', '8995', '8996', '8997', '8998', '8999', '9000', '9001', '9002', '9003', '9004', '9005', '9006', '9007', '9008', '9009', '9010', '9011', '9012', '9013', '9014', '9015', '9016', '9017', '9018', '9019', '9020', '9021', '9022', '9023', '9024', '9025', '9026', '9027', '9028', '9029', '9030', '9031', '9032', '9033', '9034', '9035', '9036', '9037', '9038', '9039', '9040', '9041', '9042', '9043', '9044', '9045', '9046', '9047', '9048', '9049', '9050', '9051', '9052', '9053', '9054', '9055', '9056', '9057', '9058', '9059', '9060', '9061', '9062', '9063', '9064', '9065', '9066', '9067', '9068', '9069', '9070', '9071', '9072', '9073', '9074', '9075', '9076', '9077', '9078', '9079', '9080', '9081', '9082', '9083', '9084', '9085', '9086', '9087', '9088', '9089', '9090', '9091', '9092', '9093', '9094', '9095', '9096', '9097', '9098', '9099', '9100', '9101', '9102', '9103', '9104', '9105', '9106', '9107', '9108', '9109', '9110', '9111', '9112', '9113', '9114', '9115', '9116', '9117', '9118', '9119', '9120', '9121', '9122', '9123', '9124', '9125', '9126', '9127', '9128', '9129', '9130', '9131', '9132', '9133', '9134', '9135', '9136', '9137', '9138', '9139', '9140', '9141', '9142', '9143', '9144', '9145', '9146', '9147', '9148', '9149', '9150', '9151', '9152', '9153', '9154', '9155', '9156', '9157', '9158', '9159', '9160', '9161', '9162', '9163', '9164', '9165', '9166', '9167', '9168', '9169', '9170', '9171', '9172', '9173', '9174', '9175', '9176', '9177', '9178', '9179', '9180', '9181', '9182', '9183', '9184', '9185', '9186', '9187', '9188', '9189', '9190', '9191', '9192', '9193', '9194', '9195', '9196', '9197', '9198', '9199', '9200', '9201', '9202', '9203', '9204', '9205', '9206', '9207', '9208', '9209', '9210', '9211', '9212', '9213', '9214', '9215', '9216', '9217', '9218', '9219', '9220', '9221', '9222', '9223', '9224', '9225', '9226', '9227', '9228', '9229', '9230', '9231', '9232', '9233', '9234', '9235', '9236', '9237', '9238', '9239', '9240', '9241', '9242', '9243', '9244', '9245', '9246', '9247', '9248', '9249', '9250', '9251', '9252', '9253', '9254', '9255', '9256', '9257', '9258', '9259', '9260', '9261', '9262', '9263', '9264', '9265', '9266', '9267', '9268', '9269', '9270', '9271', '9272', '9273', '9274', '9275', '9276', '9277', '9278', '9279', '9280', '9281', '9282', '9283', '9284', '9285', '9286', '9287', '9288', '9289', '9290', '9291', '9292', '9293', '9294', '9295', '9296', '9297', '9298', '9299', '9300', '9301', '9302', '9303', '9304', '9305', '9306', '9307', '9308', '9309', '9310', '9311', '9312', '9313', '9314', '9315', '9316', '9317', '9318', '9319', '9320', '9321', '9322', '9323', '9324', '9325', '9326', '9327', '9328', '9329', '9330', '9331', '9332', '9333', '9334', '9335', '9336', '9337', '9338', '9339', '9340', '9341', '9342', '9343', '9344', '9345', '9346', '9347', '9348', '9349', '9350', '9351', '9352', '9353', '9354', '9355', '9356', '9357', '9358', '9359', '9360', '9361', '9362', '9363', '9364', '9365', '9366', '9367', '9368', '9369', '9370', '9371', '9372', '9373', '9374', '9375', '9376', '9377', '9378', '9379', '9380', '9381', '9382', '9383', '9384', '9385', '9386', '9387', '9388', '9389', '9390', '9391', '9392', '9393', '9394', '9395', '9396', '9397', '9398', '9399', '9400', '9401', '9402', '9403', '9404', '9405', '9406', '9407', '9408', '9409', '9410', '9411', '9412', '9413', '9414', '9415', '9416', '9417', '9418', '9419', '9420', '9421', '9422', '9423', '9424', '9425', '9426', '9427', '9428', '9429', '9430', '9431', '9432', '9433', '9434', '9435', '9436', '9437', '9438', '9439', '9440', '9441', '9442', '9443', '9444', '9445', '9446', '9447', '9448', '9449', '9450', '9451', '9452', '9453', '9454', '9455', '9456', '9457', '9458', '9459', '9460', '9461', '9462', '9463', '9464', '9465', '9466', '9467', '9468', '9469', '9470', '9471', '9472', '9473', '9474', '9475', '9476', '9477', '9478', '9479', '9480', '9481', '9482', '9483', '9484', '9485', '9486', '9487', '9488', '9489', '9490', '9491', '9492', '9493', '9494', '9495', '9496', '9497', '9498', '9499', '9500', '9501', '9502', '9503', '9504', '9505', '9506', '9507', '9508', '9509', '9510', '9511', '9512', '9513', '9514', '9515', '9516', '9517', '9518', '9519', '9520', '9521', '9522', '9523', '9524', '9525', '9526', '9527', '9528', '9529', '9530', '9531', '9532', '9533', '9534', '9535', '9536', '9537', '9538', '9539', '9540', '9541', '9542', '9543', '9544', '9545', '9546', '9547', '9548', '9549', '9550', '9551', '9552', '9553', '9554', '9555', '9556', '9557', '9558', '9559', '9560', '9561', '9562', '9563', '9564', '9565', '9566', '9567', '9568', '9569', '9570', '9571', '9572', '9573', '9574', '9575', '9576', '9577', '9578', '9579', '9580', '9581', '9582', '9583', '9584', '9585', '9586', '9587', '9588', '9589', '9590', '9591', '9592', '9593', '9594', '9595', '9596', '9597', '9598', '9599', '9600', '9601', '9602', '9603', '9604', '9605', '9606', '9607', '9608', '9609', '9610', '9611', '9612', '9613', '9614', '9615', '9616', '9617', '9618', '9619', '9620', '9621', '9622', '9623', '9624', '9625', '9626', '9627', '9628', '9629', '9630', '9631', '9632', '9633', '9634', '9635', '9636', '9637', '9638', '9639', '9640', '9641', '9642', '9643', '9644', '9645', '9646', '9647', '9648', '9649', '9650', '9651', '9652', '9653', '9654', '9655', '9656', '9657', '9658', '9659', '9660', '9661', '9662', '9663', '9664', '9665', '9666', '9667', '9668', '9669', '9670', '9671', '9672', '9673', '9674', '9675', '9676', '9677', '9678', '9679', '9680', '9681', '9682', '9683', '9684', '9685', '9686', '9687', '9688', '9689', '9690', '9691', '9692', '9693', '9694', '9695', '9696', '9697', '9698', '9699', '9700', '9701', '9702', '9703', '9704', '9705', '9706', '9707', '9708', '9709', '9710', '9711', '9712', '9713', '9714', '9715', '9716', '9717', '9718', '9719', '9720', '9721', '9722', '9723', '9724', '9725', '9726', '9727', '9728', '9729', '9730', '9731', '9732', '9733', '9734', '9735', '9736', '9737', '9738', '9739', '9740', '9741', '9742', '9743', '9744', '9745', '9746', '9747', '9748', '9749', '9750', '9751', '9752', '9753', '9754', '9755', '9756', '9757', '9758', '9759', '9760', '9761', '9762', '9763', '9764', '9765', '9766', '9767', '9768', '9769', '9770', '9771', '9772', '9773', '9774', '9775', '9776', '9777', '9778', '9779', '9780', '9781', '9782', '9783', '9784', '9785', '9786', '9787', '9788', '9789', '9790', '9791', '9792', '9793', '9794', '9795', '9796', '9797', '9798', '9799', '9800', '9801', '9802', '9803', '9804', '9805', '9806', '9807', '9808', '9809', '9810', '9811', '9812', '9813', '9814', '9815', '9816', '9817', '9818', '9819', '9820', '9821', '9822', '9823', '9824', '9825', '9826', '9827', '9828', '9829', '9830', '9831', '9832', '9833', '9834', '9835', '9836', '9837', '9838', '9839', '9840', '9841', '9842', '9843', '9844', '9845', '9846', '9847', '9848', '9849', '9850', '9851', '9852', '9853', '9854', '9855', '9856', '9857', '9858', '9859', '9860', '9861', '9862', '9863', '9864', '9865', '9866', '9867', '9868', '9869', '9870', '9871', '9872', '9873', '9874', '9875', '9876', '9877', '9878', '9879', '9880', '9881', '9882', '9883', '9884', '9885', '9886', '9887', '9888', '9889', '9890', '9891', '9892', '9893', '9894', '9895', '9896', '9897', '9898', '9899', '9900', '9901', '9902', '9903', '9904', '9905', '9906', '9907', '9908', '9909', '9910', '9911', '9912', '9913', '9914', '9915', '9916', '9917', '9918', '9919', '9920', '9921', '9922', '9923', '9924', '9925', '9926', '9927', '9928', '9929', '9930', '9931', '9932', '9933', '9934', '9935', '9936', '9937', '9938', '9939', '9940', '9941', '9942', '9943', '9944', '9945', '9946', '9947', '9948', '9949', '9950', '9951', '9952', '9953', '9954', '9955', '9956', '9957', '9958', '9959', '9960', '9961', '9962', '9963', '9964', '9965', '9966', '9967', '9968', '9969', '9970', '9971', '9972', '9973', '9974', '9975', '9976', '9977', '9978', '9979', '9980', '9981', '9982', '9983', '9984', '9985', '9986', '9987', '9988', '9989', '9990', '9991', '9992', '9993', '9994', '9995', '9996', '9997', '9998', '9999']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xjLSWZJvuVK"
      },
      "source": [
        "for i in range(len(submission)):\n",
        "    submission[\"id\"][i] = mylist[i]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNg9gk9z3Noq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "4c3f9cc3-32ae-474c-bd9c-547be3dec637"
      },
      "source": [
        "submission[\"EfficientNetB5_predict\"] = EfficientNetB5_predict\n",
        "submission.head()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>digit</th>\n",
              "      <th>EfficientNetB5_predict</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10000</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10001</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10002</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10003</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10004</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id  digit  EfficientNetB5_predict\n",
              "0  10000      0                       4\n",
              "1  10001      0                       4\n",
              "2  10002      0                       6\n",
              "3  10003      0                       9\n",
              "4  10004      0                       5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Smd-xg6deOK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a316588d-6b12-478f-958f-c565b3b6d135"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "for i in range(len(submission)) :\n",
        "    predicts = submission.loc[i, ['EfficientNetB5_predict']]\n",
        "    submission.at[i, \"digit\"] = Counter(predicts).most_common(n=1)[0][0]\n",
        "\n",
        "submission.head()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>digit</th>\n",
              "      <th>EfficientNetB5_predict</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10000</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10001</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10002</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10003</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10004</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id  digit  EfficientNetB5_predict\n",
              "0  10000      4                       4\n",
              "1  10001      4                       4\n",
              "2  10002      6                       6\n",
              "3  10003      9                       9\n",
              "4  10004      5                       5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg9m6Zgk4foS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a2be30fc-eb4b-4278-9d8e-499a95b0c51e"
      },
      "source": [
        "submission = submission[['id', 'digit']]\n",
        "submission.head()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>digit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10000</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10001</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10002</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10003</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10004</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id  digit\n",
              "0  10000      4\n",
              "1  10001      4\n",
              "2  10002      6\n",
              "3  10003      9\n",
              "4  10004      5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flAHWrtH4flu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "5b9afeab-5425-4081-ed61-c5e77c2a4f01"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "submission.to_csv('/content/drive/MyDrive/DACON_CVLC/Submission/EfficientNetB5_model.csv', index=False)\n",
        "files.download('/content/drive/MyDrive/DACON_CVLC/Submission/EfficientNetB5_model.csv')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_e2852b66-75cc-4231-bb09-7fd502b38e3e\", \"EfficientNetB5_model.csv\", 155898)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}