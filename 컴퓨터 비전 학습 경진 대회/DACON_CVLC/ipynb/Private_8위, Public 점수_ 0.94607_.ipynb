{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"Private_8위, Public 점수_ 0.94607_.ipynb","provenance":[],"collapsed_sections":[]},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"DTF1lNxTyLxN"},"source":["# Pytoch를 이용한 초보 code 입니다.\n","\n","대회 초반 baseline 으로 올려주신 code를 \n","https://dacon.io/competitions/official/235626/codeshare/1538\n","바탕으로 시작했습니다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9ncXB3qByLxO","executionInfo":{"status":"ok","timestamp":1622320347944,"user_tz":-540,"elapsed":2687,"user":{"displayName":"이상민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTtK8dO4H2M2Xk5k_NkLzdZIUtyrNANJ0VxafBbA=s64","userId":"17307703622932801245"}},"outputId":"5efaca07-0bfb-4c3c-fbbe-f2ed5de89336"},"source":["#Import\n","!pip install tensorboardX\n","import os\n","import random\n","import numpy as np\n","import pandas as pd\n","import cv2\n","from collections import defaultdict\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from sklearn.model_selection import train_test_split\n","from datetime import datetime\n","from tensorboardX import SummaryWriter"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorboardX in /usr/local/lib/python3.7/dist-packages (2.2)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.12.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (56.1.0)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"A5X48yzUyLxP"},"source":["Data 변형:\n","\n","Test size 는 변환하면서 사용.\n","\n"," 초반 psuedo 만들시 train data의 30% 를 validation에 사용.\n"," Psuedo는 여러개 test 하였으며, 일부를 random 하게 test set으로 바꾸어서 train data에서 활용되지 않게함.\n"," \n","Psuedo 전 확인 단계에서는 train data의 30% 를 validation 로 사용.\n","\n","Psuedo 이 후 확인단계에서는 train data의 30% ~ 99.9% 를 validation 로 사용.\n","또한 pseudo data의 100% ~ 80% 를 train data 로 사용.\n","\n","최종 확인 단계에서는 psuedo의 99%를 train data로 사용."]},{"cell_type":"code","metadata":{"id":"dMwwlVTyyLxP"},"source":["device = torch.device(\"cuda\")\n","class reshape_input():\n","    def __init__(self,input,noise, is_pseudo=False):\n","        self.input = input\n","        self.noise = noise\n","        self.is_pseudo = is_pseudo\n","    def merge(self, list1, list2):\n","        merged_list = [(list1[i], list2[i]) for i in range(0, len(list1))]\n","        return merged_list\n","    def  reshape_x(self):\n","        inputx=np.concatenate(\n","            [\n","                pd.get_dummies(self.input.letter).values.reshape(-1, 1, 26),\n","                (self.input[[str(j) for j in range(784)]]).values.reshape(-1, 1, 784)\n","            ],\n","            axis=2\n","        )\n","\n","        inputy=self.input['digit'].values\n","        X_image=np.array(inputx[:, :, 26:], dtype=\"uint8\")\n","        X_image = X_image.squeeze()\n","\n","        appendimg = []\n","        for index in range(0, len(X_image)):\n","            img = X_image[index]\n","        # print(img.shape)\n","            img = np.where(img < self.noise, 0, img)\n","            img = img.reshape(28, 28)\n","            appendimg.append(img)\n","        appended = np.asarray(appendimg, dtype=\"float\")\n","        inputxnum = appended/255.\n","        if self.is_pseudo:\n","            X_train, X_valid, xtrainl, xvalidl, y_train, y_valid = train_test_split(inputxnum, inputx, inputy,\n","                                                                                    test_size=0.001, random_state=42)\n","\n","        else:\n","            X_train, X_valid, xtrainl, xvalidl, y_train, y_valid = train_test_split(inputxnum, inputx, inputy, test_size=0.99, random_state=42)\n","\n","        X_train = torch.Tensor(X_train)\n","        xtrainl = torch.Tensor(xtrainl)\n","        X_valid = torch.Tensor(X_valid)\n","        xvalidl = torch.Tensor(xvalidl)\n","        y_train = torch.LongTensor(y_train)\n","        y_valid = torch.LongTensor(y_valid)\n","\n","        return X_train, X_valid, xtrainl, xvalidl, y_train, y_valid\n","\n","\n","seed_val = 42\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","os.environ['PYTHONHASHSEED'] = str(seed_val)\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d80waxGOyLxQ"},"source":["여러가지 model test: \n","\n","Model은 변환하면서 사용:\n","\n","dropout 은 0.2~0.4 test 진행\n","\n","conv1은 letter를 불러오는 model이니 고정.\n","\n","최종 단계에서는 conv4 사용."]},{"cell_type":"code","metadata":{"id":"Ian7H9oCyLxQ"},"source":["class ConvClassifier(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Sequential(\n","            nn.Conv1d(1, 16, 3, padding=1), nn.ReLU(),\n","            nn.Conv1d(16, 64, 4, padding=1), nn.ReLU(),\n","            nn.Conv1d(64, 128, 5, padding=2), nn.ReLU(),\n","            nn.Conv1d(128, 64, 4, padding=2), nn.ReLU(),\n","            nn.Conv1d(64, 16, 3), nn.ReLU(),\n","            nn.Dropout(0.4),\n","        )\n","\n","        self.conv4 = nn.Sequential(\n","            nn.Conv2d(1, 32, 3), nn.ReLU(),\n","            nn.BatchNorm2d(32),\n","            nn.Conv2d(32, 32, 3), nn.ReLU(),\n","            nn.BatchNorm2d(32),\n","            nn.Conv2d(32, 64, 5, stride=2, padding=2), nn.ReLU(),\n","            nn.BatchNorm2d(64),\n","            nn.Dropout(0.4),\n","\n","            nn.Conv2d(64, 64, 3), nn.ReLU(),\n","            nn.BatchNorm2d(64),\n","            nn.Conv2d(64, 64, 3), nn.ReLU(),\n","            nn.BatchNorm2d(64),\n","            nn.Conv2d(64, 128, 5, stride=2, padding=2), nn.ReLU(),\n","            nn.BatchNorm2d(128),\n","            nn.Dropout(0.4),\n","\n","            nn.Conv2d(128, 128, 4), nn.ReLU(),\n","            nn.BatchNorm2d(128),\n","        )\n","                                # Image를 처리할 2D Conv Block\n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(),\n","            nn.Conv2d(32, 128, 5, padding=2), nn.ReLU(),\n","            nn.Conv2d(128, 256, 7, padding=3), nn.ReLU(),\n","            nn.Conv2d(256, 512, 9, padding=3), nn.ReLU(),\n","            nn.Conv2d(512, 256, 9, padding=3), nn.ReLU(),\n","            nn.Conv2d(256, 128, 7, padding=3), nn.ReLU(),\n","            nn.Conv2d(128, 64, 7, padding=3), nn.ReLU(),\n","            nn.Conv2d(64, 32, 5, padding=3), nn.ReLU(),\n","        )\n","\n","        self.conv3 = nn.Sequential(\n","            nn.Conv1d(1, 128, 5, padding=2), nn.ReLU(),  # 28\n","            # nn.BatchNorm2d(128),\n","            nn.Conv1d(128, 128, 3, padding=1), nn.ReLU(),  # 28\n","            # nn.BatchNorm2d(128),\n","            # nn.MaxPool2d(2, stride=2), #14\n","            nn.Conv1d(128, 256, 3, padding=1), nn.ReLU(),  # 14\n","            # nn.BatchNorm2d(256),\n","            nn.Conv1d(256, 256, 3, padding=1), nn.ReLU(),  # 14\n","            nn.Conv1d(256, 16, 3), nn.ReLU(),  # 26\n","        )\n","        \n","        self.dacon_base = nn.Sequential(\n","            nn.Conv2d(1, 10, 3, padding=1), nn.ReLU(),  # 26\n","            nn.Conv2d(10, 128, 5, padding=2), nn.ReLU(),  # 26\n","            nn.BatchNorm2d(128),\n","            nn.Conv2d(128, 128, 5, padding=2), nn.ReLU(),  # 26\n","            nn.MaxPool2d(2, stride=2),\n","            nn.BatchNorm2d(128),\n","            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(),\n","            nn.Dropout(0.2),\n","            nn.BatchNorm2d(256),\n","            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.BatchNorm2d(256),\n","            nn.Conv2d(256, 128, 4), nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.BatchNorm2d(128)\n","\n","        )\n","\n","        self.conv_test1 = nn.Sequential(\n","            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(),  # 26\n","            nn.BatchNorm2d(32),\n","            nn.Conv2d(32, 256, 5, padding=2), nn.ReLU(),  # 26\n","            nn.BatchNorm2d(256),\n","            nn.Conv2d(256, 512, 9, padding=3), nn.ReLU(),\n","            nn.Conv2d(512, 256, 9, padding=3), nn.ReLU(),\n","            nn.Conv2d(256, 128, 7, padding=3), nn.ReLU(),\n","            nn.Dropout(0.2),\n","            nn.Conv2d(128, 128, 7, padding=3), nn.ReLU(),\n","            nn.BatchNorm2d(128)\n","\n","        )\n","\n","        self.out = nn.Sequential(\n","            nn.Linear(512, 256), nn.ReLU(),\n","            nn.Dropout(0.4),\n","            nn.Linear(256, 32), nn.ReLU(),\n","            nn.Linear(32, 10)\n","        )\n","\n","\n","    def forward(self, x1, x2, label=False):\n","        bsz = x1.size(0)\n","        x1 = self.conv1(x1)\n","        x2 = self.conv4(x2)\n","        x1 = x1.view(bsz, -1)\n","        x2 = x2.view(bsz, -1)\n","        x = torch.cat([x1, x2], dim=1)\n","        torchsize=x[0].shape\n","        out = self.out(x)\n","\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L-2Dk_BJyLxS"},"source":["Model과 test size 외에도, 주로 많이 실험해던 변수들:\n","\n","Data에서 주로 매우 높은 밝기?를 가진 pixel들이 숫자를 보여주는데 사용되었기에 낮은 밝기?의 pixel들의 noise제거 진행. \n","\n","Noise 제거는 5~15 사이로 test 진행."]},{"cell_type":"code","metadata":{"id":"LltmAiDsyLxS"},"source":["######### 주요 변수\n","batch_size = 32\n","noise = 10\n","is_pseudo_ex = True\n","epochs = 85\n","#########"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":443},"id":"T_8uiEcSyLxT","executionInfo":{"status":"error","timestamp":1622320503453,"user_tz":-540,"elapsed":564,"user":{"displayName":"이상민","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTtK8dO4H2M2Xk5k_NkLzdZIUtyrNANJ0VxafBbA=s64","userId":"17307703622932801245"}},"outputId":"6d7d0b40-9810-4d22-8118-2bd6fccb070d"},"source":["#log_setting\n","now=datetime.now()\n","date = now.strftime(\"%m%d.%Y\")\n","time = now.strftime(\"%H%M\")\n","from pathlib import Path\n","Path(\"./logs/\"+date).mkdir(parents=True, exist_ok=True)\n","logpath=\"./logs/\"+date\n","boardlogname = str(time)+\".noise.\"+str(noise)+\".pseudo.\"+str(is_pseudo_ex)\n","Path(\"./logsboard/\"+boardlogname).mkdir(parents=True, exist_ok=True)\n","writer = SummaryWriter(os.path.join('./logsboard/', boardlogname))\n","\n","#call cuda\n","torch.cuda.is_available() #cuda\n","device = torch.device(\"cuda\") #or cpu\n","\n","#input 주요 input\n","\n","train = pd.read_csv('../data/train.csv')\n","if is_pseudo_ex:\n","    pseudo = pd.read_csv('../data/pseudo_df.csv')\n","    #pseudo = pd.read_csv('./data/0903_update_20376.csv')\n","    pseudo.set_index('id')\n","test  = pd.read_csv('../data/test.csv')"],"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-c4921d640cf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_pseudo_ex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mpseudo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/pseudo_df.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;31m#pseudo = pd.read_csv('./data/0903_update_20376.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mpseudo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/pseudo_df.csv'"]}]},{"cell_type":"code","metadata":{"scrolled":false,"id":"w5mQrz3SyLxT"},"source":["train.set_index('id')\n","test.set_index('id')\n","\n","#inputdata normalization\n","reshape_input_class = reshape_input(train,noise, is_pseudo=False)\n","X_train, X_valid, xtrainl, xvalidl, y_train, y_valid = reshape_input_class.reshape_x()\n","#print(X_train.shape, X_valid.shape, y_train.shape, y_valid.shape)\n","if is_pseudo_ex:\n","    reshape_pseudo_class = reshape_input(pseudo, noise, is_pseudo= is_pseudo_ex)\n","    pX_train, pX_valid, pxtrainl, pxvalidl, py_train, py_valid = reshape_pseudo_class.reshape_x()\n","    print(X_train.shape, X_valid.shape, y_train.shape, y_valid.shape, xvalidl.shape, xtrainl.shape)\n","    print(pX_train.shape, pX_valid.shape, py_train.shape, py_valid.shape, pxvalidl.shape, pxtrainl.shape)\n","    X_train=torch.cat([X_train, pX_train],dim=0)\n","    xtrainl = torch.cat([xtrainl, pxtrainl], dim=0)\n","    y_train = torch.cat([y_train, py_train], dim=0)\n","    print(X_train.shape, X_valid.shape, y_train.shape, y_valid.shape, xvalidl.shape, xtrainl.shape)\n","\n","#dataloader\n","from torch.utils.data import (\n","    TensorDataset, \n","    DataLoader, \n","    RandomSampler, \n","    SequentialSampler\n",")\n","\n","train_data = TensorDataset(\n","    xtrainl[:, :, :26], # Letter\n","    X_train.reshape(-1, 1, 28, 28), # Image (28, 28)\n","    y_train # Label\n",")\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(\n","    train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","validation_data = TensorDataset(\n","    xvalidl[:, :, :26], # Letter\n","    X_valid.reshape(-1, 1, 28, 28), # Image (28, 28)\n","    y_valid # Label\n",")\n","validation_sampler = SequentialSampler(\n","    validation_data)\n","validation_dataloader = DataLoader(\n","    validation_data, sampler=validation_sampler, batch_size=batch_size)\n","\n","\n","model = ConvClassifier()\n","model.cuda()\n","\n","x1 = xtrainl[:32, :, :26].cuda()\n","x2 = X_train[:32, :, :].reshape(-1, 1, 28, 28).cuda()\n","model(x1, x2).shape\n","\n","# Define loss model, multi-label to cross entropy loss\n","ce_loss = nn.CrossEntropyLoss()\n","\n","# Define optimizer and scheduler\n","from torch.optim import Adam\n","import torch.optim.lr_scheduler as lr_scheduler\n","optimizer = Adam(\n","    model.parameters(),\n","    lr=1e-3, # 학습률\n","    eps=1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값\n",")\n","scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[20, 50], gamma=0.1 )\n","\n","\n","#재현을 위해 랜덤시드 고정\n","seed_val = 42\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","os.environ['PYTHONHASHSEED'] = str(seed_val)\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n","\n","device = torch.device(\"cuda\")\n","\n","def flat_accuracy(probs, labels):\n","    pred_flat = np.argmax(probs, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","max_dict = dict()\n","\n","\n","for epoch_i in range(0, epochs):\n","    # 로스 초기화\n","    total_loss = 0\n","    train_accuracy = 0.0\n","    model.train()\n","    nb_train_steps = 0\n","    for step, batch in enumerate(train_dataloader):\n","        batch = tuple(t.to(device) for t in batch)\n","        x1, x2, label = batch\n","        logits = model(x1, x2, label)\n","        probs = F.softmax(logits, dim=1)\n","        ## forward\n","        loss = ce_loss(probs, label)\n","        # Backward 수행으로 그래디언트 계산\n","        loss.backward()\n","        # 그래디언트 클리핑\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        optimizer.step()\n","        # 그래디언트 초기화\n","        model.zero_grad()\n","        # 총 로스 계산\n","        total_loss += loss.item()\n","        # 정확도 계산\n","        probs = probs.detach().cpu().numpy()\n","        label = label.to('cpu').numpy()\n","        tmp_train_accuracy = flat_accuracy(probs, label)\n","        train_accuracy += tmp_train_accuracy\n","        nb_train_steps += 1\n","    # 평균 로스 계산\n","    traccu= train_accuracy/nb_train_steps\n","    avg_train_loss = total_loss / len(train_dataloader)\n","    writer.add_scalar('train/loss', avg_train_loss, epoch_i)\n","    # 평가모드로 변경\n","    model.eval()\n","\n","    # 변수 초기화\n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","\n","    # 데이터로더에서 배치만큼 반복하여 가져옴\n","    for batch in validation_dataloader:\n","        \n","        # 배치를 GPU에 넣음\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # 배치에서 데이터 추출\n","        x1, x2, label = batch\n","        \n","        # 그래디언트 계산 안함\n","        with torch.no_grad():     \n","            # Forward 수행\n","            logits = model(x1, x2, label)\n","            # label이 입력으로 들어오면 loss도 계산해서 return\n","            probs = F.softmax(logits, dim=1)\n","            # 로스 구함\n","            loss = ce_loss(probs, label)\n","\n","            # CPU로 데이터 이동\n","            probs = probs.detach().cpu().numpy()\n","            label = label.to('cpu').numpy()\n","\n","            # 출력 로짓과 라벨을 비교하여 정확도 계산\n","            tmp_eval_accuracy = flat_accuracy(probs, label)\n","            eval_accuracy += tmp_eval_accuracy\n","            nb_eval_steps += 1\n","    lrnrate = str(optimizer.param_groups[0]['lr'])\n","    test_set_accuracy = eval_accuracy/nb_eval_steps\n","    max_dict[epoch_i]=test_set_accuracy\n","    s = f'\\r[Epoch {epoch_i+1}/{epochs}]'\n","    s += \" Train Acc: {0:.2f}\".format(train_accuracy/nb_train_steps)\n","    s += f' Avg Training Loss: {avg_train_loss:.2f}'\n","    s += \" Valid Acc: {0:.2f}\".format(test_set_accuracy)\n","    s += \" learning rate: \"+lrnrate\n","    print(s, end='')\n","    scheduler.step()\n","\n","    writer.add_scalar('train/score', traccu, epoch_i)\n","    writer.add_scalar('val/score', test_set_accuracy, epoch_i)\n","\n","\n","    torch.save(model.state_dict(), os.path.join(\"./modelsaves_1/epoch-{a:d}-{p:.3f}.pt\".format(a=epoch_i,p=test_set_accuracy)))\n","#     torch.save(model, os.path.join(\"./modelsaves_1/epoch_model-{a:d}-{p:.3f}.pt\".format(a=epoch_i,p=test_set_accuracy)))\n","max_val = max(max_dict.values())\n","def maxkey():\n","    max_val = max(max_dict.values())\n","    max_index = list(max_dict.values()).index(max_val)\n","    # keys = filter(lambda x: max_dict[x] == max_val, max_dict.keys())\n","    return max_index\n","\n","\n","print(\"\")\n","print(\"Training complete! max accuracy: \", max_val, maxkey() )\n","\n","\n","logname=str(time)+\".noise.\"+str(noise)+\".pseudo.\"+str(is_pseudo_ex)+\".max_accu.\"+str(format(max_val, '.3f')+\".txt\")\n","print(logname)\n","with open(os.path.join(logpath, logname), 'w') as fp:\n","    pass\n","\n","maxepoch=str(maxkey())\n","max_val=float(\"{:.3f}\".format(max_val))\n","maxval=str(max_val)\n","maxepochpt=\"./modelsaves_1/epoch-\"+maxepoch+\"-\"+maxval+\".pt\"\n","\n","print(maxepochpt)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"drRpb3eKyLxV"},"source":["Inference 용 data 불러오는 함수"]},{"cell_type":"code","metadata":{"id":"cotYvMhlyLxX"},"source":["class reshape_input_inf():\n","    def __init__(self,input,noise):\n","        self.input = input\n","        self.noise = noise\n","    def merge(self, list1, list2):\n","        merged_list = [(list1[i], list2[i]) for i in range(0, len(list1))]\n","        return merged_list\n","    def  reshape_x_inf(self):\n","        inputx=np.concatenate(\n","            [\n","                pd.get_dummies(self.input.letter).values.reshape(-1, 1, 26),\n","                (self.input[[str(j) for j in range(784)]]).values.reshape(-1, 1, 784)\n","            ],\n","            axis=2\n","        )\n","\n","        X_image=np.array(inputx[:, :, 26:], dtype=\"uint8\")\n","        X_image = X_image.squeeze()\n","\n","        appendimg = []\n","        for index in range(0, len(X_image)):\n","            img = X_image[index]\n","        # print(img.shape)\n","            img = np.where(img < self.noise, 0, img)\n","            img = img.reshape(28, 28)\n","            appendimg.append(img)\n","        appended = np.asarray(appendimg, dtype=\"float\")\n","        inputxnum = appended/255.\n","\n","        X_test = torch.Tensor(inputxnum)\n","        X_testl = torch.Tensor(inputx)\n","\n","\n","        return X_test, X_testl\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WcIojeluyLxY"},"source":["앞서 진행한 epoch 의 maximum accuracy가 저장된 model을 current model로 사용하여 inference 진행:\n","\n","Psuedo를 만들때도 사용한 code로 submission시 image 정보 제거해서 제출."]},{"cell_type":"code","metadata":{"id":"jT9KLzZOyLxY"},"source":["#same parmeter as best trained model\n","batch_size = 32\n","noise = 10\n","current_model = './modelsaves_1/epoch-76-0.933.pt' #예시\n","\n","test  = pd.read_csv('../data/test.csv')\n","submission = pd.read_csv('../data/submission.csv')\n","test.set_index('id')\n","\n","from torch.utils.data import (\n","    TensorDataset,\n","    DataLoader,\n","    RandomSampler,\n","    SequentialSampler\n",")\n","\n","modelinf = ConvClassifier()\n","modelinf.cuda()\n","modelinf.eval()\n","reshape_test_class = reshape_input_inf(test, noise)\n","X_test, X_testl = reshape_test_class.reshape_x_inf()\n","\n","test_data = TensorDataset(\n","    X_testl[:, :, :26].cuda(),  # Letter\n","    X_test.reshape(-1, 1, 28, 28).cuda()\n",")\n","test_sampler = SequentialSampler(test_data)\n","test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n","torch.load(current_model)\n","y_pred = []\n","y_raw_pred = []\n","modelinf.load_state_dict(torch.load(current_model))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XnVAvtPVyLxY"},"source":["#submission 혹은 pseudo data 만들때 같이 사용.\n","for letter, number in test_dataloader:\n","    with torch.no_grad():\n","        outputss = modelinf(letter, number)\n","        outputs = F.softmax(outputss, dim=1)\n","        y_raw_pred.append(outputs.cpu().detach().numpy())\n","    y_pred.append(torch.argmax(outputs, dim=1))\n","y_raw_pred = np.concatenate(y_raw_pred, axis=0)\n","pred_columns = []\n","for i in range(10):\n","   pred_columns.append(\"p\"+str(i))\n","\n","submission.digit = torch.cat(y_pred).detach().cpu().numpy()\n","submission2 = pd.DataFrame(data=y_raw_pred,columns=pred_columns)\n","submission2.insert(0,'id',value= submission.id.values)\n","submission2.insert(1,'digit',value= submission.digit.values)\n","submission2.to_csv(\"submission_\"+str(noise)+\"_\"+\".csv\", index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V4xqHN-VyLxY"},"source":["Baselne code에서 차이점은 noise제거 와 pseudo labelling 입니다.\n","\n","Noise들은 image를 숫자로 보면 쉽게 볼 수 있었는데 제거시 도움이 많이 되었습니다.\n","\n","이후 psuedo 가지고 test를 많이 했습니다. Probability를 정보를 받아와서 여러가지 cutoff로 test를 진행하기도 하였고,\n","\n","import 한 psuedo data를 전부 사용하지 않고 random 하기 일부를 제거하여 test를 진행했습니다.\n","\n","최종 test 중 psuedo file에서 0.1%를 제거했을때 valid accuracy가 가장 올라갔습니다."]}]}