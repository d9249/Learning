{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BatchSize_08_3_DenseNet121(public-, private-).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPVKJab59vUaWOMsz95CNSK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/d9249/DACON/blob/main/BatchSize_08_3_DenseNet121(public-%2C%20private-).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMLx8uC2eHeP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12edc1df-a25c-48a3-c2af-af00783a8d4b"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Sep  4 08:30:36 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmEaPJckuX-D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ffab71c-4eee-4f3b-ceec-ec2a9cab6802"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88GAtllsufPj"
      },
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv('/content/drive/MyDrive/DACON_CVLC/data/train.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/DACON_CVLC/data/test.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qBWziyZrqBo"
      },
      "source": [
        "!mkdir images_train\n",
        "!mkdir images_train/0\n",
        "!mkdir images_train/1\n",
        "!mkdir images_train/2\n",
        "!mkdir images_train/3\n",
        "!mkdir images_train/4\n",
        "!mkdir images_train/5\n",
        "!mkdir images_train/6\n",
        "!mkdir images_train/7\n",
        "!mkdir images_train/8\n",
        "!mkdir images_train/9\n",
        "!mkdir images_test"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fjN8mIDrazg"
      },
      "source": [
        "import cv2\n",
        "\n",
        "for idx in range(len(train)) :\n",
        "    img = train.loc[idx, '0':].values.reshape(28, 28).astype(int)\n",
        "    digit = train.loc[idx, 'digit']\n",
        "    cv2.imwrite(f'./images_train/{digit}/{train[\"id\"][idx]}.png', img)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4P9AD1gyotc"
      },
      "source": [
        "import cv2\n",
        "\n",
        "for idx in range(len(test)) :\n",
        "    img = test.loc[idx, '0':].values.reshape(28, 28).astype(int)\n",
        "    cv2.imwrite(f'./images_test/{test[\"id\"][idx]}.png', img)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUJTlJ6GxNmK"
      },
      "source": [
        "import tensorflow as tf\n",
        "DenseNet121_model = tf.keras.applications.DenseNet121(weights=None, include_top=True, input_shape=(224, 224, 1), classes=10)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlVMd30ZxUMQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45a77881-a70b-4bfc-aef3-e1757ba6cc68"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "DenseNet121_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.002,epsilon=None), metrics=['accuracy'])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1haI0Zjxa74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c66cfe8b-e0fc-49b9-d167-90cf397f1e9b"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "                             rescale=1./255, \n",
        "                             validation_split=0.2,\n",
        "                             rotation_range=10,\n",
        "                             width_shift_range=0.1,\n",
        "                             height_shift_range=0.1)\n",
        "\n",
        "batch_size = 8\n",
        "train_generator = datagen.flow_from_directory('./images_train', target_size=(224,224), batch_size = batch_size, color_mode='grayscale', class_mode='categorical', subset='training')\n",
        "val_generator = datagen.flow_from_directory('./images_train', target_size=(224,224), batch_size = batch_size,  color_mode='grayscale', class_mode='categorical', subset='validation')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1642 images belonging to 10 classes.\n",
            "Found 406 images belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRP2R9hdxsyY"
      },
      "source": [
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(f'/content/drive/MyDrive/DACON_CVLC/Checkpoint/BS_08_3_DN121.h5', monitor='val_accuracy', save_best_only=True, verbose=1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKMJhbFnxotA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c18f165-0147-4234-c4a8-f312bd757b7b"
      },
      "source": [
        "DenseNet121_model.fit_generator(train_generator, epochs = 500, validation_data = val_generator, callbacks = [checkpoint])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "206/206 [==============================] - 58s 148ms/step - loss: 2.1471 - accuracy: 0.2527 - val_loss: 5.7676 - val_accuracy: 0.1700\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.16995, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/BS_08_3_DN121.h5\n",
            "Epoch 2/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 1.5094 - accuracy: 0.4854 - val_loss: 3.1022 - val_accuracy: 0.2389\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.16995 to 0.23892, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/BS_08_3_DN121.h5\n",
            "Epoch 3/500\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 1.2305 - accuracy: 0.5883 - val_loss: 3.9094 - val_accuracy: 0.3448\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.23892 to 0.34483, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/BS_08_3_DN121.h5\n",
            "Epoch 4/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 1.1096 - accuracy: 0.6230 - val_loss: 1.2227 - val_accuracy: 0.6453\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.34483 to 0.64532, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/BS_08_3_DN121.h5\n",
            "Epoch 5/500\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.9587 - accuracy: 0.6821 - val_loss: 3.3377 - val_accuracy: 0.3941\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.64532\n",
            "Epoch 6/500\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.9107 - accuracy: 0.6961 - val_loss: 1.1648 - val_accuracy: 0.6946\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.64532 to 0.69458, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/BS_08_3_DN121.h5\n",
            "Epoch 7/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.8079 - accuracy: 0.7320 - val_loss: 0.9172 - val_accuracy: 0.7291\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.69458 to 0.72906, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/BS_08_3_DN121.h5\n",
            "Epoch 8/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.7341 - accuracy: 0.7631 - val_loss: 1.8241 - val_accuracy: 0.5320\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.72906\n",
            "Epoch 9/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.6654 - accuracy: 0.7722 - val_loss: 1.2126 - val_accuracy: 0.6798\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.72906\n",
            "Epoch 10/500\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.6211 - accuracy: 0.7875 - val_loss: 0.9757 - val_accuracy: 0.7094\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.72906\n",
            "Epoch 11/500\n",
            "206/206 [==============================] - 26s 124ms/step - loss: 0.5879 - accuracy: 0.8069 - val_loss: 0.5646 - val_accuracy: 0.8030\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.72906 to 0.80296, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/BS_08_3_DN121.h5\n",
            "Epoch 12/500\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.5514 - accuracy: 0.8100 - val_loss: 1.0679 - val_accuracy: 0.7044\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.80296\n",
            "Epoch 13/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.5182 - accuracy: 0.8350 - val_loss: 0.9274 - val_accuracy: 0.7094\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.80296\n",
            "Epoch 14/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.4725 - accuracy: 0.8404 - val_loss: 0.5851 - val_accuracy: 0.8424\n",
            "\n",
            "Epoch 00014: val_accuracy improved from 0.80296 to 0.84236, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/BS_08_3_DN121.h5\n",
            "Epoch 15/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.4362 - accuracy: 0.8599 - val_loss: 0.5301 - val_accuracy: 0.8054\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.84236\n",
            "Epoch 16/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.4481 - accuracy: 0.8544 - val_loss: 0.6283 - val_accuracy: 0.8128\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.84236\n",
            "Epoch 17/500\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.3995 - accuracy: 0.8691 - val_loss: 0.5190 - val_accuracy: 0.8424\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.84236\n",
            "Epoch 18/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.3794 - accuracy: 0.8733 - val_loss: 0.5172 - val_accuracy: 0.8448\n",
            "\n",
            "Epoch 00018: val_accuracy improved from 0.84236 to 0.84483, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/BS_08_3_DN121.h5\n",
            "Epoch 19/500\n",
            "206/206 [==============================] - 27s 128ms/step - loss: 0.3587 - accuracy: 0.8861 - val_loss: 0.7324 - val_accuracy: 0.7931\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.84483\n",
            "Epoch 20/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.3863 - accuracy: 0.8739 - val_loss: 0.4688 - val_accuracy: 0.8448\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.84483\n",
            "Epoch 21/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.3507 - accuracy: 0.8752 - val_loss: 0.5123 - val_accuracy: 0.8276\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.84483\n",
            "Epoch 22/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.3201 - accuracy: 0.9013 - val_loss: 0.4241 - val_accuracy: 0.8621\n",
            "\n",
            "Epoch 00022: val_accuracy improved from 0.84483 to 0.86207, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/BS_08_3_DN121.h5\n",
            "Epoch 23/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.2768 - accuracy: 0.9019 - val_loss: 0.5809 - val_accuracy: 0.8374\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.86207\n",
            "Epoch 24/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.3170 - accuracy: 0.8971 - val_loss: 0.5315 - val_accuracy: 0.8374\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.86207\n",
            "Epoch 25/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.2625 - accuracy: 0.9099 - val_loss: 0.4957 - val_accuracy: 0.8448\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.86207\n",
            "Epoch 26/500\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.2836 - accuracy: 0.9044 - val_loss: 0.6698 - val_accuracy: 0.8202\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.86207\n",
            "Epoch 27/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.2618 - accuracy: 0.9147 - val_loss: 0.3843 - val_accuracy: 0.8768\n",
            "\n",
            "Epoch 00027: val_accuracy improved from 0.86207 to 0.87685, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/BS_08_3_DN121.h5\n",
            "Epoch 28/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.2506 - accuracy: 0.9214 - val_loss: 0.3786 - val_accuracy: 0.8793\n",
            "\n",
            "Epoch 00028: val_accuracy improved from 0.87685 to 0.87931, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/BS_08_3_DN121.h5\n",
            "Epoch 29/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.2496 - accuracy: 0.9172 - val_loss: 0.4594 - val_accuracy: 0.8374\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.87931\n",
            "Epoch 30/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.2570 - accuracy: 0.9160 - val_loss: 0.6125 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00030: val_accuracy did not improve from 0.87931\n",
            "Epoch 31/500\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.2365 - accuracy: 0.9239 - val_loss: 0.4067 - val_accuracy: 0.8695\n",
            "\n",
            "Epoch 00031: val_accuracy did not improve from 0.87931\n",
            "Epoch 32/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.1902 - accuracy: 0.9379 - val_loss: 0.3940 - val_accuracy: 0.8892\n",
            "\n",
            "Epoch 00032: val_accuracy improved from 0.87931 to 0.88916, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/BS_08_3_DN121.h5\n",
            "Epoch 33/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.1894 - accuracy: 0.9409 - val_loss: 0.5964 - val_accuracy: 0.8374\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.88916\n",
            "Epoch 34/500\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.1783 - accuracy: 0.9361 - val_loss: 0.4574 - val_accuracy: 0.8424\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.88916\n",
            "Epoch 35/500\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.2124 - accuracy: 0.9330 - val_loss: 0.5282 - val_accuracy: 0.8621\n",
            "\n",
            "Epoch 00035: val_accuracy did not improve from 0.88916\n",
            "Epoch 36/500\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.1960 - accuracy: 0.9324 - val_loss: 0.4824 - val_accuracy: 0.8473\n",
            "\n",
            "Epoch 00036: val_accuracy did not improve from 0.88916\n",
            "Epoch 37/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.2307 - accuracy: 0.9233 - val_loss: 0.4119 - val_accuracy: 0.8695\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.88916\n",
            "Epoch 38/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.1519 - accuracy: 0.9452 - val_loss: 0.5035 - val_accuracy: 0.8793\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.88916\n",
            "Epoch 39/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.1587 - accuracy: 0.9415 - val_loss: 0.4042 - val_accuracy: 0.8916\n",
            "\n",
            "Epoch 00039: val_accuracy improved from 0.88916 to 0.89163, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/BS_08_3_DN121.h5\n",
            "Epoch 40/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.1801 - accuracy: 0.9373 - val_loss: 0.6544 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00040: val_accuracy did not improve from 0.89163\n",
            "Epoch 41/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.1513 - accuracy: 0.9440 - val_loss: 0.4972 - val_accuracy: 0.8621\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.89163\n",
            "Epoch 42/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.1443 - accuracy: 0.9507 - val_loss: 0.5386 - val_accuracy: 0.8547\n",
            "\n",
            "Epoch 00042: val_accuracy did not improve from 0.89163\n",
            "Epoch 43/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.1447 - accuracy: 0.9501 - val_loss: 0.4640 - val_accuracy: 0.8695\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.89163\n",
            "Epoch 44/500\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.1294 - accuracy: 0.9507 - val_loss: 0.4014 - val_accuracy: 0.8768\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.89163\n",
            "Epoch 45/500\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.1360 - accuracy: 0.9519 - val_loss: 5.3123 - val_accuracy: 0.1872\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.89163\n",
            "Epoch 46/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.1055 - accuracy: 0.9641 - val_loss: 0.4129 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00046: val_accuracy improved from 0.89163 to 0.89655, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/BS_08_3_DN121.h5\n",
            "Epoch 47/500\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.1247 - accuracy: 0.9549 - val_loss: 0.5109 - val_accuracy: 0.8645\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.89655\n",
            "Epoch 48/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.1331 - accuracy: 0.9525 - val_loss: 0.3957 - val_accuracy: 0.8744\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.89655\n",
            "Epoch 49/500\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.1061 - accuracy: 0.9622 - val_loss: 0.4292 - val_accuracy: 0.8941\n",
            "\n",
            "Epoch 00049: val_accuracy did not improve from 0.89655\n",
            "Epoch 50/500\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.1274 - accuracy: 0.9568 - val_loss: 0.3477 - val_accuracy: 0.8916\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.89655\n",
            "Epoch 51/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.1121 - accuracy: 0.9665 - val_loss: 0.3965 - val_accuracy: 0.8916\n",
            "\n",
            "Epoch 00051: val_accuracy did not improve from 0.89655\n",
            "Epoch 52/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0926 - accuracy: 0.9665 - val_loss: 0.6246 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00052: val_accuracy did not improve from 0.89655\n",
            "Epoch 53/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.1083 - accuracy: 0.9586 - val_loss: 0.6525 - val_accuracy: 0.8621\n",
            "\n",
            "Epoch 00053: val_accuracy did not improve from 0.89655\n",
            "Epoch 54/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.1476 - accuracy: 0.9513 - val_loss: 0.4638 - val_accuracy: 0.8695\n",
            "\n",
            "Epoch 00054: val_accuracy did not improve from 0.89655\n",
            "Epoch 55/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.1370 - accuracy: 0.9543 - val_loss: 0.4930 - val_accuracy: 0.8768\n",
            "\n",
            "Epoch 00055: val_accuracy did not improve from 0.89655\n",
            "Epoch 56/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.1009 - accuracy: 0.9635 - val_loss: 0.4327 - val_accuracy: 0.9039\n",
            "\n",
            "Epoch 00056: val_accuracy improved from 0.89655 to 0.90394, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/BS_08_3_DN121.h5\n",
            "Epoch 57/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0881 - accuracy: 0.9702 - val_loss: 0.8556 - val_accuracy: 0.8276\n",
            "\n",
            "Epoch 00057: val_accuracy did not improve from 0.90394\n",
            "Epoch 58/500\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.0943 - accuracy: 0.9671 - val_loss: 0.5215 - val_accuracy: 0.8768\n",
            "\n",
            "Epoch 00058: val_accuracy did not improve from 0.90394\n",
            "Epoch 59/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.1019 - accuracy: 0.9689 - val_loss: 0.5396 - val_accuracy: 0.8744\n",
            "\n",
            "Epoch 00059: val_accuracy did not improve from 0.90394\n",
            "Epoch 60/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.1140 - accuracy: 0.9641 - val_loss: 0.4647 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00060: val_accuracy did not improve from 0.90394\n",
            "Epoch 61/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.1151 - accuracy: 0.9616 - val_loss: 0.4774 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00061: val_accuracy did not improve from 0.90394\n",
            "Epoch 62/500\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.1308 - accuracy: 0.9622 - val_loss: 0.4930 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00062: val_accuracy did not improve from 0.90394\n",
            "Epoch 63/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0919 - accuracy: 0.9647 - val_loss: 0.5546 - val_accuracy: 0.8818\n",
            "\n",
            "Epoch 00063: val_accuracy did not improve from 0.90394\n",
            "Epoch 64/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0901 - accuracy: 0.9683 - val_loss: 0.3953 - val_accuracy: 0.8892\n",
            "\n",
            "Epoch 00064: val_accuracy did not improve from 0.90394\n",
            "Epoch 65/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0606 - accuracy: 0.9823 - val_loss: 0.4874 - val_accuracy: 0.8719\n",
            "\n",
            "Epoch 00065: val_accuracy did not improve from 0.90394\n",
            "Epoch 66/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.1217 - accuracy: 0.9586 - val_loss: 0.7364 - val_accuracy: 0.8399\n",
            "\n",
            "Epoch 00066: val_accuracy did not improve from 0.90394\n",
            "Epoch 67/500\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.1416 - accuracy: 0.9525 - val_loss: 0.3901 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00067: val_accuracy improved from 0.90394 to 0.90887, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/BS_08_3_DN121.h5\n",
            "Epoch 68/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0560 - accuracy: 0.9842 - val_loss: 0.3478 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00068: val_accuracy did not improve from 0.90887\n",
            "Epoch 69/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0635 - accuracy: 0.9817 - val_loss: 0.4350 - val_accuracy: 0.8768\n",
            "\n",
            "Epoch 00069: val_accuracy did not improve from 0.90887\n",
            "Epoch 70/500\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.0484 - accuracy: 0.9854 - val_loss: 0.4047 - val_accuracy: 0.8793\n",
            "\n",
            "Epoch 00070: val_accuracy did not improve from 0.90887\n",
            "Epoch 71/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0567 - accuracy: 0.9799 - val_loss: 0.4387 - val_accuracy: 0.8941\n",
            "\n",
            "Epoch 00071: val_accuracy did not improve from 0.90887\n",
            "Epoch 72/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0560 - accuracy: 0.9823 - val_loss: 0.4586 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00072: val_accuracy did not improve from 0.90887\n",
            "Epoch 73/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0674 - accuracy: 0.9787 - val_loss: 0.4289 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00073: val_accuracy did not improve from 0.90887\n",
            "Epoch 74/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0754 - accuracy: 0.9744 - val_loss: 0.5255 - val_accuracy: 0.8695\n",
            "\n",
            "Epoch 00074: val_accuracy did not improve from 0.90887\n",
            "Epoch 75/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0566 - accuracy: 0.9829 - val_loss: 0.6815 - val_accuracy: 0.8768\n",
            "\n",
            "Epoch 00075: val_accuracy did not improve from 0.90887\n",
            "Epoch 76/500\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.1298 - accuracy: 0.9635 - val_loss: 0.3685 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00076: val_accuracy did not improve from 0.90887\n",
            "Epoch 77/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0803 - accuracy: 0.9738 - val_loss: 0.6494 - val_accuracy: 0.8424\n",
            "\n",
            "Epoch 00077: val_accuracy did not improve from 0.90887\n",
            "Epoch 78/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0968 - accuracy: 0.9714 - val_loss: 0.4474 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00078: val_accuracy did not improve from 0.90887\n",
            "Epoch 79/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0454 - accuracy: 0.9884 - val_loss: 0.4848 - val_accuracy: 0.8695\n",
            "\n",
            "Epoch 00079: val_accuracy did not improve from 0.90887\n",
            "Epoch 80/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0620 - accuracy: 0.9817 - val_loss: 0.4791 - val_accuracy: 0.8818\n",
            "\n",
            "Epoch 00080: val_accuracy did not improve from 0.90887\n",
            "Epoch 81/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0686 - accuracy: 0.9787 - val_loss: 0.7955 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00081: val_accuracy did not improve from 0.90887\n",
            "Epoch 82/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0582 - accuracy: 0.9793 - val_loss: 0.4518 - val_accuracy: 0.8941\n",
            "\n",
            "Epoch 00082: val_accuracy did not improve from 0.90887\n",
            "Epoch 83/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0529 - accuracy: 0.9842 - val_loss: 0.3808 - val_accuracy: 0.9064\n",
            "\n",
            "Epoch 00083: val_accuracy did not improve from 0.90887\n",
            "Epoch 84/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0599 - accuracy: 0.9848 - val_loss: 0.3707 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00084: val_accuracy did not improve from 0.90887\n",
            "Epoch 85/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0596 - accuracy: 0.9823 - val_loss: 0.4500 - val_accuracy: 0.9039\n",
            "\n",
            "Epoch 00085: val_accuracy did not improve from 0.90887\n",
            "Epoch 86/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0562 - accuracy: 0.9836 - val_loss: 0.3364 - val_accuracy: 0.9113\n",
            "\n",
            "Epoch 00086: val_accuracy improved from 0.90887 to 0.91133, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/BS_08_3_DN121.h5\n",
            "Epoch 87/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0389 - accuracy: 0.9860 - val_loss: 0.3764 - val_accuracy: 0.9039\n",
            "\n",
            "Epoch 00087: val_accuracy did not improve from 0.91133\n",
            "Epoch 88/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0857 - accuracy: 0.9762 - val_loss: 0.5043 - val_accuracy: 0.8793\n",
            "\n",
            "Epoch 00088: val_accuracy did not improve from 0.91133\n",
            "Epoch 89/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0695 - accuracy: 0.9756 - val_loss: 0.4450 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00089: val_accuracy did not improve from 0.91133\n",
            "Epoch 90/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0342 - accuracy: 0.9878 - val_loss: 0.5096 - val_accuracy: 0.8793\n",
            "\n",
            "Epoch 00090: val_accuracy did not improve from 0.91133\n",
            "Epoch 91/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0835 - accuracy: 0.9744 - val_loss: 0.7330 - val_accuracy: 0.8251\n",
            "\n",
            "Epoch 00091: val_accuracy did not improve from 0.91133\n",
            "Epoch 92/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0521 - accuracy: 0.9817 - val_loss: 0.3771 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00092: val_accuracy did not improve from 0.91133\n",
            "Epoch 93/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0481 - accuracy: 0.9823 - val_loss: 0.5129 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00093: val_accuracy did not improve from 0.91133\n",
            "Epoch 94/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0335 - accuracy: 0.9927 - val_loss: 0.3077 - val_accuracy: 0.9212\n",
            "\n",
            "Epoch 00094: val_accuracy improved from 0.91133 to 0.92118, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/BS_08_3_DN121.h5\n",
            "Epoch 95/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0597 - accuracy: 0.9829 - val_loss: 0.4426 - val_accuracy: 0.8892\n",
            "\n",
            "Epoch 00095: val_accuracy did not improve from 0.92118\n",
            "Epoch 96/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0286 - accuracy: 0.9921 - val_loss: 0.4238 - val_accuracy: 0.8916\n",
            "\n",
            "Epoch 00096: val_accuracy did not improve from 0.92118\n",
            "Epoch 97/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0618 - accuracy: 0.9842 - val_loss: 0.3500 - val_accuracy: 0.9138\n",
            "\n",
            "Epoch 00097: val_accuracy did not improve from 0.92118\n",
            "Epoch 98/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0468 - accuracy: 0.9872 - val_loss: 0.6812 - val_accuracy: 0.8473\n",
            "\n",
            "Epoch 00098: val_accuracy did not improve from 0.92118\n",
            "Epoch 99/500\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.0454 - accuracy: 0.9890 - val_loss: 0.5375 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00099: val_accuracy did not improve from 0.92118\n",
            "Epoch 100/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0605 - accuracy: 0.9811 - val_loss: 0.6212 - val_accuracy: 0.8744\n",
            "\n",
            "Epoch 00100: val_accuracy did not improve from 0.92118\n",
            "Epoch 101/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0982 - accuracy: 0.9714 - val_loss: 0.5632 - val_accuracy: 0.8818\n",
            "\n",
            "Epoch 00101: val_accuracy did not improve from 0.92118\n",
            "Epoch 102/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0362 - accuracy: 0.9884 - val_loss: 0.4196 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00102: val_accuracy did not improve from 0.92118\n",
            "Epoch 103/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0191 - accuracy: 0.9951 - val_loss: 0.4053 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00103: val_accuracy did not improve from 0.92118\n",
            "Epoch 104/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0242 - accuracy: 0.9939 - val_loss: 0.3650 - val_accuracy: 0.9212\n",
            "\n",
            "Epoch 00104: val_accuracy did not improve from 0.92118\n",
            "Epoch 105/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0511 - accuracy: 0.9842 - val_loss: 0.4256 - val_accuracy: 0.8916\n",
            "\n",
            "Epoch 00105: val_accuracy did not improve from 0.92118\n",
            "Epoch 106/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0880 - accuracy: 0.9799 - val_loss: 0.3516 - val_accuracy: 0.9138\n",
            "\n",
            "Epoch 00106: val_accuracy did not improve from 0.92118\n",
            "Epoch 107/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0396 - accuracy: 0.9866 - val_loss: 0.3247 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00107: val_accuracy did not improve from 0.92118\n",
            "Epoch 108/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0647 - accuracy: 0.9805 - val_loss: 0.5132 - val_accuracy: 0.8916\n",
            "\n",
            "Epoch 00108: val_accuracy did not improve from 0.92118\n",
            "Epoch 109/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0293 - accuracy: 0.9866 - val_loss: 0.4538 - val_accuracy: 0.9039\n",
            "\n",
            "Epoch 00109: val_accuracy did not improve from 0.92118\n",
            "Epoch 110/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0341 - accuracy: 0.9896 - val_loss: 0.4291 - val_accuracy: 0.8941\n",
            "\n",
            "Epoch 00110: val_accuracy did not improve from 0.92118\n",
            "Epoch 111/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0256 - accuracy: 0.9915 - val_loss: 0.4066 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00111: val_accuracy did not improve from 0.92118\n",
            "Epoch 112/500\n",
            "206/206 [==============================] - 26s 125ms/step - loss: 0.0486 - accuracy: 0.9842 - val_loss: 0.4329 - val_accuracy: 0.9039\n",
            "\n",
            "Epoch 00112: val_accuracy did not improve from 0.92118\n",
            "Epoch 113/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0176 - accuracy: 0.9957 - val_loss: 0.3752 - val_accuracy: 0.9039\n",
            "\n",
            "Epoch 00113: val_accuracy did not improve from 0.92118\n",
            "Epoch 114/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0510 - accuracy: 0.9848 - val_loss: 0.5826 - val_accuracy: 0.8744\n",
            "\n",
            "Epoch 00114: val_accuracy did not improve from 0.92118\n",
            "Epoch 115/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0412 - accuracy: 0.9866 - val_loss: 0.4682 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00115: val_accuracy did not improve from 0.92118\n",
            "Epoch 116/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0338 - accuracy: 0.9872 - val_loss: 0.7213 - val_accuracy: 0.8448\n",
            "\n",
            "Epoch 00116: val_accuracy did not improve from 0.92118\n",
            "Epoch 117/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0278 - accuracy: 0.9927 - val_loss: 0.4638 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00117: val_accuracy did not improve from 0.92118\n",
            "Epoch 118/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0168 - accuracy: 0.9963 - val_loss: 0.3703 - val_accuracy: 0.9187\n",
            "\n",
            "Epoch 00118: val_accuracy did not improve from 0.92118\n",
            "Epoch 119/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0207 - accuracy: 0.9939 - val_loss: 0.5680 - val_accuracy: 0.8719\n",
            "\n",
            "Epoch 00119: val_accuracy did not improve from 0.92118\n",
            "Epoch 120/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0337 - accuracy: 0.9878 - val_loss: 0.6940 - val_accuracy: 0.8547\n",
            "\n",
            "Epoch 00120: val_accuracy did not improve from 0.92118\n",
            "Epoch 121/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0611 - accuracy: 0.9799 - val_loss: 0.5848 - val_accuracy: 0.9039\n",
            "\n",
            "Epoch 00121: val_accuracy did not improve from 0.92118\n",
            "Epoch 122/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0768 - accuracy: 0.9762 - val_loss: 0.6204 - val_accuracy: 0.8768\n",
            "\n",
            "Epoch 00122: val_accuracy did not improve from 0.92118\n",
            "Epoch 123/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0838 - accuracy: 0.9695 - val_loss: 0.5521 - val_accuracy: 0.8793\n",
            "\n",
            "Epoch 00123: val_accuracy did not improve from 0.92118\n",
            "Epoch 124/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0255 - accuracy: 0.9921 - val_loss: 0.4345 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00124: val_accuracy did not improve from 0.92118\n",
            "Epoch 125/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0260 - accuracy: 0.9921 - val_loss: 0.4810 - val_accuracy: 0.8892\n",
            "\n",
            "Epoch 00125: val_accuracy did not improve from 0.92118\n",
            "Epoch 126/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0217 - accuracy: 0.9945 - val_loss: 0.4291 - val_accuracy: 0.9113\n",
            "\n",
            "Epoch 00126: val_accuracy did not improve from 0.92118\n",
            "Epoch 127/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0326 - accuracy: 0.9890 - val_loss: 0.3720 - val_accuracy: 0.9138\n",
            "\n",
            "Epoch 00127: val_accuracy did not improve from 0.92118\n",
            "Epoch 128/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0179 - accuracy: 0.9933 - val_loss: 0.5333 - val_accuracy: 0.8596\n",
            "\n",
            "Epoch 00128: val_accuracy did not improve from 0.92118\n",
            "Epoch 129/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0852 - accuracy: 0.9708 - val_loss: 0.4167 - val_accuracy: 0.9163\n",
            "\n",
            "Epoch 00129: val_accuracy did not improve from 0.92118\n",
            "Epoch 130/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0235 - accuracy: 0.9933 - val_loss: 0.4750 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00130: val_accuracy did not improve from 0.92118\n",
            "Epoch 131/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0072 - accuracy: 0.9988 - val_loss: 0.4561 - val_accuracy: 0.8768\n",
            "\n",
            "Epoch 00131: val_accuracy did not improve from 0.92118\n",
            "Epoch 132/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0163 - accuracy: 0.9957 - val_loss: 0.4455 - val_accuracy: 0.8941\n",
            "\n",
            "Epoch 00132: val_accuracy did not improve from 0.92118\n",
            "Epoch 133/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0313 - accuracy: 0.9884 - val_loss: 0.4620 - val_accuracy: 0.8892\n",
            "\n",
            "Epoch 00133: val_accuracy did not improve from 0.92118\n",
            "Epoch 134/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0479 - accuracy: 0.9854 - val_loss: 0.6386 - val_accuracy: 0.8547\n",
            "\n",
            "Epoch 00134: val_accuracy did not improve from 0.92118\n",
            "Epoch 135/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0389 - accuracy: 0.9890 - val_loss: 0.5075 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00135: val_accuracy did not improve from 0.92118\n",
            "Epoch 136/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0423 - accuracy: 0.9829 - val_loss: 0.4925 - val_accuracy: 0.8744\n",
            "\n",
            "Epoch 00136: val_accuracy did not improve from 0.92118\n",
            "Epoch 137/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0112 - accuracy: 0.9976 - val_loss: 0.5133 - val_accuracy: 0.8941\n",
            "\n",
            "Epoch 00137: val_accuracy did not improve from 0.92118\n",
            "Epoch 138/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0161 - accuracy: 0.9951 - val_loss: 0.3885 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00138: val_accuracy did not improve from 0.92118\n",
            "Epoch 139/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0300 - accuracy: 0.9896 - val_loss: 0.5274 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00139: val_accuracy did not improve from 0.92118\n",
            "Epoch 140/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0165 - accuracy: 0.9963 - val_loss: 0.4555 - val_accuracy: 0.9113\n",
            "\n",
            "Epoch 00140: val_accuracy did not improve from 0.92118\n",
            "Epoch 141/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0104 - accuracy: 0.9951 - val_loss: 0.5743 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00141: val_accuracy did not improve from 0.92118\n",
            "Epoch 142/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0314 - accuracy: 0.9890 - val_loss: 0.7358 - val_accuracy: 0.8448\n",
            "\n",
            "Epoch 00142: val_accuracy did not improve from 0.92118\n",
            "Epoch 143/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0972 - accuracy: 0.9732 - val_loss: 0.6809 - val_accuracy: 0.8399\n",
            "\n",
            "Epoch 00143: val_accuracy did not improve from 0.92118\n",
            "Epoch 144/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0425 - accuracy: 0.9860 - val_loss: 0.4745 - val_accuracy: 0.8892\n",
            "\n",
            "Epoch 00144: val_accuracy did not improve from 0.92118\n",
            "Epoch 145/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0442 - accuracy: 0.9866 - val_loss: 0.5149 - val_accuracy: 0.8941\n",
            "\n",
            "Epoch 00145: val_accuracy did not improve from 0.92118\n",
            "Epoch 146/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0321 - accuracy: 0.9915 - val_loss: 0.6131 - val_accuracy: 0.8916\n",
            "\n",
            "Epoch 00146: val_accuracy did not improve from 0.92118\n",
            "Epoch 147/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0713 - accuracy: 0.9829 - val_loss: 0.4512 - val_accuracy: 0.9039\n",
            "\n",
            "Epoch 00147: val_accuracy did not improve from 0.92118\n",
            "Epoch 148/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0123 - accuracy: 0.9970 - val_loss: 0.4339 - val_accuracy: 0.9039\n",
            "\n",
            "Epoch 00148: val_accuracy did not improve from 0.92118\n",
            "Epoch 149/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0134 - accuracy: 0.9957 - val_loss: 0.3872 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00149: val_accuracy did not improve from 0.92118\n",
            "Epoch 150/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0314 - accuracy: 0.9927 - val_loss: 0.7884 - val_accuracy: 0.8522\n",
            "\n",
            "Epoch 00150: val_accuracy did not improve from 0.92118\n",
            "Epoch 151/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0318 - accuracy: 0.9903 - val_loss: 0.5320 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00151: val_accuracy did not improve from 0.92118\n",
            "Epoch 152/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0199 - accuracy: 0.9939 - val_loss: 0.3973 - val_accuracy: 0.9236\n",
            "\n",
            "Epoch 00152: val_accuracy improved from 0.92118 to 0.92365, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/BS_08_3_DN121.h5\n",
            "Epoch 153/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0292 - accuracy: 0.9896 - val_loss: 0.4590 - val_accuracy: 0.8941\n",
            "\n",
            "Epoch 00153: val_accuracy did not improve from 0.92365\n",
            "Epoch 154/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0338 - accuracy: 0.9909 - val_loss: 0.7637 - val_accuracy: 0.8424\n",
            "\n",
            "Epoch 00154: val_accuracy did not improve from 0.92365\n",
            "Epoch 155/500\n",
            "206/206 [==============================] - 26s 126ms/step - loss: 0.0384 - accuracy: 0.9890 - val_loss: 0.4566 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00155: val_accuracy did not improve from 0.92365\n",
            "Epoch 156/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0712 - accuracy: 0.9775 - val_loss: 0.3731 - val_accuracy: 0.9064\n",
            "\n",
            "Epoch 00156: val_accuracy did not improve from 0.92365\n",
            "Epoch 157/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0269 - accuracy: 0.9909 - val_loss: 0.3677 - val_accuracy: 0.9064\n",
            "\n",
            "Epoch 00157: val_accuracy did not improve from 0.92365\n",
            "Epoch 158/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0110 - accuracy: 0.9963 - val_loss: 0.4274 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00158: val_accuracy did not improve from 0.92365\n",
            "Epoch 159/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0078 - accuracy: 0.9994 - val_loss: 0.4063 - val_accuracy: 0.9113\n",
            "\n",
            "Epoch 00159: val_accuracy did not improve from 0.92365\n",
            "Epoch 160/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0214 - accuracy: 0.9927 - val_loss: 0.4411 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00160: val_accuracy did not improve from 0.92365\n",
            "Epoch 161/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0259 - accuracy: 0.9927 - val_loss: 0.4347 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00161: val_accuracy did not improve from 0.92365\n",
            "Epoch 162/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0149 - accuracy: 0.9951 - val_loss: 1.0301 - val_accuracy: 0.7512\n",
            "\n",
            "Epoch 00162: val_accuracy did not improve from 0.92365\n",
            "Epoch 163/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0383 - accuracy: 0.9915 - val_loss: 0.6535 - val_accuracy: 0.8695\n",
            "\n",
            "Epoch 00163: val_accuracy did not improve from 0.92365\n",
            "Epoch 164/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0358 - accuracy: 0.9872 - val_loss: 0.3574 - val_accuracy: 0.9236\n",
            "\n",
            "Epoch 00164: val_accuracy did not improve from 0.92365\n",
            "Epoch 165/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0214 - accuracy: 0.9933 - val_loss: 0.3626 - val_accuracy: 0.9113\n",
            "\n",
            "Epoch 00165: val_accuracy did not improve from 0.92365\n",
            "Epoch 166/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0204 - accuracy: 0.9957 - val_loss: 0.5574 - val_accuracy: 0.8916\n",
            "\n",
            "Epoch 00166: val_accuracy did not improve from 0.92365\n",
            "Epoch 167/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0138 - accuracy: 0.9963 - val_loss: 0.4470 - val_accuracy: 0.9212\n",
            "\n",
            "Epoch 00167: val_accuracy did not improve from 0.92365\n",
            "Epoch 168/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0080 - accuracy: 0.9976 - val_loss: 0.4080 - val_accuracy: 0.9113\n",
            "\n",
            "Epoch 00168: val_accuracy did not improve from 0.92365\n",
            "Epoch 169/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0157 - accuracy: 0.9927 - val_loss: 0.5248 - val_accuracy: 0.8818\n",
            "\n",
            "Epoch 00169: val_accuracy did not improve from 0.92365\n",
            "Epoch 170/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0141 - accuracy: 0.9957 - val_loss: 0.6374 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00170: val_accuracy did not improve from 0.92365\n",
            "Epoch 171/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0364 - accuracy: 0.9884 - val_loss: 0.6910 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00171: val_accuracy did not improve from 0.92365\n",
            "Epoch 172/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0239 - accuracy: 0.9927 - val_loss: 0.5561 - val_accuracy: 0.9015\n",
            "\n",
            "Epoch 00172: val_accuracy did not improve from 0.92365\n",
            "Epoch 173/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0254 - accuracy: 0.9933 - val_loss: 0.3966 - val_accuracy: 0.9113\n",
            "\n",
            "Epoch 00173: val_accuracy did not improve from 0.92365\n",
            "Epoch 174/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0511 - accuracy: 0.9872 - val_loss: 0.7538 - val_accuracy: 0.8596\n",
            "\n",
            "Epoch 00174: val_accuracy did not improve from 0.92365\n",
            "Epoch 175/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0803 - accuracy: 0.9726 - val_loss: 0.4508 - val_accuracy: 0.9310\n",
            "\n",
            "Epoch 00175: val_accuracy improved from 0.92365 to 0.93103, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/BS_08_3_DN121.h5\n",
            "Epoch 176/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0183 - accuracy: 0.9939 - val_loss: 0.4270 - val_accuracy: 0.9138\n",
            "\n",
            "Epoch 00176: val_accuracy did not improve from 0.93103\n",
            "Epoch 177/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0611 - accuracy: 0.9829 - val_loss: 0.4357 - val_accuracy: 0.9015\n",
            "\n",
            "Epoch 00177: val_accuracy did not improve from 0.93103\n",
            "Epoch 178/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0104 - accuracy: 0.9957 - val_loss: 0.3855 - val_accuracy: 0.9187\n",
            "\n",
            "Epoch 00178: val_accuracy did not improve from 0.93103\n",
            "Epoch 179/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0107 - accuracy: 0.9963 - val_loss: 0.4071 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00179: val_accuracy did not improve from 0.93103\n",
            "Epoch 180/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0158 - accuracy: 0.9951 - val_loss: 0.7490 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00180: val_accuracy did not improve from 0.93103\n",
            "Epoch 181/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0164 - accuracy: 0.9963 - val_loss: 0.4186 - val_accuracy: 0.9236\n",
            "\n",
            "Epoch 00181: val_accuracy did not improve from 0.93103\n",
            "Epoch 182/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0168 - accuracy: 0.9951 - val_loss: 0.3958 - val_accuracy: 0.9236\n",
            "\n",
            "Epoch 00182: val_accuracy did not improve from 0.93103\n",
            "Epoch 183/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0573 - accuracy: 0.9854 - val_loss: 1.0081 - val_accuracy: 0.7685\n",
            "\n",
            "Epoch 00183: val_accuracy did not improve from 0.93103\n",
            "Epoch 184/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0754 - accuracy: 0.9762 - val_loss: 0.5463 - val_accuracy: 0.8645\n",
            "\n",
            "Epoch 00184: val_accuracy did not improve from 0.93103\n",
            "Epoch 185/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0363 - accuracy: 0.9872 - val_loss: 0.5294 - val_accuracy: 0.8941\n",
            "\n",
            "Epoch 00185: val_accuracy did not improve from 0.93103\n",
            "Epoch 186/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0078 - accuracy: 0.9976 - val_loss: 0.4232 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00186: val_accuracy did not improve from 0.93103\n",
            "Epoch 187/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0049 - accuracy: 0.9988 - val_loss: 0.3636 - val_accuracy: 0.9138\n",
            "\n",
            "Epoch 00187: val_accuracy did not improve from 0.93103\n",
            "Epoch 188/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0087 - accuracy: 0.9976 - val_loss: 0.3160 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00188: val_accuracy did not improve from 0.93103\n",
            "Epoch 189/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0208 - accuracy: 0.9939 - val_loss: 0.4950 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00189: val_accuracy did not improve from 0.93103\n",
            "Epoch 190/500\n",
            "206/206 [==============================] - 26s 129ms/step - loss: 0.0209 - accuracy: 0.9921 - val_loss: 0.4127 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00190: val_accuracy did not improve from 0.93103\n",
            "Epoch 191/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0049 - accuracy: 0.9988 - val_loss: 0.4233 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00191: val_accuracy did not improve from 0.93103\n",
            "Epoch 192/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0098 - accuracy: 0.9976 - val_loss: 0.5789 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00192: val_accuracy did not improve from 0.93103\n",
            "Epoch 193/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0121 - accuracy: 0.9963 - val_loss: 0.5962 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00193: val_accuracy did not improve from 0.93103\n",
            "Epoch 194/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0198 - accuracy: 0.9951 - val_loss: 0.4508 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00194: val_accuracy did not improve from 0.93103\n",
            "Epoch 195/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0329 - accuracy: 0.9890 - val_loss: 0.6325 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00195: val_accuracy did not improve from 0.93103\n",
            "Epoch 196/500\n",
            "206/206 [==============================] - 26s 129ms/step - loss: 0.0223 - accuracy: 0.9921 - val_loss: 0.6267 - val_accuracy: 0.8695\n",
            "\n",
            "Epoch 00196: val_accuracy did not improve from 0.93103\n",
            "Epoch 197/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0101 - accuracy: 0.9970 - val_loss: 0.4216 - val_accuracy: 0.9187\n",
            "\n",
            "Epoch 00197: val_accuracy did not improve from 0.93103\n",
            "Epoch 198/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0142 - accuracy: 0.9963 - val_loss: 0.4705 - val_accuracy: 0.9113\n",
            "\n",
            "Epoch 00198: val_accuracy did not improve from 0.93103\n",
            "Epoch 199/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0315 - accuracy: 0.9915 - val_loss: 0.4821 - val_accuracy: 0.9015\n",
            "\n",
            "Epoch 00199: val_accuracy did not improve from 0.93103\n",
            "Epoch 200/500\n",
            "206/206 [==============================] - 26s 129ms/step - loss: 0.0344 - accuracy: 0.9878 - val_loss: 0.7111 - val_accuracy: 0.8695\n",
            "\n",
            "Epoch 00200: val_accuracy did not improve from 0.93103\n",
            "Epoch 201/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0264 - accuracy: 0.9927 - val_loss: 0.7048 - val_accuracy: 0.8448\n",
            "\n",
            "Epoch 00201: val_accuracy did not improve from 0.93103\n",
            "Epoch 202/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0558 - accuracy: 0.9848 - val_loss: 0.6756 - val_accuracy: 0.8177\n",
            "\n",
            "Epoch 00202: val_accuracy did not improve from 0.93103\n",
            "Epoch 203/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0214 - accuracy: 0.9939 - val_loss: 0.5515 - val_accuracy: 0.8399\n",
            "\n",
            "Epoch 00203: val_accuracy did not improve from 0.93103\n",
            "Epoch 204/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0163 - accuracy: 0.9945 - val_loss: 0.4150 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00204: val_accuracy did not improve from 0.93103\n",
            "Epoch 205/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0069 - accuracy: 0.9982 - val_loss: 0.4772 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00205: val_accuracy did not improve from 0.93103\n",
            "Epoch 206/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0068 - accuracy: 0.9982 - val_loss: 0.4101 - val_accuracy: 0.9064\n",
            "\n",
            "Epoch 00206: val_accuracy did not improve from 0.93103\n",
            "Epoch 207/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0147 - accuracy: 0.9945 - val_loss: 0.5255 - val_accuracy: 0.8695\n",
            "\n",
            "Epoch 00207: val_accuracy did not improve from 0.93103\n",
            "Epoch 208/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0383 - accuracy: 0.9890 - val_loss: 0.6516 - val_accuracy: 0.8793\n",
            "\n",
            "Epoch 00208: val_accuracy did not improve from 0.93103\n",
            "Epoch 209/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0218 - accuracy: 0.9921 - val_loss: 0.5531 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00209: val_accuracy did not improve from 0.93103\n",
            "Epoch 210/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0657 - accuracy: 0.9805 - val_loss: 0.4349 - val_accuracy: 0.9064\n",
            "\n",
            "Epoch 00210: val_accuracy did not improve from 0.93103\n",
            "Epoch 211/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0200 - accuracy: 0.9939 - val_loss: 0.3662 - val_accuracy: 0.9064\n",
            "\n",
            "Epoch 00211: val_accuracy did not improve from 0.93103\n",
            "Epoch 212/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0115 - accuracy: 0.9976 - val_loss: 0.4638 - val_accuracy: 0.8941\n",
            "\n",
            "Epoch 00212: val_accuracy did not improve from 0.93103\n",
            "Epoch 213/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0116 - accuracy: 0.9951 - val_loss: 0.4023 - val_accuracy: 0.8916\n",
            "\n",
            "Epoch 00213: val_accuracy did not improve from 0.93103\n",
            "Epoch 214/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0068 - accuracy: 0.9982 - val_loss: 0.3211 - val_accuracy: 0.9236\n",
            "\n",
            "Epoch 00214: val_accuracy did not improve from 0.93103\n",
            "Epoch 215/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0059 - accuracy: 0.9988 - val_loss: 0.4545 - val_accuracy: 0.8744\n",
            "\n",
            "Epoch 00215: val_accuracy did not improve from 0.93103\n",
            "Epoch 216/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0099 - accuracy: 0.9963 - val_loss: 0.6005 - val_accuracy: 0.8596\n",
            "\n",
            "Epoch 00216: val_accuracy did not improve from 0.93103\n",
            "Epoch 217/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0074 - accuracy: 0.9988 - val_loss: 0.8225 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00217: val_accuracy did not improve from 0.93103\n",
            "Epoch 218/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0457 - accuracy: 0.9872 - val_loss: 0.6443 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00218: val_accuracy did not improve from 0.93103\n",
            "Epoch 219/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0308 - accuracy: 0.9890 - val_loss: 1.6049 - val_accuracy: 0.7759\n",
            "\n",
            "Epoch 00219: val_accuracy did not improve from 0.93103\n",
            "Epoch 220/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0109 - accuracy: 0.9976 - val_loss: 0.4478 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00220: val_accuracy did not improve from 0.93103\n",
            "Epoch 221/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0159 - accuracy: 0.9957 - val_loss: 0.5451 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00221: val_accuracy did not improve from 0.93103\n",
            "Epoch 222/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0280 - accuracy: 0.9884 - val_loss: 0.4037 - val_accuracy: 0.9212\n",
            "\n",
            "Epoch 00222: val_accuracy did not improve from 0.93103\n",
            "Epoch 223/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0331 - accuracy: 0.9909 - val_loss: 0.6969 - val_accuracy: 0.8498\n",
            "\n",
            "Epoch 00223: val_accuracy did not improve from 0.93103\n",
            "Epoch 224/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0325 - accuracy: 0.9903 - val_loss: 0.4585 - val_accuracy: 0.8892\n",
            "\n",
            "Epoch 00224: val_accuracy did not improve from 0.93103\n",
            "Epoch 225/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0097 - accuracy: 0.9982 - val_loss: 0.4143 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00225: val_accuracy did not improve from 0.93103\n",
            "Epoch 226/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.3301 - val_accuracy: 0.9310\n",
            "\n",
            "Epoch 00226: val_accuracy did not improve from 0.93103\n",
            "Epoch 227/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0063 - accuracy: 0.9982 - val_loss: 0.3608 - val_accuracy: 0.9187\n",
            "\n",
            "Epoch 00227: val_accuracy did not improve from 0.93103\n",
            "Epoch 228/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0116 - accuracy: 0.9945 - val_loss: 0.8679 - val_accuracy: 0.8522\n",
            "\n",
            "Epoch 00228: val_accuracy did not improve from 0.93103\n",
            "Epoch 229/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0351 - accuracy: 0.9854 - val_loss: 0.8435 - val_accuracy: 0.7389\n",
            "\n",
            "Epoch 00229: val_accuracy did not improve from 0.93103\n",
            "Epoch 230/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0119 - accuracy: 0.9976 - val_loss: 0.3343 - val_accuracy: 0.9138\n",
            "\n",
            "Epoch 00230: val_accuracy did not improve from 0.93103\n",
            "Epoch 231/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0217 - accuracy: 0.9951 - val_loss: 0.5087 - val_accuracy: 0.8892\n",
            "\n",
            "Epoch 00231: val_accuracy did not improve from 0.93103\n",
            "Epoch 232/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0079 - accuracy: 0.9963 - val_loss: 0.3541 - val_accuracy: 0.9064\n",
            "\n",
            "Epoch 00232: val_accuracy did not improve from 0.93103\n",
            "Epoch 233/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0105 - accuracy: 0.9957 - val_loss: 0.4843 - val_accuracy: 0.8818\n",
            "\n",
            "Epoch 00233: val_accuracy did not improve from 0.93103\n",
            "Epoch 234/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0255 - accuracy: 0.9927 - val_loss: 0.4141 - val_accuracy: 0.9015\n",
            "\n",
            "Epoch 00234: val_accuracy did not improve from 0.93103\n",
            "Epoch 235/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0439 - accuracy: 0.9896 - val_loss: 1.8182 - val_accuracy: 0.7438\n",
            "\n",
            "Epoch 00235: val_accuracy did not improve from 0.93103\n",
            "Epoch 236/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0330 - accuracy: 0.9890 - val_loss: 0.4293 - val_accuracy: 0.9015\n",
            "\n",
            "Epoch 00236: val_accuracy did not improve from 0.93103\n",
            "Epoch 237/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0133 - accuracy: 0.9951 - val_loss: 0.5445 - val_accuracy: 0.9015\n",
            "\n",
            "Epoch 00237: val_accuracy did not improve from 0.93103\n",
            "Epoch 238/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0107 - accuracy: 0.9976 - val_loss: 0.4140 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00238: val_accuracy did not improve from 0.93103\n",
            "Epoch 239/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0090 - accuracy: 0.9970 - val_loss: 0.5233 - val_accuracy: 0.8892\n",
            "\n",
            "Epoch 00239: val_accuracy did not improve from 0.93103\n",
            "Epoch 240/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0359 - accuracy: 0.9903 - val_loss: 0.7165 - val_accuracy: 0.8103\n",
            "\n",
            "Epoch 00240: val_accuracy did not improve from 0.93103\n",
            "Epoch 241/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0309 - accuracy: 0.9903 - val_loss: 0.5357 - val_accuracy: 0.8941\n",
            "\n",
            "Epoch 00241: val_accuracy did not improve from 0.93103\n",
            "Epoch 242/500\n",
            "206/206 [==============================] - 26s 127ms/step - loss: 0.0152 - accuracy: 0.9957 - val_loss: 0.4172 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00242: val_accuracy did not improve from 0.93103\n",
            "Epoch 243/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0051 - accuracy: 0.9988 - val_loss: 0.3784 - val_accuracy: 0.9187\n",
            "\n",
            "Epoch 00243: val_accuracy did not improve from 0.93103\n",
            "Epoch 244/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0094 - accuracy: 0.9970 - val_loss: 0.6602 - val_accuracy: 0.8005\n",
            "\n",
            "Epoch 00244: val_accuracy did not improve from 0.93103\n",
            "Epoch 245/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0442 - accuracy: 0.9872 - val_loss: 0.5580 - val_accuracy: 0.8547\n",
            "\n",
            "Epoch 00245: val_accuracy did not improve from 0.93103\n",
            "Epoch 246/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0152 - accuracy: 0.9939 - val_loss: 0.6452 - val_accuracy: 0.8768\n",
            "\n",
            "Epoch 00246: val_accuracy did not improve from 0.93103\n",
            "Epoch 247/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0118 - accuracy: 0.9951 - val_loss: 0.6074 - val_accuracy: 0.8941\n",
            "\n",
            "Epoch 00247: val_accuracy did not improve from 0.93103\n",
            "Epoch 248/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0215 - accuracy: 0.9927 - val_loss: 1.4973 - val_accuracy: 0.8005\n",
            "\n",
            "Epoch 00248: val_accuracy did not improve from 0.93103\n",
            "Epoch 249/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0370 - accuracy: 0.9896 - val_loss: 0.8749 - val_accuracy: 0.8251\n",
            "\n",
            "Epoch 00249: val_accuracy did not improve from 0.93103\n",
            "Epoch 250/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0106 - accuracy: 0.9957 - val_loss: 0.5977 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00250: val_accuracy did not improve from 0.93103\n",
            "Epoch 251/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0192 - accuracy: 0.9933 - val_loss: 0.4968 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00251: val_accuracy did not improve from 0.93103\n",
            "Epoch 252/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0082 - accuracy: 0.9994 - val_loss: 0.4348 - val_accuracy: 0.9064\n",
            "\n",
            "Epoch 00252: val_accuracy did not improve from 0.93103\n",
            "Epoch 253/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.4433 - val_accuracy: 0.9187\n",
            "\n",
            "Epoch 00253: val_accuracy did not improve from 0.93103\n",
            "Epoch 254/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0042 - accuracy: 0.9982 - val_loss: 0.4797 - val_accuracy: 0.9039\n",
            "\n",
            "Epoch 00254: val_accuracy did not improve from 0.93103\n",
            "Epoch 255/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.4752 - val_accuracy: 0.8768\n",
            "\n",
            "Epoch 00255: val_accuracy did not improve from 0.93103\n",
            "Epoch 256/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.4389 - val_accuracy: 0.9261\n",
            "\n",
            "Epoch 00256: val_accuracy did not improve from 0.93103\n",
            "Epoch 257/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 7.2116e-04 - accuracy: 1.0000 - val_loss: 0.3377 - val_accuracy: 0.9261\n",
            "\n",
            "Epoch 00257: val_accuracy did not improve from 0.93103\n",
            "Epoch 258/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0030 - accuracy: 0.9988 - val_loss: 0.4715 - val_accuracy: 0.8768\n",
            "\n",
            "Epoch 00258: val_accuracy did not improve from 0.93103\n",
            "Epoch 259/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0306 - accuracy: 0.9915 - val_loss: 0.8817 - val_accuracy: 0.8596\n",
            "\n",
            "Epoch 00259: val_accuracy did not improve from 0.93103\n",
            "Epoch 260/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0372 - accuracy: 0.9878 - val_loss: 0.5224 - val_accuracy: 0.9039\n",
            "\n",
            "Epoch 00260: val_accuracy did not improve from 0.93103\n",
            "Epoch 261/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0087 - accuracy: 0.9982 - val_loss: 0.4569 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00261: val_accuracy did not improve from 0.93103\n",
            "Epoch 262/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0035 - accuracy: 0.9994 - val_loss: 0.3581 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00262: val_accuracy did not improve from 0.93103\n",
            "Epoch 263/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0294 - accuracy: 0.9896 - val_loss: 0.4411 - val_accuracy: 0.8916\n",
            "\n",
            "Epoch 00263: val_accuracy did not improve from 0.93103\n",
            "Epoch 264/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0225 - accuracy: 0.9933 - val_loss: 0.5672 - val_accuracy: 0.8941\n",
            "\n",
            "Epoch 00264: val_accuracy did not improve from 0.93103\n",
            "Epoch 265/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0114 - accuracy: 0.9957 - val_loss: 0.3814 - val_accuracy: 0.9212\n",
            "\n",
            "Epoch 00265: val_accuracy did not improve from 0.93103\n",
            "Epoch 266/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0046 - accuracy: 0.9982 - val_loss: 0.4045 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00266: val_accuracy did not improve from 0.93103\n",
            "Epoch 267/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0074 - accuracy: 0.9976 - val_loss: 0.4235 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00267: val_accuracy did not improve from 0.93103\n",
            "Epoch 268/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0142 - accuracy: 0.9945 - val_loss: 0.4670 - val_accuracy: 0.9064\n",
            "\n",
            "Epoch 00268: val_accuracy did not improve from 0.93103\n",
            "Epoch 269/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0143 - accuracy: 0.9957 - val_loss: 0.3767 - val_accuracy: 0.9113\n",
            "\n",
            "Epoch 00269: val_accuracy did not improve from 0.93103\n",
            "Epoch 270/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0051 - accuracy: 0.9976 - val_loss: 0.5519 - val_accuracy: 0.8916\n",
            "\n",
            "Epoch 00270: val_accuracy did not improve from 0.93103\n",
            "Epoch 271/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0405 - accuracy: 0.9854 - val_loss: 0.8552 - val_accuracy: 0.8621\n",
            "\n",
            "Epoch 00271: val_accuracy did not improve from 0.93103\n",
            "Epoch 272/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0125 - accuracy: 0.9957 - val_loss: 0.4435 - val_accuracy: 0.9015\n",
            "\n",
            "Epoch 00272: val_accuracy did not improve from 0.93103\n",
            "Epoch 273/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0099 - accuracy: 0.9970 - val_loss: 0.4592 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00273: val_accuracy did not improve from 0.93103\n",
            "Epoch 274/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.4059 - val_accuracy: 0.9138\n",
            "\n",
            "Epoch 00274: val_accuracy did not improve from 0.93103\n",
            "Epoch 275/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0069 - accuracy: 0.9970 - val_loss: 0.6279 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00275: val_accuracy did not improve from 0.93103\n",
            "Epoch 276/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0197 - accuracy: 0.9939 - val_loss: 0.4692 - val_accuracy: 0.9039\n",
            "\n",
            "Epoch 00276: val_accuracy did not improve from 0.93103\n",
            "Epoch 277/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0084 - accuracy: 0.9982 - val_loss: 0.3944 - val_accuracy: 0.9039\n",
            "\n",
            "Epoch 00277: val_accuracy did not improve from 0.93103\n",
            "Epoch 278/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0181 - accuracy: 0.9945 - val_loss: 0.5478 - val_accuracy: 0.8645\n",
            "\n",
            "Epoch 00278: val_accuracy did not improve from 0.93103\n",
            "Epoch 279/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0123 - accuracy: 0.9963 - val_loss: 0.4524 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00279: val_accuracy did not improve from 0.93103\n",
            "Epoch 280/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0200 - accuracy: 0.9927 - val_loss: 0.4929 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00280: val_accuracy did not improve from 0.93103\n",
            "Epoch 281/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0207 - accuracy: 0.9939 - val_loss: 1.2018 - val_accuracy: 0.8276\n",
            "\n",
            "Epoch 00281: val_accuracy did not improve from 0.93103\n",
            "Epoch 282/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0320 - accuracy: 0.9890 - val_loss: 1.3947 - val_accuracy: 0.7611\n",
            "\n",
            "Epoch 00282: val_accuracy did not improve from 0.93103\n",
            "Epoch 283/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0191 - accuracy: 0.9927 - val_loss: 0.4535 - val_accuracy: 0.8941\n",
            "\n",
            "Epoch 00283: val_accuracy did not improve from 0.93103\n",
            "Epoch 284/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0256 - accuracy: 0.9945 - val_loss: 0.7521 - val_accuracy: 0.8744\n",
            "\n",
            "Epoch 00284: val_accuracy did not improve from 0.93103\n",
            "Epoch 285/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0173 - accuracy: 0.9951 - val_loss: 0.4139 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00285: val_accuracy did not improve from 0.93103\n",
            "Epoch 286/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0055 - accuracy: 0.9976 - val_loss: 0.4455 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00286: val_accuracy did not improve from 0.93103\n",
            "Epoch 287/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.3747 - val_accuracy: 0.9310\n",
            "\n",
            "Epoch 00287: val_accuracy did not improve from 0.93103\n",
            "Epoch 288/500\n",
            "206/206 [==============================] - 26s 128ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.4652 - val_accuracy: 0.8941\n",
            "\n",
            "Epoch 00288: val_accuracy did not improve from 0.93103\n",
            "Epoch 289/500\n",
            "206/206 [==============================] - 27s 131ms/step - loss: 0.0059 - accuracy: 0.9976 - val_loss: 0.5206 - val_accuracy: 0.8719\n",
            "\n",
            "Epoch 00289: val_accuracy did not improve from 0.93103\n",
            "Epoch 290/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0198 - accuracy: 0.9927 - val_loss: 0.6055 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00290: val_accuracy did not improve from 0.93103\n",
            "Epoch 291/500\n",
            "206/206 [==============================] - 27s 131ms/step - loss: 0.0119 - accuracy: 0.9957 - val_loss: 0.4404 - val_accuracy: 0.9187\n",
            "\n",
            "Epoch 00291: val_accuracy did not improve from 0.93103\n",
            "Epoch 292/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0120 - accuracy: 0.9951 - val_loss: 0.4496 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00292: val_accuracy did not improve from 0.93103\n",
            "Epoch 293/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0060 - accuracy: 0.9994 - val_loss: 0.3673 - val_accuracy: 0.9212\n",
            "\n",
            "Epoch 00293: val_accuracy did not improve from 0.93103\n",
            "Epoch 294/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0091 - accuracy: 0.9982 - val_loss: 0.3871 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00294: val_accuracy did not improve from 0.93103\n",
            "Epoch 295/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.4791 - val_accuracy: 0.9138\n",
            "\n",
            "Epoch 00295: val_accuracy did not improve from 0.93103\n",
            "Epoch 296/500\n",
            "206/206 [==============================] - 27s 131ms/step - loss: 0.0218 - accuracy: 0.9915 - val_loss: 0.7653 - val_accuracy: 0.8079\n",
            "\n",
            "Epoch 00296: val_accuracy did not improve from 0.93103\n",
            "Epoch 297/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0079 - accuracy: 0.9970 - val_loss: 0.4042 - val_accuracy: 0.8818\n",
            "\n",
            "Epoch 00297: val_accuracy did not improve from 0.93103\n",
            "Epoch 298/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0170 - accuracy: 0.9915 - val_loss: 0.4538 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00298: val_accuracy did not improve from 0.93103\n",
            "Epoch 299/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0173 - accuracy: 0.9951 - val_loss: 0.5754 - val_accuracy: 0.8941\n",
            "\n",
            "Epoch 00299: val_accuracy did not improve from 0.93103\n",
            "Epoch 300/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0254 - accuracy: 0.9890 - val_loss: 0.5949 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00300: val_accuracy did not improve from 0.93103\n",
            "Epoch 301/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0117 - accuracy: 0.9976 - val_loss: 0.4657 - val_accuracy: 0.8719\n",
            "\n",
            "Epoch 00301: val_accuracy did not improve from 0.93103\n",
            "Epoch 302/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0215 - accuracy: 0.9951 - val_loss: 0.8267 - val_accuracy: 0.8547\n",
            "\n",
            "Epoch 00302: val_accuracy did not improve from 0.93103\n",
            "Epoch 303/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0111 - accuracy: 0.9957 - val_loss: 0.6218 - val_accuracy: 0.8916\n",
            "\n",
            "Epoch 00303: val_accuracy did not improve from 0.93103\n",
            "Epoch 304/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0129 - accuracy: 0.9970 - val_loss: 0.4668 - val_accuracy: 0.9163\n",
            "\n",
            "Epoch 00304: val_accuracy did not improve from 0.93103\n",
            "Epoch 305/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0033 - accuracy: 0.9988 - val_loss: 0.5432 - val_accuracy: 0.9039\n",
            "\n",
            "Epoch 00305: val_accuracy did not improve from 0.93103\n",
            "Epoch 306/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0033 - accuracy: 0.9988 - val_loss: 0.5627 - val_accuracy: 0.9236\n",
            "\n",
            "Epoch 00306: val_accuracy did not improve from 0.93103\n",
            "Epoch 307/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0103 - accuracy: 0.9970 - val_loss: 0.5870 - val_accuracy: 0.8744\n",
            "\n",
            "Epoch 00307: val_accuracy did not improve from 0.93103\n",
            "Epoch 308/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0060 - accuracy: 0.9982 - val_loss: 0.3986 - val_accuracy: 0.9113\n",
            "\n",
            "Epoch 00308: val_accuracy did not improve from 0.93103\n",
            "Epoch 309/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0159 - accuracy: 0.9957 - val_loss: 0.6856 - val_accuracy: 0.8300\n",
            "\n",
            "Epoch 00309: val_accuracy did not improve from 0.93103\n",
            "Epoch 310/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0082 - accuracy: 0.9957 - val_loss: 0.6665 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00310: val_accuracy did not improve from 0.93103\n",
            "Epoch 311/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0154 - accuracy: 0.9945 - val_loss: 0.4206 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00311: val_accuracy did not improve from 0.93103\n",
            "Epoch 312/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0122 - accuracy: 0.9970 - val_loss: 0.8245 - val_accuracy: 0.8695\n",
            "\n",
            "Epoch 00312: val_accuracy did not improve from 0.93103\n",
            "Epoch 313/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0238 - accuracy: 0.9945 - val_loss: 0.4371 - val_accuracy: 0.9236\n",
            "\n",
            "Epoch 00313: val_accuracy did not improve from 0.93103\n",
            "Epoch 314/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0036 - accuracy: 0.9994 - val_loss: 0.4043 - val_accuracy: 0.9113\n",
            "\n",
            "Epoch 00314: val_accuracy did not improve from 0.93103\n",
            "Epoch 315/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0506 - accuracy: 0.9872 - val_loss: 1.4331 - val_accuracy: 0.6404\n",
            "\n",
            "Epoch 00315: val_accuracy did not improve from 0.93103\n",
            "Epoch 316/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0326 - accuracy: 0.9915 - val_loss: 0.4380 - val_accuracy: 0.9113\n",
            "\n",
            "Epoch 00316: val_accuracy did not improve from 0.93103\n",
            "Epoch 317/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0068 - accuracy: 0.9970 - val_loss: 0.4836 - val_accuracy: 0.9039\n",
            "\n",
            "Epoch 00317: val_accuracy did not improve from 0.93103\n",
            "Epoch 318/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0041 - accuracy: 0.9994 - val_loss: 0.3591 - val_accuracy: 0.9236\n",
            "\n",
            "Epoch 00318: val_accuracy did not improve from 0.93103\n",
            "Epoch 319/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 0.0075 - accuracy: 0.9970 - val_loss: 0.3635 - val_accuracy: 0.9138\n",
            "\n",
            "Epoch 00319: val_accuracy did not improve from 0.93103\n",
            "Epoch 320/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0043 - accuracy: 0.9982 - val_loss: 0.3773 - val_accuracy: 0.9039\n",
            "\n",
            "Epoch 00320: val_accuracy did not improve from 0.93103\n",
            "Epoch 321/500\n",
            "206/206 [==============================] - 27s 131ms/step - loss: 0.0062 - accuracy: 0.9994 - val_loss: 0.4828 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00321: val_accuracy did not improve from 0.93103\n",
            "Epoch 322/500\n",
            "206/206 [==============================] - 27s 131ms/step - loss: 0.0060 - accuracy: 0.9982 - val_loss: 0.4349 - val_accuracy: 0.9039\n",
            "\n",
            "Epoch 00322: val_accuracy did not improve from 0.93103\n",
            "Epoch 323/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0108 - accuracy: 0.9957 - val_loss: 0.4847 - val_accuracy: 0.9138\n",
            "\n",
            "Epoch 00323: val_accuracy did not improve from 0.93103\n",
            "Epoch 324/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0184 - accuracy: 0.9951 - val_loss: 0.5565 - val_accuracy: 0.8818\n",
            "\n",
            "Epoch 00324: val_accuracy did not improve from 0.93103\n",
            "Epoch 325/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0126 - accuracy: 0.9970 - val_loss: 0.8528 - val_accuracy: 0.8695\n",
            "\n",
            "Epoch 00325: val_accuracy did not improve from 0.93103\n",
            "Epoch 326/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0410 - accuracy: 0.9860 - val_loss: 0.6181 - val_accuracy: 0.8645\n",
            "\n",
            "Epoch 00326: val_accuracy did not improve from 0.93103\n",
            "Epoch 327/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0121 - accuracy: 0.9957 - val_loss: 0.4389 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00327: val_accuracy did not improve from 0.93103\n",
            "Epoch 328/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0123 - accuracy: 0.9963 - val_loss: 0.4332 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00328: val_accuracy did not improve from 0.93103\n",
            "Epoch 329/500\n",
            "206/206 [==============================] - 27s 131ms/step - loss: 0.0049 - accuracy: 0.9982 - val_loss: 0.4338 - val_accuracy: 0.9212\n",
            "\n",
            "Epoch 00329: val_accuracy did not improve from 0.93103\n",
            "Epoch 330/500\n",
            "206/206 [==============================] - 27s 131ms/step - loss: 0.0054 - accuracy: 0.9988 - val_loss: 0.3960 - val_accuracy: 0.9138\n",
            "\n",
            "Epoch 00330: val_accuracy did not improve from 0.93103\n",
            "Epoch 331/500\n",
            "206/206 [==============================] - 27s 129ms/step - loss: 8.9536e-04 - accuracy: 1.0000 - val_loss: 0.4063 - val_accuracy: 0.9310\n",
            "\n",
            "Epoch 00331: val_accuracy did not improve from 0.93103\n",
            "Epoch 332/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0036 - accuracy: 0.9988 - val_loss: 0.3916 - val_accuracy: 0.9039\n",
            "\n",
            "Epoch 00332: val_accuracy did not improve from 0.93103\n",
            "Epoch 333/500\n",
            "206/206 [==============================] - 27s 131ms/step - loss: 0.0044 - accuracy: 0.9994 - val_loss: 0.5643 - val_accuracy: 0.8941\n",
            "\n",
            "Epoch 00333: val_accuracy did not improve from 0.93103\n",
            "Epoch 334/500\n",
            "206/206 [==============================] - 27s 131ms/step - loss: 0.0448 - accuracy: 0.9896 - val_loss: 0.5951 - val_accuracy: 0.8892\n",
            "\n",
            "Epoch 00334: val_accuracy did not improve from 0.93103\n",
            "Epoch 335/500\n",
            "206/206 [==============================] - 27s 131ms/step - loss: 0.0079 - accuracy: 0.9976 - val_loss: 0.5260 - val_accuracy: 0.9015\n",
            "\n",
            "Epoch 00335: val_accuracy did not improve from 0.93103\n",
            "Epoch 336/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0073 - accuracy: 0.9982 - val_loss: 0.4852 - val_accuracy: 0.9064\n",
            "\n",
            "Epoch 00336: val_accuracy did not improve from 0.93103\n",
            "Epoch 337/500\n",
            "206/206 [==============================] - 27s 131ms/step - loss: 0.0050 - accuracy: 0.9994 - val_loss: 0.5361 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00337: val_accuracy did not improve from 0.93103\n",
            "Epoch 338/500\n",
            "206/206 [==============================] - 27s 131ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.5483 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00338: val_accuracy did not improve from 0.93103\n",
            "Epoch 339/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0236 - accuracy: 0.9945 - val_loss: 0.9829 - val_accuracy: 0.8670\n",
            "\n",
            "Epoch 00339: val_accuracy did not improve from 0.93103\n",
            "Epoch 340/500\n",
            "206/206 [==============================] - 27s 131ms/step - loss: 0.0252 - accuracy: 0.9951 - val_loss: 0.6000 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00340: val_accuracy did not improve from 0.93103\n",
            "Epoch 341/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0146 - accuracy: 0.9957 - val_loss: 0.4501 - val_accuracy: 0.8793\n",
            "\n",
            "Epoch 00341: val_accuracy did not improve from 0.93103\n",
            "Epoch 342/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4503 - val_accuracy: 0.9064\n",
            "\n",
            "Epoch 00342: val_accuracy did not improve from 0.93103\n",
            "Epoch 343/500\n",
            "206/206 [==============================] - 27s 131ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.3576 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00343: val_accuracy did not improve from 0.93103\n",
            "Epoch 344/500\n",
            "206/206 [==============================] - 27s 131ms/step - loss: 0.0304 - accuracy: 0.9927 - val_loss: 0.5165 - val_accuracy: 0.9138\n",
            "\n",
            "Epoch 00344: val_accuracy did not improve from 0.93103\n",
            "Epoch 345/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0061 - accuracy: 0.9988 - val_loss: 0.4623 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00345: val_accuracy did not improve from 0.93103\n",
            "Epoch 346/500\n",
            "206/206 [==============================] - 27s 131ms/step - loss: 0.0223 - accuracy: 0.9933 - val_loss: 0.4496 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00346: val_accuracy did not improve from 0.93103\n",
            "Epoch 347/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0089 - accuracy: 0.9970 - val_loss: 0.5863 - val_accuracy: 0.8916\n",
            "\n",
            "Epoch 00347: val_accuracy did not improve from 0.93103\n",
            "Epoch 348/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0036 - accuracy: 0.9982 - val_loss: 0.6639 - val_accuracy: 0.8892\n",
            "\n",
            "Epoch 00348: val_accuracy did not improve from 0.93103\n",
            "Epoch 349/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0058 - accuracy: 0.9982 - val_loss: 0.3558 - val_accuracy: 0.9113\n",
            "\n",
            "Epoch 00349: val_accuracy did not improve from 0.93103\n",
            "Epoch 350/500\n",
            "206/206 [==============================] - 27s 131ms/step - loss: 0.0069 - accuracy: 0.9970 - val_loss: 0.8200 - val_accuracy: 0.7980\n",
            "\n",
            "Epoch 00350: val_accuracy did not improve from 0.93103\n",
            "Epoch 351/500\n",
            "206/206 [==============================] - 27s 131ms/step - loss: 0.0086 - accuracy: 0.9976 - val_loss: 0.4261 - val_accuracy: 0.9212\n",
            "\n",
            "Epoch 00351: val_accuracy did not improve from 0.93103\n",
            "Epoch 352/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0027 - accuracy: 0.9988 - val_loss: 0.4839 - val_accuracy: 0.9015\n",
            "\n",
            "Epoch 00352: val_accuracy did not improve from 0.93103\n",
            "Epoch 353/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0257 - accuracy: 0.9927 - val_loss: 0.6863 - val_accuracy: 0.8054\n",
            "\n",
            "Epoch 00353: val_accuracy did not improve from 0.93103\n",
            "Epoch 354/500\n",
            "206/206 [==============================] - 27s 131ms/step - loss: 0.0168 - accuracy: 0.9945 - val_loss: 0.6490 - val_accuracy: 0.8892\n",
            "\n",
            "Epoch 00354: val_accuracy did not improve from 0.93103\n",
            "Epoch 355/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0194 - accuracy: 0.9939 - val_loss: 0.4982 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00355: val_accuracy did not improve from 0.93103\n",
            "Epoch 356/500\n",
            "206/206 [==============================] - 27s 131ms/step - loss: 0.0135 - accuracy: 0.9951 - val_loss: 0.5427 - val_accuracy: 0.8892\n",
            "\n",
            "Epoch 00356: val_accuracy did not improve from 0.93103\n",
            "Epoch 357/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0165 - accuracy: 0.9945 - val_loss: 0.7361 - val_accuracy: 0.8670\n",
            "\n",
            "Epoch 00357: val_accuracy did not improve from 0.93103\n",
            "Epoch 358/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0057 - accuracy: 0.9988 - val_loss: 0.5313 - val_accuracy: 0.9015\n",
            "\n",
            "Epoch 00358: val_accuracy did not improve from 0.93103\n",
            "Epoch 359/500\n",
            "206/206 [==============================] - 27s 131ms/step - loss: 0.0220 - accuracy: 0.9939 - val_loss: 0.8271 - val_accuracy: 0.8621\n",
            "\n",
            "Epoch 00359: val_accuracy did not improve from 0.93103\n",
            "Epoch 360/500\n",
            "206/206 [==============================] - 27s 131ms/step - loss: 0.0234 - accuracy: 0.9951 - val_loss: 0.5964 - val_accuracy: 0.8916\n",
            "\n",
            "Epoch 00360: val_accuracy did not improve from 0.93103\n",
            "Epoch 361/500\n",
            "206/206 [==============================] - 27s 131ms/step - loss: 0.0066 - accuracy: 0.9976 - val_loss: 0.3452 - val_accuracy: 0.9163\n",
            "\n",
            "Epoch 00361: val_accuracy did not improve from 0.93103\n",
            "Epoch 362/500\n",
            "206/206 [==============================] - 27s 131ms/step - loss: 0.0111 - accuracy: 0.9976 - val_loss: 0.4523 - val_accuracy: 0.8941\n",
            "\n",
            "Epoch 00362: val_accuracy did not improve from 0.93103\n",
            "Epoch 363/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0041 - accuracy: 0.9994 - val_loss: 0.5023 - val_accuracy: 0.9138\n",
            "\n",
            "Epoch 00363: val_accuracy did not improve from 0.93103\n",
            "Epoch 364/500\n",
            "206/206 [==============================] - 27s 131ms/step - loss: 0.0063 - accuracy: 0.9982 - val_loss: 0.5137 - val_accuracy: 0.9015\n",
            "\n",
            "Epoch 00364: val_accuracy did not improve from 0.93103\n",
            "Epoch 365/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0184 - accuracy: 0.9939 - val_loss: 0.8878 - val_accuracy: 0.8670\n",
            "\n",
            "Epoch 00365: val_accuracy did not improve from 0.93103\n",
            "Epoch 366/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0234 - accuracy: 0.9945 - val_loss: 0.5993 - val_accuracy: 0.9015\n",
            "\n",
            "Epoch 00366: val_accuracy did not improve from 0.93103\n",
            "Epoch 367/500\n",
            "206/206 [==============================] - 27s 131ms/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.4291 - val_accuracy: 0.9236\n",
            "\n",
            "Epoch 00367: val_accuracy did not improve from 0.93103\n",
            "Epoch 368/500\n",
            "206/206 [==============================] - 27s 131ms/step - loss: 0.0054 - accuracy: 0.9982 - val_loss: 0.6329 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00368: val_accuracy did not improve from 0.93103\n",
            "Epoch 369/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0115 - accuracy: 0.9976 - val_loss: 0.5193 - val_accuracy: 0.9113\n",
            "\n",
            "Epoch 00369: val_accuracy did not improve from 0.93103\n",
            "Epoch 370/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0049 - accuracy: 0.9982 - val_loss: 0.4692 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00370: val_accuracy did not improve from 0.93103\n",
            "Epoch 371/500\n",
            "206/206 [==============================] - 27s 130ms/step - loss: 0.0297 - accuracy: 0.9921 - val_loss: 2.0504 - val_accuracy: 0.5320\n",
            "\n",
            "Epoch 00371: val_accuracy did not improve from 0.93103\n",
            "Epoch 372/500\n",
            "206/206 [==============================] - 27s 131ms/step - loss: 0.0292 - accuracy: 0.9890 - val_loss: 1.3755 - val_accuracy: 0.6724\n",
            "\n",
            "Epoch 00372: val_accuracy did not improve from 0.93103\n",
            "Epoch 373/500\n",
            "206/206 [==============================] - 27s 131ms/step - loss: 0.0150 - accuracy: 0.9933 - val_loss: 0.4744 - val_accuracy: 0.9015\n",
            "\n",
            "Epoch 00373: val_accuracy did not improve from 0.93103\n",
            "Epoch 374/500\n",
            "206/206 [==============================] - 27s 131ms/step - loss: 0.0042 - accuracy: 0.9994 - val_loss: 0.4078 - val_accuracy: 0.9163\n",
            "\n",
            "Epoch 00374: val_accuracy did not improve from 0.93103\n",
            "Epoch 375/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.4367 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00375: val_accuracy did not improve from 0.93103\n",
            "Epoch 376/500\n",
            "206/206 [==============================] - 27s 131ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.4332 - val_accuracy: 0.9187\n",
            "\n",
            "Epoch 00376: val_accuracy did not improve from 0.93103\n",
            "Epoch 377/500\n",
            "206/206 [==============================] - 27s 131ms/step - loss: 8.8612e-04 - accuracy: 1.0000 - val_loss: 0.4881 - val_accuracy: 0.9138\n",
            "\n",
            "Epoch 00377: val_accuracy did not improve from 0.93103\n",
            "Epoch 378/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4095 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00378: val_accuracy did not improve from 0.93103\n",
            "Epoch 379/500\n",
            "206/206 [==============================] - 27s 131ms/step - loss: 0.0012 - accuracy: 0.9994 - val_loss: 0.4352 - val_accuracy: 0.9212\n",
            "\n",
            "Epoch 00379: val_accuracy did not improve from 0.93103\n",
            "Epoch 380/500\n",
            "206/206 [==============================] - 27s 131ms/step - loss: 0.0572 - accuracy: 0.9848 - val_loss: 4.8113 - val_accuracy: 0.3571\n",
            "\n",
            "Epoch 00380: val_accuracy did not improve from 0.93103\n",
            "Epoch 381/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0312 - accuracy: 0.9921 - val_loss: 0.5355 - val_accuracy: 0.8916\n",
            "\n",
            "Epoch 00381: val_accuracy did not improve from 0.93103\n",
            "Epoch 382/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0121 - accuracy: 0.9963 - val_loss: 0.5209 - val_accuracy: 0.9064\n",
            "\n",
            "Epoch 00382: val_accuracy did not improve from 0.93103\n",
            "Epoch 383/500\n",
            "206/206 [==============================] - 27s 131ms/step - loss: 0.0249 - accuracy: 0.9933 - val_loss: 0.4855 - val_accuracy: 0.9039\n",
            "\n",
            "Epoch 00383: val_accuracy did not improve from 0.93103\n",
            "Epoch 384/500\n",
            "206/206 [==============================] - 27s 131ms/step - loss: 0.0042 - accuracy: 0.9988 - val_loss: 0.4661 - val_accuracy: 0.9064\n",
            "\n",
            "Epoch 00384: val_accuracy did not improve from 0.93103\n",
            "Epoch 385/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0031 - accuracy: 0.9988 - val_loss: 0.4211 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00385: val_accuracy did not improve from 0.93103\n",
            "Epoch 386/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 9.5444e-04 - accuracy: 1.0000 - val_loss: 0.4440 - val_accuracy: 0.9113\n",
            "\n",
            "Epoch 00386: val_accuracy did not improve from 0.93103\n",
            "Epoch 387/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0054 - accuracy: 0.9988 - val_loss: 0.5135 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00387: val_accuracy did not improve from 0.93103\n",
            "Epoch 388/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0042 - accuracy: 0.9982 - val_loss: 0.4184 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00388: val_accuracy did not improve from 0.93103\n",
            "Epoch 389/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0072 - accuracy: 0.9988 - val_loss: 0.4759 - val_accuracy: 0.8892\n",
            "\n",
            "Epoch 00389: val_accuracy did not improve from 0.93103\n",
            "Epoch 390/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0031 - accuracy: 0.9988 - val_loss: 0.4745 - val_accuracy: 0.9064\n",
            "\n",
            "Epoch 00390: val_accuracy did not improve from 0.93103\n",
            "Epoch 391/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.4547 - val_accuracy: 0.9138\n",
            "\n",
            "Epoch 00391: val_accuracy did not improve from 0.93103\n",
            "Epoch 392/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.4450 - val_accuracy: 0.9187\n",
            "\n",
            "Epoch 00392: val_accuracy did not improve from 0.93103\n",
            "Epoch 393/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0147 - accuracy: 0.9951 - val_loss: 0.6053 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00393: val_accuracy did not improve from 0.93103\n",
            "Epoch 394/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0032 - accuracy: 0.9988 - val_loss: 0.4692 - val_accuracy: 0.9163\n",
            "\n",
            "Epoch 00394: val_accuracy did not improve from 0.93103\n",
            "Epoch 395/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0272 - accuracy: 0.9939 - val_loss: 0.5893 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00395: val_accuracy did not improve from 0.93103\n",
            "Epoch 396/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0119 - accuracy: 0.9963 - val_loss: 0.4854 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00396: val_accuracy did not improve from 0.93103\n",
            "Epoch 397/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0046 - accuracy: 0.9976 - val_loss: 0.5551 - val_accuracy: 0.9039\n",
            "\n",
            "Epoch 00397: val_accuracy did not improve from 0.93103\n",
            "Epoch 398/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0062 - accuracy: 0.9982 - val_loss: 0.4902 - val_accuracy: 0.9015\n",
            "\n",
            "Epoch 00398: val_accuracy did not improve from 0.93103\n",
            "Epoch 399/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0284 - accuracy: 0.9933 - val_loss: 0.8631 - val_accuracy: 0.8596\n",
            "\n",
            "Epoch 00399: val_accuracy did not improve from 0.93103\n",
            "Epoch 400/500\n",
            "206/206 [==============================] - 27s 131ms/step - loss: 0.0043 - accuracy: 0.9988 - val_loss: 0.4743 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00400: val_accuracy did not improve from 0.93103\n",
            "Epoch 401/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0088 - accuracy: 0.9963 - val_loss: 1.1252 - val_accuracy: 0.7217\n",
            "\n",
            "Epoch 00401: val_accuracy did not improve from 0.93103\n",
            "Epoch 402/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4429 - val_accuracy: 0.9064\n",
            "\n",
            "Epoch 00402: val_accuracy did not improve from 0.93103\n",
            "Epoch 403/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0060 - accuracy: 0.9988 - val_loss: 0.4220 - val_accuracy: 0.9113\n",
            "\n",
            "Epoch 00403: val_accuracy did not improve from 0.93103\n",
            "Epoch 404/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0047 - accuracy: 0.9988 - val_loss: 0.4616 - val_accuracy: 0.8670\n",
            "\n",
            "Epoch 00404: val_accuracy did not improve from 0.93103\n",
            "Epoch 405/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0034 - accuracy: 0.9982 - val_loss: 0.5065 - val_accuracy: 0.9015\n",
            "\n",
            "Epoch 00405: val_accuracy did not improve from 0.93103\n",
            "Epoch 406/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.5496 - val_accuracy: 0.8818\n",
            "\n",
            "Epoch 00406: val_accuracy did not improve from 0.93103\n",
            "Epoch 407/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0039 - accuracy: 0.9988 - val_loss: 0.5615 - val_accuracy: 0.9015\n",
            "\n",
            "Epoch 00407: val_accuracy did not improve from 0.93103\n",
            "Epoch 408/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0083 - accuracy: 0.9970 - val_loss: 0.5364 - val_accuracy: 0.9064\n",
            "\n",
            "Epoch 00408: val_accuracy did not improve from 0.93103\n",
            "Epoch 409/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0102 - accuracy: 0.9951 - val_loss: 0.8603 - val_accuracy: 0.7611\n",
            "\n",
            "Epoch 00409: val_accuracy did not improve from 0.93103\n",
            "Epoch 410/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0484 - accuracy: 0.9878 - val_loss: 0.6817 - val_accuracy: 0.8374\n",
            "\n",
            "Epoch 00410: val_accuracy did not improve from 0.93103\n",
            "Epoch 411/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0161 - accuracy: 0.9957 - val_loss: 0.5368 - val_accuracy: 0.9015\n",
            "\n",
            "Epoch 00411: val_accuracy did not improve from 0.93103\n",
            "Epoch 412/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0088 - accuracy: 0.9963 - val_loss: 0.5956 - val_accuracy: 0.9064\n",
            "\n",
            "Epoch 00412: val_accuracy did not improve from 0.93103\n",
            "Epoch 413/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0038 - accuracy: 0.9994 - val_loss: 0.3623 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00413: val_accuracy did not improve from 0.93103\n",
            "Epoch 414/500\n",
            "206/206 [==============================] - 27s 131ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.3182 - val_accuracy: 0.9310\n",
            "\n",
            "Epoch 00414: val_accuracy did not improve from 0.93103\n",
            "Epoch 415/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0136 - accuracy: 0.9963 - val_loss: 0.6216 - val_accuracy: 0.8941\n",
            "\n",
            "Epoch 00415: val_accuracy did not improve from 0.93103\n",
            "Epoch 416/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0173 - accuracy: 0.9951 - val_loss: 0.4822 - val_accuracy: 0.9039\n",
            "\n",
            "Epoch 00416: val_accuracy did not improve from 0.93103\n",
            "Epoch 417/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0229 - accuracy: 0.9933 - val_loss: 0.6074 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00417: val_accuracy did not improve from 0.93103\n",
            "Epoch 418/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0499 - accuracy: 0.9848 - val_loss: 0.5649 - val_accuracy: 0.8842\n",
            "\n",
            "Epoch 00418: val_accuracy did not improve from 0.93103\n",
            "Epoch 419/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0082 - accuracy: 0.9982 - val_loss: 0.4461 - val_accuracy: 0.9113\n",
            "\n",
            "Epoch 00419: val_accuracy did not improve from 0.93103\n",
            "Epoch 420/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.4333 - val_accuracy: 0.9236\n",
            "\n",
            "Epoch 00420: val_accuracy did not improve from 0.93103\n",
            "Epoch 421/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.3716 - val_accuracy: 0.9310\n",
            "\n",
            "Epoch 00421: val_accuracy did not improve from 0.93103\n",
            "Epoch 422/500\n",
            "206/206 [==============================] - 28s 134ms/step - loss: 9.7230e-04 - accuracy: 1.0000 - val_loss: 0.4437 - val_accuracy: 0.9187\n",
            "\n",
            "Epoch 00422: val_accuracy did not improve from 0.93103\n",
            "Epoch 423/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 7.2686e-04 - accuracy: 1.0000 - val_loss: 0.4681 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00423: val_accuracy did not improve from 0.93103\n",
            "Epoch 424/500\n",
            "206/206 [==============================] - 27s 131ms/step - loss: 5.4979e-04 - accuracy: 1.0000 - val_loss: 0.4102 - val_accuracy: 0.9310\n",
            "\n",
            "Epoch 00424: val_accuracy did not improve from 0.93103\n",
            "Epoch 425/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 4.0463e-04 - accuracy: 1.0000 - val_loss: 0.4233 - val_accuracy: 0.9335\n",
            "\n",
            "Epoch 00425: val_accuracy improved from 0.93103 to 0.93350, saving model to /content/drive/MyDrive/DACON_CVLC/Checkpoint/BS_08_3_DN121.h5\n",
            "Epoch 426/500\n",
            "206/206 [==============================] - 28s 134ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.4840 - val_accuracy: 0.9015\n",
            "\n",
            "Epoch 00426: val_accuracy did not improve from 0.93350\n",
            "Epoch 427/500\n",
            "206/206 [==============================] - 28s 135ms/step - loss: 0.0011 - accuracy: 0.9994 - val_loss: 0.3757 - val_accuracy: 0.9163\n",
            "\n",
            "Epoch 00427: val_accuracy did not improve from 0.93350\n",
            "Epoch 428/500\n",
            "206/206 [==============================] - 28s 134ms/step - loss: 8.6307e-04 - accuracy: 1.0000 - val_loss: 0.4365 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00428: val_accuracy did not improve from 0.93350\n",
            "Epoch 429/500\n",
            "206/206 [==============================] - 28s 134ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.5046 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00429: val_accuracy did not improve from 0.93350\n",
            "Epoch 430/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0104 - accuracy: 0.9970 - val_loss: 0.4572 - val_accuracy: 0.8892\n",
            "\n",
            "Epoch 00430: val_accuracy did not improve from 0.93350\n",
            "Epoch 431/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0199 - accuracy: 0.9963 - val_loss: 0.6860 - val_accuracy: 0.8916\n",
            "\n",
            "Epoch 00431: val_accuracy did not improve from 0.93350\n",
            "Epoch 432/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0127 - accuracy: 0.9963 - val_loss: 0.7823 - val_accuracy: 0.8202\n",
            "\n",
            "Epoch 00432: val_accuracy did not improve from 0.93350\n",
            "Epoch 433/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0214 - accuracy: 0.9915 - val_loss: 0.5970 - val_accuracy: 0.8251\n",
            "\n",
            "Epoch 00433: val_accuracy did not improve from 0.93350\n",
            "Epoch 434/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0091 - accuracy: 0.9970 - val_loss: 0.5516 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00434: val_accuracy did not improve from 0.93350\n",
            "Epoch 435/500\n",
            "206/206 [==============================] - 28s 134ms/step - loss: 0.0034 - accuracy: 0.9982 - val_loss: 0.5058 - val_accuracy: 0.9138\n",
            "\n",
            "Epoch 00435: val_accuracy did not improve from 0.93350\n",
            "Epoch 436/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0014 - accuracy: 0.9994 - val_loss: 0.5023 - val_accuracy: 0.8941\n",
            "\n",
            "Epoch 00436: val_accuracy did not improve from 0.93350\n",
            "Epoch 437/500\n",
            "206/206 [==============================] - 28s 134ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.3662 - val_accuracy: 0.9261\n",
            "\n",
            "Epoch 00437: val_accuracy did not improve from 0.93350\n",
            "Epoch 438/500\n",
            "206/206 [==============================] - 28s 134ms/step - loss: 4.7909e-04 - accuracy: 1.0000 - val_loss: 0.4160 - val_accuracy: 0.9163\n",
            "\n",
            "Epoch 00438: val_accuracy did not improve from 0.93350\n",
            "Epoch 439/500\n",
            "206/206 [==============================] - 28s 133ms/step - loss: 3.8974e-04 - accuracy: 1.0000 - val_loss: 0.3910 - val_accuracy: 0.9261\n",
            "\n",
            "Epoch 00439: val_accuracy did not improve from 0.93350\n",
            "Epoch 440/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 4.6419e-04 - accuracy: 1.0000 - val_loss: 0.4593 - val_accuracy: 0.9163\n",
            "\n",
            "Epoch 00440: val_accuracy did not improve from 0.93350\n",
            "Epoch 441/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.7811 - val_accuracy: 0.8818\n",
            "\n",
            "Epoch 00441: val_accuracy did not improve from 0.93350\n",
            "Epoch 442/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0307 - accuracy: 0.9927 - val_loss: 0.6463 - val_accuracy: 0.8892\n",
            "\n",
            "Epoch 00442: val_accuracy did not improve from 0.93350\n",
            "Epoch 443/500\n",
            "206/206 [==============================] - 28s 136ms/step - loss: 0.0263 - accuracy: 0.9890 - val_loss: 0.5190 - val_accuracy: 0.8892\n",
            "\n",
            "Epoch 00443: val_accuracy did not improve from 0.93350\n",
            "Epoch 444/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0083 - accuracy: 0.9988 - val_loss: 0.4152 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00444: val_accuracy did not improve from 0.93350\n",
            "Epoch 445/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0058 - accuracy: 0.9988 - val_loss: 0.4545 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00445: val_accuracy did not improve from 0.93350\n",
            "Epoch 446/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.4237 - val_accuracy: 0.9163\n",
            "\n",
            "Epoch 00446: val_accuracy did not improve from 0.93350\n",
            "Epoch 447/500\n",
            "206/206 [==============================] - 28s 134ms/step - loss: 0.0044 - accuracy: 0.9982 - val_loss: 0.4416 - val_accuracy: 0.9113\n",
            "\n",
            "Epoch 00447: val_accuracy did not improve from 0.93350\n",
            "Epoch 448/500\n",
            "206/206 [==============================] - 28s 133ms/step - loss: 0.0044 - accuracy: 0.9982 - val_loss: 0.3875 - val_accuracy: 0.9261\n",
            "\n",
            "Epoch 00448: val_accuracy did not improve from 0.93350\n",
            "Epoch 449/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0128 - accuracy: 0.9970 - val_loss: 0.5691 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00449: val_accuracy did not improve from 0.93350\n",
            "Epoch 450/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0105 - accuracy: 0.9963 - val_loss: 0.5195 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00450: val_accuracy did not improve from 0.93350\n",
            "Epoch 451/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0114 - accuracy: 0.9951 - val_loss: 0.5255 - val_accuracy: 0.9163\n",
            "\n",
            "Epoch 00451: val_accuracy did not improve from 0.93350\n",
            "Epoch 452/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0213 - accuracy: 0.9939 - val_loss: 0.8417 - val_accuracy: 0.8768\n",
            "\n",
            "Epoch 00452: val_accuracy did not improve from 0.93350\n",
            "Epoch 453/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0105 - accuracy: 0.9963 - val_loss: 0.9363 - val_accuracy: 0.8522\n",
            "\n",
            "Epoch 00453: val_accuracy did not improve from 0.93350\n",
            "Epoch 454/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0233 - accuracy: 0.9927 - val_loss: 0.5149 - val_accuracy: 0.8892\n",
            "\n",
            "Epoch 00454: val_accuracy did not improve from 0.93350\n",
            "Epoch 455/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0097 - accuracy: 0.9970 - val_loss: 0.4168 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00455: val_accuracy did not improve from 0.93350\n",
            "Epoch 456/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0239 - accuracy: 0.9945 - val_loss: 0.5148 - val_accuracy: 0.9113\n",
            "\n",
            "Epoch 00456: val_accuracy did not improve from 0.93350\n",
            "Epoch 457/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0042 - accuracy: 0.9988 - val_loss: 0.4806 - val_accuracy: 0.9015\n",
            "\n",
            "Epoch 00457: val_accuracy did not improve from 0.93350\n",
            "Epoch 458/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0063 - accuracy: 0.9976 - val_loss: 0.4503 - val_accuracy: 0.9236\n",
            "\n",
            "Epoch 00458: val_accuracy did not improve from 0.93350\n",
            "Epoch 459/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.3698 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00459: val_accuracy did not improve from 0.93350\n",
            "Epoch 460/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3834 - val_accuracy: 0.9310\n",
            "\n",
            "Epoch 00460: val_accuracy did not improve from 0.93350\n",
            "Epoch 461/500\n",
            "206/206 [==============================] - 28s 134ms/step - loss: 4.0039e-04 - accuracy: 1.0000 - val_loss: 0.4132 - val_accuracy: 0.9286\n",
            "\n",
            "Epoch 00461: val_accuracy did not improve from 0.93350\n",
            "Epoch 462/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 8.4220e-04 - accuracy: 1.0000 - val_loss: 0.4370 - val_accuracy: 0.9212\n",
            "\n",
            "Epoch 00462: val_accuracy did not improve from 0.93350\n",
            "Epoch 463/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 4.8186e-04 - accuracy: 1.0000 - val_loss: 0.4262 - val_accuracy: 0.9163\n",
            "\n",
            "Epoch 00463: val_accuracy did not improve from 0.93350\n",
            "Epoch 464/500\n",
            "206/206 [==============================] - 28s 134ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4335 - val_accuracy: 0.9261\n",
            "\n",
            "Epoch 00464: val_accuracy did not improve from 0.93350\n",
            "Epoch 465/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0021 - accuracy: 0.9988 - val_loss: 0.7943 - val_accuracy: 0.8719\n",
            "\n",
            "Epoch 00465: val_accuracy did not improve from 0.93350\n",
            "Epoch 466/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0177 - accuracy: 0.9963 - val_loss: 0.4910 - val_accuracy: 0.9236\n",
            "\n",
            "Epoch 00466: val_accuracy did not improve from 0.93350\n",
            "Epoch 467/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0233 - accuracy: 0.9933 - val_loss: 0.4852 - val_accuracy: 0.8916\n",
            "\n",
            "Epoch 00467: val_accuracy did not improve from 0.93350\n",
            "Epoch 468/500\n",
            "206/206 [==============================] - 28s 134ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.4957 - val_accuracy: 0.9064\n",
            "\n",
            "Epoch 00468: val_accuracy did not improve from 0.93350\n",
            "Epoch 469/500\n",
            "206/206 [==============================] - 27s 132ms/step - loss: 0.0165 - accuracy: 0.9945 - val_loss: 0.8546 - val_accuracy: 0.8325\n",
            "\n",
            "Epoch 00469: val_accuracy did not improve from 0.93350\n",
            "Epoch 470/500\n",
            "206/206 [==============================] - 28s 135ms/step - loss: 0.0226 - accuracy: 0.9921 - val_loss: 0.5832 - val_accuracy: 0.9015\n",
            "\n",
            "Epoch 00470: val_accuracy did not improve from 0.93350\n",
            "Epoch 471/500\n",
            "206/206 [==============================] - 28s 134ms/step - loss: 0.0165 - accuracy: 0.9963 - val_loss: 0.4125 - val_accuracy: 0.9113\n",
            "\n",
            "Epoch 00471: val_accuracy did not improve from 0.93350\n",
            "Epoch 472/500\n",
            "206/206 [==============================] - 28s 134ms/step - loss: 0.0201 - accuracy: 0.9939 - val_loss: 0.6291 - val_accuracy: 0.8818\n",
            "\n",
            "Epoch 00472: val_accuracy did not improve from 0.93350\n",
            "Epoch 473/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0042 - accuracy: 0.9982 - val_loss: 0.4417 - val_accuracy: 0.9261\n",
            "\n",
            "Epoch 00473: val_accuracy did not improve from 0.93350\n",
            "Epoch 474/500\n",
            "206/206 [==============================] - 28s 134ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.4982 - val_accuracy: 0.9212\n",
            "\n",
            "Epoch 00474: val_accuracy did not improve from 0.93350\n",
            "Epoch 475/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0043 - accuracy: 0.9988 - val_loss: 0.5307 - val_accuracy: 0.8941\n",
            "\n",
            "Epoch 00475: val_accuracy did not improve from 0.93350\n",
            "Epoch 476/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0128 - accuracy: 0.9945 - val_loss: 0.6341 - val_accuracy: 0.8916\n",
            "\n",
            "Epoch 00476: val_accuracy did not improve from 0.93350\n",
            "Epoch 477/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0255 - accuracy: 0.9933 - val_loss: 1.0004 - val_accuracy: 0.7291\n",
            "\n",
            "Epoch 00477: val_accuracy did not improve from 0.93350\n",
            "Epoch 478/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0049 - accuracy: 0.9988 - val_loss: 0.5254 - val_accuracy: 0.8621\n",
            "\n",
            "Epoch 00478: val_accuracy did not improve from 0.93350\n",
            "Epoch 479/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0123 - accuracy: 0.9970 - val_loss: 0.6095 - val_accuracy: 0.8916\n",
            "\n",
            "Epoch 00479: val_accuracy did not improve from 0.93350\n",
            "Epoch 480/500\n",
            "206/206 [==============================] - 28s 134ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.5416 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00480: val_accuracy did not improve from 0.93350\n",
            "Epoch 481/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0146 - accuracy: 0.9951 - val_loss: 0.5019 - val_accuracy: 0.8990\n",
            "\n",
            "Epoch 00481: val_accuracy did not improve from 0.93350\n",
            "Epoch 482/500\n",
            "206/206 [==============================] - 28s 134ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.5179 - val_accuracy: 0.9163\n",
            "\n",
            "Epoch 00482: val_accuracy did not improve from 0.93350\n",
            "Epoch 483/500\n",
            "206/206 [==============================] - 28s 134ms/step - loss: 0.0013 - accuracy: 0.9994 - val_loss: 0.4926 - val_accuracy: 0.8867\n",
            "\n",
            "Epoch 00483: val_accuracy did not improve from 0.93350\n",
            "Epoch 484/500\n",
            "206/206 [==============================] - 28s 133ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.3954 - val_accuracy: 0.9089\n",
            "\n",
            "Epoch 00484: val_accuracy did not improve from 0.93350\n",
            "Epoch 485/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.4041 - val_accuracy: 0.9261\n",
            "\n",
            "Epoch 00485: val_accuracy did not improve from 0.93350\n",
            "Epoch 486/500\n",
            "206/206 [==============================] - 28s 136ms/step - loss: 0.0023 - accuracy: 0.9988 - val_loss: 0.4023 - val_accuracy: 0.9163\n",
            "\n",
            "Epoch 00486: val_accuracy did not improve from 0.93350\n",
            "Epoch 487/500\n",
            "206/206 [==============================] - 28s 135ms/step - loss: 0.0025 - accuracy: 0.9988 - val_loss: 0.6473 - val_accuracy: 0.7956\n",
            "\n",
            "Epoch 00487: val_accuracy did not improve from 0.93350\n",
            "Epoch 488/500\n",
            "206/206 [==============================] - 28s 135ms/step - loss: 0.0126 - accuracy: 0.9957 - val_loss: 0.5044 - val_accuracy: 0.9163\n",
            "\n",
            "Epoch 00488: val_accuracy did not improve from 0.93350\n",
            "Epoch 489/500\n",
            "206/206 [==============================] - 28s 136ms/step - loss: 0.0035 - accuracy: 0.9988 - val_loss: 0.3922 - val_accuracy: 0.9236\n",
            "\n",
            "Epoch 00489: val_accuracy did not improve from 0.93350\n",
            "Epoch 490/500\n",
            "206/206 [==============================] - 28s 134ms/step - loss: 0.0040 - accuracy: 0.9982 - val_loss: 0.5671 - val_accuracy: 0.8892\n",
            "\n",
            "Epoch 00490: val_accuracy did not improve from 0.93350\n",
            "Epoch 491/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.4323 - val_accuracy: 0.9261\n",
            "\n",
            "Epoch 00491: val_accuracy did not improve from 0.93350\n",
            "Epoch 492/500\n",
            "206/206 [==============================] - 28s 134ms/step - loss: 0.0116 - accuracy: 0.9976 - val_loss: 0.5194 - val_accuracy: 0.8695\n",
            "\n",
            "Epoch 00492: val_accuracy did not improve from 0.93350\n",
            "Epoch 493/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.4806 - val_accuracy: 0.8941\n",
            "\n",
            "Epoch 00493: val_accuracy did not improve from 0.93350\n",
            "Epoch 494/500\n",
            "206/206 [==============================] - 28s 134ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.4221 - val_accuracy: 0.9236\n",
            "\n",
            "Epoch 00494: val_accuracy did not improve from 0.93350\n",
            "Epoch 495/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0329 - accuracy: 0.9915 - val_loss: 0.6679 - val_accuracy: 0.9015\n",
            "\n",
            "Epoch 00495: val_accuracy did not improve from 0.93350\n",
            "Epoch 496/500\n",
            "206/206 [==============================] - 28s 134ms/step - loss: 0.0035 - accuracy: 0.9988 - val_loss: 0.4018 - val_accuracy: 0.9113\n",
            "\n",
            "Epoch 00496: val_accuracy did not improve from 0.93350\n",
            "Epoch 497/500\n",
            "206/206 [==============================] - 28s 135ms/step - loss: 0.0108 - accuracy: 0.9957 - val_loss: 0.9974 - val_accuracy: 0.8571\n",
            "\n",
            "Epoch 00497: val_accuracy did not improve from 0.93350\n",
            "Epoch 498/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0512 - accuracy: 0.9854 - val_loss: 0.8192 - val_accuracy: 0.8670\n",
            "\n",
            "Epoch 00498: val_accuracy did not improve from 0.93350\n",
            "Epoch 499/500\n",
            "206/206 [==============================] - 28s 134ms/step - loss: 0.0133 - accuracy: 0.9957 - val_loss: 0.3878 - val_accuracy: 0.9212\n",
            "\n",
            "Epoch 00499: val_accuracy did not improve from 0.93350\n",
            "Epoch 500/500\n",
            "206/206 [==============================] - 27s 133ms/step - loss: 0.0044 - accuracy: 0.9982 - val_loss: 0.4411 - val_accuracy: 0.8966\n",
            "\n",
            "Epoch 00500: val_accuracy did not improve from 0.93350\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdf800e8b90>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHmpkzRJyCrf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "cd0049b2-2399-4255-df5a-79031cf891d0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(DenseNet121_model.history.history[\"accuracy\"], label='DenseNet121_acc')\n",
        "plt.plot(DenseNet121_model.history.history[\"val_accuracy\"], label='DenseNet121_val')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydd5gURfrHP7WzOcIG4pLzkjOIKCggIoqYDuNP5cR4p2c677wznXp6ep6HeCrGU8905oCHoogYQJAoOcOSdmHZnGfq90dNz/TM9OzOLrM7M0t9nmefnenu6a7urvrWW2+9VSWklGg0Go0m8okKdQI0Go1GExy0oGs0Gk0LQQu6RqPRtBC0oGs0Gk0LQQu6RqPRtBCiQ3XhzMxM2bVr11BdXqPRaCKSn3/++YiUMstqX8gEvWvXrqxcuTJUl9doNJqIRAixx98+7XLRaDSaFoIWdI1Go2khaEHXaDSaFoIWdI1Go2khaEHXaDSaFkK9gi6EeEkIkSeE+MXPfiGEmCuE2C6EWCeEGBb8ZGo0Go2mPgKx0F8Bptax/0ygl/NvDvDM8SdLo9FoNA2l3jh0KeW3QoiudRwyA3hVqnl4lwkhWgkh2kspDwYpjZogkl9SRWZyLEKIRp9j1d5jDOiQRmx04z12B4sqaJ0YC0BRRQ1tU+P9HltVa0dKiI+xubZJKck9VkHrpFgcUpIaH+PaV1Ft56fdBQzr3IoU03YrCsqqiYuOIinOXRRq7Q7e/TkXCZRV1VJtd3DBsGyS4qJZs6+Qk3pkAOCQYItyP8cF6w/Sp10KXTOSPLYb/LjjKJW1dib0zqLGLvl2az5CwCm9s4ixeT7LfQXl2B2Sjq0TXPsKy6vZV1DBgI6pLNtZQI82SWw6WMLIrq1JjHWnf+/Rcr7Zmkd6UizjemRSXmPnyw2HOKlnJq0SYnh7xT7G9sige1YyqfHRVNY6iI+O4nBJFR1bJQBQWWOn1iFJjotmf2EFR0urGNgxje+2H2FIp1bEx9iIsUWxau8xNuwvIr+kiuT4aGYM6Ujb1HjyS6r4ec8xyqtrGdgxjSOl1eSVVHJSj0wSY20kxUVTY3dwrLyatfuKsDsctEtL4EhJFSO7pZOWEIOUks9/OUSN3cGMIR2RUvLRmgPszC913Wu0LYrY6Ci6ZyYxOactQgiqax0s3HCIbYdLAMhKieNXIzuz60gZucfK6dkmmU6tE4kyvSMpJbuOlLF02xGOllYBIIRgUr+2tE2N42hZNf3ap2J3SBasP0iblDiGd2nNoeJK9h+rICpKsD63iIoaO2O6p9MlI4m1+wo5UlqF3QE92yTTq00yrZNi2ZlfSrfMpOMqg/4IxsCijsA+0/dc5zYfQRdCzEFZ8XTu3DkIlz5xkFLy486jDOnUyqPwSil5f9V+pg5oR4wtinmLt3PmgHb0a5/qOuZIaRWZyXH8vOcY5z/zA1eP68Y9Z+ewaONhnvt2BzOGdKSwvJrZJ3cnIVaJ5vPf7mRvQTm3T+lDSny0K/O/sXwvf/xgPb8+uRtTB7RjWOfWfLM1j9eX7aWgrJrZJ3dj+qD2rsy6v7CCl77bRY3dQZuUOGaf3J03f9rLA59upH1aPDntU/l6Sx5T+7fjtil9ePXH3VTVOPjz2Tkkx0Wz9XAJlzy/nFqHg3evG0uPrGTufHcd//0513V/w7u0pmOrBGKjo3h45kD+76Wf+Gl3AZ3TExnXM5Nf9hcxvEtrrhjbhdSEGG55aw3Vdgd3T+vHVa+sINYWxStXj6SovIb2aQnc/PZqVu8t9Hj+n6w9SFWtnZ35ZTw8cyAvf7+LQ0WVDOqURlmVnf4dUvnP8r2u4zu2SuCxCwYxtkcG+aVVzPt6O6/+qMaDjOmeTkFZNVsPu4XpLzP6A/DOyly6ZSbxyboDGEsVnD8sm5LKGr7YeBiAmyb2ZN7i7a7fdkpPYESXdLplJvHL/iLXcd7ktE8lPiaKVXsL4Uu1LSU+mpLKWmJtUdQ4HLz4fyOItdm4+t8rqK51MKprOj/tLnBdZ19BBQBJsTbemjOWWc8to9rucF1j44FiOqcnMvfr7T7XN9MjK4kd+WWW+7JS4njo3AEsWH+QD9ccAJQhEmOL4t6PNwBgaKF5OYeTemTQPi2BH3Yc4WBRpcc5P1l70HUfoN7P9MHtSYmLZueRMt5ftd+1z3zup77ehhACu0Py5+k5PPPNdo6UVgPQu22yxzsMhN9P7cuTi7Zy59S+zD65W4N+GwgikAUunBb6p1LKARb7PgUekVJ+5/z+FfB7KWWdw0BHjBgh9UhRa95YvpdfDhTRIyvZ9dIXrD/IDf9ZRb/2qXx+83gA8oormfv1Nl5ftpdLR3cmPSmWp77eTqwtip//PImCsmru+WgDS7bmA8qatDvU+15x9yTu/mC9R+G/9pTu/GFaP/73y0Gue32Va3tsdBQPnNOfM/q34/QnllBQVu3aN7pbOst3uQsKQIxNMLpbBjE2weIt+R77bp3cm4UbDrHnaDlVtXZq7JI+bVM4UFRBSWWt67iTemRQVlXLxoPF1NhVmqMEnNo7y3XOk3pk8MOOox7nn9SvLYs2HWZMd5Uuc/ZunxZPn3YpfOOVJjPRUYJah6R322Run9KHnA6pfLHhMA98utHjuMRYG/ExNo9n0SUjkT1Hy33OmRBjo6LGzhVju1BRbXdVRn3aprDFaUV6P78LhneidWIMn/9yiF1HfIUvMdbGmQPaM7hTGv9ctI2jpnTcMKEHZw5ozxUvLedYeQ2DstMY3yuTpxfvQAglZrnHKlzHX3dqD7YcKvZ5VwaXju7M2txCftlfTHSUYObQjh4V6n9+PZqTemRwyfPL+XGn+33cMqkXJ/XIZPfRMtqnxbNi9zHmfrUNgJS4aEqq1Pvu3yGVWyb1pqSyhsRYG49/sZXteUooLxqRze6j5fzkzGPt0+L54a7TXAZDZY2dsqpaXv1xDy8s3UlZtR2AV64ayfheWdiiBJe+sIzvt6t0PXvZMPYXVrJkaz5Lt+XjLX9zLx7KOYM7AKrleNXLP6kK0MQ5gzuwZl8hewvc77pjqwTevnYMsbYoXvhuFx+t2c/lY7ogJWw6VMyC9Ydcx3bLTOLd68aSkRxn+bzrQwjxs5RyhOW+IAj6c8A3Uso3nd+3ABPqc7mcCIIupfRpVj311TZ6tU1m6oD2rm1lVbXE2KJ4bskOrhjblcEPfOHaN6xzKwZ2TGNPQblLiN69biy2KMHMf/3gce4Ym6B9WgJ7C8q5/5z+FFfU8Pcvt3oc409EANISYnhgRn9ufmsNAKO6pbsKkpk7zujDsp1HWbrtCKDEZdWfJ/PQZ5t4bZnvqOSBHdN4bfYoLn/xJ2rsDrYeLuGm03oxsmtrnly0jYdmDmDr4VJ+++ZqUuOjGdsjg4Ub3BXNRzeO46ddBTy0YJNr26o/TyY9KZavNh1m9r9989HSOycy81/fc6S0mrvO7Mv4Xplc8MyPVNTYuXR0Z7bllfLTrgKuPbU7zy3ZCbit1fZp8bw2ezQ92yQDyi0z7C9fepz/wxvH0TUjkR35pZz/zI8ArL13CoeKKimprOFYeQ2/f28dBWXVpMRF8/a1Y8npkEpReQ2P/G8zM4Z0YESX1rzx017u+WiD67zje2Xy7GXDPVxApVW17Mgr5UhpleteF916qit91bUOHvxsI6/+uIeZQzvyj18NAeCvn2/iuSU7efH/RjCxTxt+3nuMHlnJSCl56LNNdMlI4pwhHeiWmQTAf5bvYdvhUoZ0akWn9ATyiqvI6ZBKl4wkCsqqWbTxMJNy2pKeFMstb63mwzUHmJzTluevUNry8IJNzP92J60SY7h1cm8uH9PFJ/+XVdXy/fYjjO6Wwbr9hZRW1nLmwPYex3y//QiXvrAcgN2PnIWUktv+u5b3V+1nQp8sXrlqlM/7BiivrmX63O+4aGQnrju1h8/50hJiWHvvFNf2fQXlLN6Sx8YDxUzOacvYHhkerV+AovIa3lyxl5T4aD5afYCzh3TggmHZHC2r4r8rc7l0dGcykuOosTt8XILGvR8uruT0vy9xGTC/Oa0nt03pY3kPgVCXoAfD5fIxcJMQ4i1gNFB0ovrPa+wOdh0pY39hBbe/sxZblOChmQMZ3yuTrzcrt4IhsB/fNI57PtqAELB6byFDO7di9d5CFm3ybC6v2lvoshCGd2nNz3uOccGzSkBibMJlvQLERdt48f9GcPNba/hg9X76tU8lPSmWV68exfSnvgPg1D5ZtE6KYdlOJdSTc9oybWA7au2SO95d5xJzgFtO70VWivIfPvPNDpelf8347tw4sSeHiio5e9533Dq5N/ExNv5y7gB6ZCVR65DMW7ydwvIa17laJcZySm9lJYKy7Mf1zGR8LzXHUI+sZP44rS/TBrYnPsbGwaJK1uUWATC4Uyt6t03hfxsO8fOeYwCkJyn/+4gu6XRslcCEPlmc1COT5buOct6wbDqlJzKmewafrjvIKb2yyOmQyhvXjObzXw5x1biu2IRg/f4ixvXMREpYvDmPz347ni83HmZSThviot2FMz0pln/OGsLbK/Zx3zn9OVpazZBOrZzvJN11XFpCDGkJbp/96X0n8fIPu5nUrw1dMpRopiXG8NfzBrqOuWJsV64Y2xW7Q/Lskh2cO7Sjh5gDJMdFM7hTK6RUrZkx3dNdYg6qBTW+Vxav/riH0d3c6fndpN4M6JDGaX3bIIRgZFf3viecom/m0tFdfLaZn8FFIzu5vj80cyC92qZw0Qj3tsxk9U5mj+vGFWO7Wp4nKS6aKf3bAbjevTdjumdwau8sfuW8nhCCcwZ34P1V+z36SrxJjI3m69sn+Gwf1zOTv10wiK7Od2DQKT3RbzoN0hJjXJWD+flkxybyu8m9Xd9tUTaP35krsrap8fxy/xk8vnAL8xZvp3+HtDqveTzUa6ELId4EJgCZwGHgXiAGQEr5rFApn4eKhCkHrqrP3QIt00L/4wfreWP5Xvp3SGXDgWKf/U/+agi3vL3G4pfW9G6bzMS+bVwW5J+n5/AXU9N/1shO9Gufys78UtqlJXDJqM6kJcbw9OLtPLZwCz3bJBMfE8WnvxnPuU9/z5p9hdx7dg7nD8/m9WV7+Nv/tvDSlSM4rW9b1uUWcs687wGYc0p35n+7k7X3TnEJ1MYDxUybuxRQVpOBVSsEYPpTS/llv3oG8y4ZyvRBHVi28yiz5i8jOkqw7r4pPtaQN++vyqVjqwRGd1edkEdKqxjx4CLSk2JZ9efJ9T6/0qpavt6c52pCNxXrcguxRYkmLagGRnn1fuZSSlbvK2Rop1ZN0tkWCCWVNTz/7U6un9DT1RcTLOwOyb8Wb+eikZ3q7EAPZypr7Hz+y0FmDO7o0SHbUI7LQpdSXlzPfgnc2Mi0tSg+cXbgHC6uJErAb0/vxZOLtrn2//dn1XccHxOFlPDPWUMZ0DGV15ft5dklO1zHPTxzIJeMVp3GUkqXoPfISuKDG07i6cXb6ZyexI0Te1j64WYM6cC8r7ezPa+UyTltPfa1T0sgNT6GGyb05PpTe7gKf48st8X3x2n9+OO0fh6/69MuBcDDMgRfYTFIdlqZT12sxBxgWOfWJMba6N02pV4xBzhvWLbH98zkOO6ZnsNYZ5RJfSTHRTe5mAMMym7V5Ncw8Pe8hRAM69y62dJhRUp8DLcehyuhLmxRgt+c3qtJzt1cxMfYmDk0u/4Dj4OQTZ8b6dgdkh93HGV4l9Yua8To5DlSWs2EPlncMqk3V53UjYl//4aCsmpXx8w3t08kIznWFY521biu7Mwv5cedRymprOWiEe6Xbi7A3TOT6ZyRyAv/N7LOtGW3TuT6CT144sut1JoiEABaJbqbrOZzG838aD+Wgy1KsPCWU8hKCawjp0/bFJbtLCDV5IKIjY7igRkDaBPgOay4ugkiAzSaloIW9Abw3s+55JdWcc347ry+bI8rhOov5w5gqtMvaGD4eNMSY/jpj6fT8+7PAejbLoW2qXE+Prb5zo4lu0P6xDC/e91YXl+2h46tEwJO6wXDs3niy60M7KjcALdP6cONb6zyCGf0ZtGtp5Ac599HaVjpgXDXmf3o0y6VU3pl+qRLo9E0DQFFuTQFkeBDtzsk5z3zA1ee1IUDhZU8tnALAC9fNZInv9zKWmenXWKsjfZp8ewtKHd1Ul4zvht3n5XjOlfXuz4DYNdfpzWbj/NAYQVZKXE+g1Y0Gk3k0tRRLi2WbXklrN1XyO/e9oxDfem7XazNLeLOqX04o387Lnl+GQcKK/nnrKHc/cF6jpXX0NppoRs8e9kwiitrm7XDqkOrwC16jUYT+WhBr4N1+4p8ts0Y0oGPnJ2fE3q3oUdWMt///jRqHZL4GBtzv9rGsfIan/Aqc9y5RqPRNAW6LW7B6r3HuPqVFfz1c/dAlr7tUlhyxwTOHdIRUEOf+7VXPuVoW5RrUME147sDkOFloWs0Gk1Toy10E8t2HqVdajzzvt7O15vzGNm1NTed1ou+7VKIjhJkJMfRsVUC90zP4YwB7SzdJ+cPz6ZPuxRy6uh81Gg0mqbghBf0H3YcoWdWMm1S47npjVWuUYo3TOjBnVP7+hwfbYuqN3RuQMemH2Ci0Wg03pzQgm7MA9K7bTIf3XiyaxY1gAtNQ5o1Go0mEjihBd3o3Nx6uJRrX/8ZUL7yvu1SXBMWaY4DKd1zkZ5I1w4HWvL9O+yAgB/mQtv+0Kv+aSAahL0GbHXPox+unNCdopsOFnNa3zacPbgD3zonnnpgxgCenDU0xClrAWxbBPe3gvyt1vsri2H9u01z7cMb1bV3fhPc89prYfV/nILSiN/elwZL/hbcNHmz8WPY9a26/61fWB+z+zvI3xLc6xbtV9cONp/eqp6bwY6v4YF0eOUsWHQvfHlPYOdx2Ot/dw4HfHA9PNYTcsN7jIw/TlhBr6yxs/NIGf07pPLUxW4Bb58WmRP/NBkOOxxc63//4Q1QU+m7fdsXnv9LDsPmBVDtnEN69Wvw3mwlBI2hshiObLPet09NvcqqVxt3bjNvXgJzh0LZUVjxAnx0Q+POW+KcgPSHpwI7vrYaDnkt43tgtRIdgANrlCVppqoU3rkC/n22+r7pI+tzv3IWPG09BW29HFiNzyTiAO/9Gt65HI7tbtx5/bHyRfXf7pwr/7t/qP97nVNHxyRa/27TJ/BoN3h2PJQcgp9fVu9u5UtKtJc8ZvGbj2HtG1BZCJ//Hp45GVa/DmVH3Mes+696vvZa398X7YdnT4ajO6BwLyy6Xx1bfACKm2cC2hNO0B0Oya3vrOEfX27F7pCuGfJO69sGgHbhIOhr34ZVr/lu37ygfkGoLrMucI3l28fhuVPg4DrffTUV8MxJ8Pp58OENSrQN0lR4J0X7lNB8fie8dTGs+Y/anucMCa30HLRFdbl1YfHmtXNhntdguc0LYPl8qHLO917mfyEL9z1UWl+vtkr9bfkMCnZCwQ73+XYvhQV3NsxSL3ZWXLFJ6npHtsHHv1HCbcUnN8Oz49xikrcZ5k+Ar+5XAjF/Aqx82fM3hXsA07uPtZiqwV+av34Q9v1U9z1s+lRdd+1bnttrq+DYLvV5wwf+f19V6q6QGkplkcrX5kquwzCo8J2vH4APrlP7Dq2DdW9DufO44gNKtBc/6Pub1a9B665wyp2wfyUcXg8f3QiPmyYFW/KoagGt/6/v7ze8D4fWqzLz5ED47gl17BP94AlngMUv7/u+tyBywgn68l0FvL9qP899q2YwPNk518gzlw1j6Z0Tw2OY/Adz4OObfLe/dTF88Sf/vys5DA93gGX/8t1XXgA/Pd9wsT/gXLmoaJ/vPkM493yvhHrBbe59NucEXMufhb92hCLnCjfVztV3jjhdMZVe0ww/3F6JdX3sV30eHlbqWxfD53e403rMd7ENHx5q67Zozcwd5llhVJWAcOaNX96Dn55TQh8oxv3HJML7v1bnXvWqenbeHFitRMe4LkC1c6mz9e/C0e2AhB1fef7O+35rLVpO5kquyrTQybePwYsWvujNC9yVea5T8ItNrap178A/BrhbIP6eSXmBygevnqNcPg2l4piy/s0Cnj1CXW/Tp77Ht3FPu8GBNe53990Tnmk3p/fYHmg3CIZe6nkuaaqE2qqlAl2tQFBlauVLyjKHuiu1d6+CT2/xv/84CQP1al4+WqMyY3brBC4e1ck1zWtctI1O6X6ab01J8QGV4ayYOxQW/1V9NltWD7b1LIwGR5x+UbMv0+GAd6+Gv3WDBbdbC0jhPmUBWiGc81rXVrm37f5OWdLVXuspGq6Z8gLf6+x3+iQdtaoAGD5cq/vYvdS/5eqN1e8LnWt7VhXDG7Pg7cutf2tUbns9V37CYYfiXPd5jOsIr+JSmgd7l7utvycHwY9PW1/LqGRikzwL/Gvnwo7FnsfOn+D+vPkz9Q5rnK6q4ly3y2f391BRCI90VqJW6BT0OOcYiFKLtUVLTE3//c7KevtXvscZvHUxPDceXj7L3Vo4tltdt6YS3r8GyvLcx5sNhmN7INdZ8S57xpnmpcrlY8ZhV30u3ta7ueVUWeg2Li55B36zChKdE7+9fanndd/7tbvyAeUWtOL9a+BVp/GwfxUc3QZpnZxW+h2+xx/ZBoedLQTzsz2yDT79nXLrANRW+P4WPFtATTSH1gkV5XLFSz/x7dZ8j2Wzgk5lsWruTf0rtO6iMuq2hdCmnxLBk5yWd201fHE3/DRffb/7EMQkeAp3wU5Y8ghM/IPbogVleeVthk5e0+hWqNV8iHd2Ih1YoywCbyuk68nu71/eC98/qT7f5zvVAcZKLBUFqvI59Au8caHaNv42z2MNAXx6lH93h6NW7TNcLevehp2LIb27Z6Wx53voMdH5G4cSjZR2ytI1++yrSyHRvRKPSodTPGsqYKua5ZKyI5DkFID//QE6j/V8DmasfPPVpWCv8tz2yjT1f9Cv4IyHlaAu/COM9VoeoOwILLrPeS8WLo8VL7jv1Zsv7lbvoHVX9zajuV9dAo86V9H57gnIHgWxyTDrDSVyhugU7oO0bBX1Yvbl/jBXVXpvX2Z9bbOg7vnOHTWz+jXIXQHnPe/e3+M02LsMkOodlebBK9Ohpgy6T4R8PwYDqGe2/Fm44mPofqryRad2gDyTEH/9oMoTwqauZYuBJNO8+OVH3e/XeD59zoLkLFUpWlX8oComew0873z+hqvwtD/BmjfcrZHyAs8Wm1nQzRV/XZhbQFXF7nIaRE4IC11Kyf7CClckS/8OQRjFufo/bgvHzL7lyu/64Q3q+3/OV6L96e9U4TRY+ZJbzAG2LlT/S03Wjhnvpuwv76lzrH8XPv6tKqhGB6MtRhWAd67w/V3JAffn2mq3mIPbHXJsN3x0k0qTUYiL9sO8kW4xB1j6d9902mvr9l07aj0jLH55V7mIFtzu+Xx2fev+/PUD8Pc+ShifHATzhrv3WRVUwxo2uxwec64xmb9FXe+dy63dSODHDbLG3SHnTckht+UGyjduuFiM3xoYla6ZhHoWyCjKdb8bf2T2Vs89uQ10Gw99pikX3L4V8OQA5W4Dt4U++BLVMthj0ToxKD/iuW/3Uvfn/M2Q51w9q+t4VYkkpCuB/Ed/JZDRcWCLVRV2iVen4M+vwM4l6vPyZ9X/qhIlov/IURFK713jPn7nYrBXq/szQgqjTZPPHd2u/ptbdgmtnGk65vnch5gqsKQMd38OQGpH92dhWnXpb16DCc3ltGAHDaaJOklPCAv98S+2uNayBJg1svPxn/Qjp2DfuUsJe69J6ruR2XJX1O022KvWBaXjCOWOWPkSZPVxN61dOAXVu0Asf8bze8dhbmti86fqzwojCiFvk2/0Sv5m6DhcFarVrynxi3OuULTuHV8XixWHLDpPzThq3a6hujCL6po33GmWXhZulTNNZiGqKoakLN+KxRz+lpDurgDjTNuP7VahcDGJnu9ihckaNZPZW1War85wb1v1qvLFjnIKkuH37T1ViVh0vGdlE5usKuYoG3Q71fcaMQm+zz7nXIhLUecr2qvSWlPhdrekd1edl1/dr76vfg1Gz1H5SETBwAuUn37Dh57nLTmkrNRXZ9Qf9pm3EaJi4PIPVL4XQnUKGly7RAnfC6er72md3JXoJzer/2eZfNpvm3zXa99S+eSsJ+CzW93bk9t4PheDbx+DS991u51AtfgS01WeM1ewXcfBmtfV58QMtysH3FZ+fZQeVm6T2irVCdpQSg5CG9+R6MfLCWGhrzXNmrjkjgmBR7JIqZqNmz5RGdUQjSpT4XpvtrLCDf+i4Q6wV8FGr8Ji5sAqVSiv+Uo1p3ctgX+NUW4Nz0TAZ7fVX6OX5XtmWn8cWg8Fu9S1PrjWc1/eJuUyMnzwRbluy6Y4VxXe+vCOb/7V69DWvSgy9hplfcfUMXDLFue+31WvuZu3Vr7QaqeFXunlLsqsZyk0h11VuqBEwog8+fFpJZ5WPlRv+k5XbhsrS3/B7fDMOPUMjQ7c9B7Kv+rdWbn1fyof/fdKWOhspZxsErFvH1MtQjOn3wMz5sHv1iv3UXmBenfxTkHvOh6Qbqv60HoVA//tY5DcFrqMU5VWiVd+W/26csEEEsNfsBNiE02DcIRbUG/dDK06QyvTwtNmt5GBWazNGOnqc6bn9ljTEog558J5L6jP2xep/GFY6qBaNQlOd5wRheOdDnu18m3HJsMFL6nnEgj2alU21rzu2YeQ3M76+Dgv94q3gRYkTghBzyupZHiX1jxx0WDX6uuA8s16NDEL4LPb3bHSDrsqEG9fpuJLH+uh/IQeHSLODJS3SdXWZqvOXxO9qlT53doPUt9TTFPrWg1oWPGCygA2i6Xbhl+lrLKyI/5DuMwcXAtzfVd8B1Sl8NYlkO9sgpYcVBabQXwqXPY+XPgKtBtoeQoff2K/syHFtK5p7krY+BGMvNp/Glt3dTf3zdE+ZuvPwHC5lHvde1Zv32MN2g+BqiJ3xENCKxUdM2+EstCT2sB4P0JjMO1xmPUfT8FqN8jzmMO/wOvnu10KGd3d+yY/4P5sjsU3BDHTK/25XiGF5lGgiRnq/VcVuy30jia3VJ9pgITFD6nvKe0gJt5XLAG+eVgZMIFgr/F0SxhJim+lrtTmaKwAACAASURBVAGeFm90A5YeNN6rWcDBM+IkKgoGXQg3O1uaz45zu4FA+eON/pWCnerZnPeCCnccd4uqCCsKVSunx2kw4Hyv0bV+Oi7TnNOCrHpVhSGm93CX4XQ/8zyN+63n99SmWeu2RQt6cWUNP+85Rl5JFf3ap/gsOsxHN6pRZwYL71ZNa6MjzZuKY6ppbPjPbLFu/+feZfBgG1hqakLmbYS2A3zPY/hD452/NWf07V+5M4yZwr3QNgeGXq6s3t5nwuwvYdpjqtCses06Vrwh7F3utuhGzAakpyURkwQ9T4f+MyHK5K0be5MqIEY6vTEXSsPlM+hX/tPRuquqGKvLPQeOGO6ca752P1ejtVR+1PMc3oJoJr2753d7Dax5U32uOGbRWeU1hH74lTDsCvU5zZSnoi1afubOwDSTqy/R1KHnqFF5ITbZfbw/YXAlySSkiRnq/itNHW3RsTDQ2d8x8teevzVaR6O8Wmj++O1q6zzpqHV3moM7Cii1g1sYzQIpvaJY6sJ4rzEJcMdOuMTZ0WnVqWy2uLd/rf5f+y2Mvs5toQP0PUtVADHxMPl+ZZQU7lGtz26n+J7XXyRK91Ohw1Dl1jy4Tk09YK/2TYuZjJ7uz2c8DN0nWB93nLRoQb/5zdWc/8wPFJbX0CbForAZsb7GizMEw7ByvGvoxAxlBRsWelyq+7fGQIXDXlZk57Ge358cBNu/VJ8NsTL72g+vh5wZcNl7SrQNdi9VHTYz5imr95K3oNMo1dyNS1XRBN6DdEA1vc1+yro46ozumP2lWwzMxJpaN2ZBP+1PMM7pEzV8k2bMFVap0+JPyvKfDqNQlB9V0UEGh9YpMeowDP7PaUWWHIJHusDLUz3PYe7c8hZo76gYe4071KyyWBV4j3OZrKnxt8HZ/3Tfk/k+WtUzoZtZ/JPbqYrRoMNQ1YdiVEyJ9fhyzSGUSZlOQS9SfnWDGU+rfNTzdEg1XbvGaVB0Hg1Xflb3daJifCtAA28L3aj4vMM7b1oJN69zV/oAM+dTJ1UlKo8Z0SzJzudsbu2Zuew99d8oA627qcrE/K4n3u35G3O+aFVHv5r5PYFyzyVmKsOsukS57M54WJVDo8UWkwRnmYIGzC2V2DrcjcdJixb0pdvcvfSuleYri1UEinlwg+EmMXq7DQva3IyPb+V0BZgEPTpeiWB8mmeBMdN5jOf3wj2qZQDuTh2zlQMqY/Sc5NtBM+oaLPEXGQPKJ5hVR+dLeneVGZPbukd6xia7m8xmYk3WsuFPF1HqORitDSuEzXdbYgZc8RFcvRDOeQom/NG9zyzo9hroaQr36nOmKqiGcO3+VhVi6fAsoHGmVkG/c7zuw6tAOUyDkyqLPKMnwP0sek5WvmszZkGf9jhMfcTz9+Y0pZkqmTb9lOvKaL207qLegUFCa+rEnGeS26rO4uoSkzGCqnR6OjvrPSobk9Vsds1YYdXqMPCx0I3zerVoMnup++t+qgqNva8IBpxX93WrSjyfY/sh6vmePdf6eKPFZpRZo2wZz3T87b4VrjnPWj5vp7HmbXz0nqIqmxKTcTJ4Fvxhn2cZ6TPN/dmc57zdSEGkxQq6lNKjtdcm1Sno3z6mRjWah+5WlaqmthFBUVOu/Ih/NzXbW3dxhkAVqE5FUE212krVpL1lvfLBedN9gv9EGi/57H96RjcYmdF7ngp/5/Lu2DLTa4qnuHkTl6ripuPT3B2McSmeLgGjo8fDQncW5NhkVZCjouD8F62vEeWVzRJaK8ur+wRV4Q27Aib83r3fJehHVOUabxIpI8bbFqPEptDUIWkulOZnFxWtIiAMzEPiY1M8QwIri9wWerYzzt+wls3pMDAX9sR0GHO9p2vBbK2axd2w+o0OxdhkzwiO+mKUzec1++6t0gjuClBEwcxn3dvr6+iOrmPlLXuNlzUuPP7Vib/ZDG9wjsCsKfNsKQmhDBrv1pUrnc5jK44pA8I4f3wq/CFXtSK9MecXK0E3Wt/msvDHA6qcR9ncfVbmPGB+nv5EXFvoDee77UeosasXMrhTK0Z2dWYEwydsjkutLvWMx60ug8UPe54ws4/KTOUFvp1G3Scq0TL7y6/7Du7a65kZvDGEO6MHzHzOvd2o5c1hWZe+5/88F1jMDRGTqJq62cM9M9Ocbzx78qc+4rym6Zi4ZM+mu2HZmC0mw+VizpxWVj34Wuh1uVtAVZ6gnnWNlx+9valDNynLM3rBbJ3GeKXVfD/mCi4uxXMoe22F+z4v/0A9Q6OCMZ/DwEpgzKGVRufe6fd6HmNYGzanYEbHu63J6HiwRdfdz2AW0vYmQY/zI+jGMzz3GeXaMfBuHYJnRIZVR7yBo9YzHf4s9IbgLyyxPlzuywrf38WlWE8lXF+LKMM5dsFsyRv53exyNOdnc0UVowU9aLywdBcdWyWw5cGpfHTjOFKMRZurvdwroATdHF1SU+7ZWw7K35iQrkLUinPdGbnnZDWQAzx9re0GKitLCGUFmjOAgTnjmV+ykTmNwjbxT+44dysGnKdinEH1uIOyFDJ7+Z67w1B3pps5H7qM9T0m1qsAGB1i5m3G/fi7BzPeouFP0Mc6I1qM51iWryrX2GTlNuo02tPa93Ylma1a78rH/PzN6bRqvRiWYVyKeoZGh5bVrJJWguio9fwc38odNTP7S7jWNEDHEL+YeHfFYYQ1nleHn9lcScYmufOMv3fg75lbCZ1HR69T0K0qCm+Xiz8fekMwC6K366u+3xnXrctNZCbVFF1m5TK86FW46DVr/7qHoJtco+b025zHjL7eq3xrQW8Q5dW1/LK/iDHdM4iLtqm4Z2NwijFAwzzN5/p31SyGhnBZzbHc9RS3NSZs0N/pAzRbbf6s8bv2woX/dn4xFSCz5Wkl6IYwWImGN0Zm7uC0YKtMcdnehdzIjB5Wq/OzYR2aMSx0c6+/cQ6zBWc1ux/4Wuj+ms1THlT+1fhW6rwlB52Cngg3LFP+djNtczy/mwXdw0K3eT7DWC8L3RtvQTDuv6yOvgozOV6Ti5kLeadRnha1Yc1HJ3hGQnhz9Reqr8PAW4in/EX99yfcp92tKkzvtFlx5qOq1QnuZzHrdZjwB8/jHLVeYYtR1mkLhC4nw4x/eQqld+d0XQjhrgD8TanrjTlc2DvPg8qnOedYp8Ofhe7twrqvCM58xLMM1hfBdBy0SEHPuWchR8uq6dvOWVj/fTZ8eL36bPhL7abIkh/mqo4xfz3dI2Z7xjV3O8VdK5stPH+CHhXldqOYxcxbdAyMl2+EaAUi6MYxZpeE6zrO8xm+VldmNE+16rwPK4FzhayZBd15PXNB8LZ2jQLjnX5/Bc4c6pbSVs07I+3qeQjhKxTeIaGGFWmL9fKh27ws9GTrzwbegt7aWQDNkTNmJvxRVUYG582H3210C0xdfmrjHcfE1y3osYmelq/3Mx35azVhlVX4HSiXwhkPBSaSqR3UBFg9J8O5zpk707vDhLs8j7PXBNYpGgg9JqhZDs3PKlBhdh0f7/m/PurreDawaikY+Skq2lOsbX76HMzPyZ9BEwRanKCXVbmbuz3beBVWe63F0HoT/l6w4dPtdqqyHKf+1V3oYwOw0MG6Seyv6WUIvUvQA5ihwbCUrAYsREXBVZ+rqBJwV0ZWbggrgTPOae7sM6xOcwY239vsRcpfDzDwIvXfuN9AlvdKae/ufPYXFdB3upqQysDoEIyO9/Wh+3W5WFRg3j7YNn3VgCqjv8GbCb+Hk37j/h4dpyJaXFFMdbw/s4VeV0GPSbS2hs1k9AjOsnPRcaoz9LJ31ZQS/nD4C1sMMA2/MQ25dwmk6XyBuk5cxyc07HeBptPSQnem07uytrL0DdJ7wJgbArtmI2lxc7nsO6YEu0NaPGN7eAlsWV7d85GY96V2NHWWOV9859HwZ2ez2/AtmsOU6poHwijcZgH01+nj7XKxCvvzxijg0qE6SVt38dzf5ST35zMeVpnLHA5oWNdmK7vH6WrObSPNVi6XKD+dQOaZILOHq6bnC5PUcPu6OtoMktuq4dzg31KLTYTZX8ADGUoYjeNssZ6FOhBBj4pxhy9aCULP0+tPszcxiSoSoq5CbjxTQzQufMV6EE9MgqcAHY+fuj4CFUSH3bNPQzTQh57Rw/3cjTwuhHpXjtqGdYqCyUJvwO+iYuqOAoO6LXRv48QVzmtRWfzWYjK/INPiBH3vUSXoz148kPiVz3kW3lWv1j3VZdsB7jksJt2vFiIA65djWM8eAl1HE9HIsIEIupFmY7RjID43I4M57DDoorqPjU/zHdputDTM93DxW6qDzjWPs5UP3ZShvcMTfdJozJIXgKCntHO3puqKCjBcMRJl1Y+4Wk2HEBWFqoilr6Cbr29Y//Gp7kE9DRUSfxgCU5fLxWj1GCLqPYjFda4EL/dGAJV8Y/HnNvDGX9hiY1wu5vfTaEFvoIUOcNce6k1vXT5079aXlfHTjLQ4QU9Z8wI/xz1L8s9nw/o3PHd+81fP79EJ7hGCUx9RYvDjPPU92dy5ZCXoTmvO/ELrasIZx7Xq7I6g8ecbNzLmyF+raJkuY62PMzPmejUZWPcJ9R9rhTEa0LwSS3Ss+jMsGPPEQy4fegNWR4+yqNT8YQ6BDHQgRnQcTDfNnyOilOUuonwFwyDO1HdgCHpDm/r+cLXKAvCh13fN6ASvEMFwsNC953JpoMvFjMf7iQEqGxblAqZO0Qb8LpAQwoZY6HW1xpqBFudDH7vtMTJECbF5dcxrkuSMdTUXihGzPS03c8REnRa6d5Mr2rpjsk1f1YtvHtThD8N1ERUVmJiD8nX+Od8zFKsh9HdGP5jnijboNFqN0JtmWrHelaEDtObALeiBWOjmyqO+JrGBd1pcfk4vC91ckRouF3NfSNAsdCP8NAAfen3XjI7zFM9AOsobS6CVtD3AkaJ1YfwmyqLDtyFRLubjg/X+DKzyqzlvmWlIeWgCWpSFvr+wgixpI1bYEd6uFbM13n0CrH9HjUab+CcVSuYzIk74+ezEbmGhA9x92L/15L1WoT+CnSEDwRYDf8qzdg8IAcP/z3ObvyZnXRjN0IAsdNOgD3/RJd54C5HxHnwE3SLixdw52pBZAevCsHSDYaEL0XwWeqAWtnfY4vHEoZvfifG8GloOXJ2iQS4/Vunwl/8DmWK6CWlRgp5bUE4iCcRSqqYSNTPpPrWyyE/zVax2YrqKHz3lds/jJj/gudwb+LHQ/Qh6MJpcoRB0aJiQ+bPQx9zg/zyGvzggQTe1NAKdatT7vB6CbrZuo31/Y256B0sQXBZ6XT70eix0YXMfYzkqs5k58zFYdK/q3/A3l0tj0iYs3k9D30NDwxYDxRarWqnmpQX9uly0oAeNwyVVdCSe1lhEsnQ92b2CTVWJGjxhhTFroPdqPt70n6lWGbJaYaY+bvrZemZEg1AV1obgL0NP/avvsQauDsAGulwCLSQ+FYXRnLf5t9BdQ+9NaQqWILh86AEUM38W+s1r3AuXNKWbJVBGz4HMnvDaTKcPvSk6RQ0LvYHvwfUug2wQCaGiqTyuZZqczmp7iGhRgp5XXEmpTPDNT+c+A+0GqBA8UDMu1ks9Lpdup1gvqhwImX4GkFz7LRxY3bhzNjf+BL0urCKD/NGYwRd+XS51CLrLGjQJekKQBn644tADeEb+LPRWnd0D3prSzdIQzCGywbLQoyz6BxoqzMZ6AOYQ3abClV6ve60rbLEZaFGCfri4kgphUasbftJhV8CeHz0HgfjDI+a3mV5O+8HqLxJoTKeoaxBNABZ6YzrYfDpFTS4Xj/BKU7Y3Yv3Nvw3WajKBRLkYBBJZEm6CDtY+9MZgFvTG+tAn3w+bP1MLWTQ1Rh7y1gZtoQePQ8VV1EQngfeiJkaUREJrtTBEQNRjoZ/oWA0sqo+G+NABrvs+sOHZ/jpbzT50Kx8tqLVfvX/rb9bIhhLISFHvY+siXATdXB6sOmobk06r99NQQe9zpvWyek2B652Gl6AH9OSFEFOFEFuEENuFEHdZ7O8shFgshFgthFgnhJhmdZ6m5EhpFct2HsUWa5EJGjOhfCgs9EjCZaE0oPA2xOUCyk2WFmCEC1gUJsOHHu0VFhftnjfFiFbyGGwUpNnwohtgoQfyTMLBhw7+55RplMvF9I5c5zTcYEHu3Awmfi300A4sqrc0CiFswNPAmUAOcLEQwmuaO/4EvCOlHArMAv4V7ITWx9drtnJ2+Yf0zHQWooTW7gLVqBVCtIjXiVGQG1J4G+JyaQz+LHTvSicqGq7/UYWY1lpY6MEiEB/67C/VajqBPMdwsdDrdbkcrw/dsNAbODlXc1KfDz1EBOJyGQVsl1LuBBBCvAXMAMwThkvAmDA5DahjCZ2mIWf1X7goZiHyYLyade+3q+HvzvUoAx2YYkZb6HXjr8lZFw2JQ28MdQ0s8tgeBVHOY40JvVIaOSCrLgIZWNRplPoLhKYW9Fs31X+Mdzos53I5zigXWyOjXJoTvxZ6aL3YgVy9I2Ba54tcYLTXMfcBXwghfgMkAZarMQgh5gBzADp3rmNR1kYgnCsQidpKNVLT7Hs9bgtdC7pfGmShNyBssTHUNbDIH0MvV1bm4IvVknOxQbQKGxK2GAhNNX9L//Ngw/uBdwZ7GDtBGlgkvFxiEPzww2Diz6Ax7iNEY0mCVeVfDLwipcwGpgGvCeH7VqWU86WUI6SUI7Ky6lmGrIFU1phWiTEKdr/p6r/2oQcfQ5wb8mxcPvRmdrnUJehRNhh2uRLdzqPV3DnBoiFhi4HQVBb6hS83LAS3Ph96MCbngtANsAsEl4XutT0uBU77M1y1oNmTBIFZ6PsB83ye2c5tZmYDUwGklD8KIeKBTCDAJV6ODykllTWm0BbjYc94Wq3SXtdCt37RFnrdGJ0+jfGhN5HLxdvyN8ehh4KGhC0GQn2zWTYX/iz041mxKFIF3Sr/e48+b0YCySErgF5CiG5CiFhUp+fHXsfsBU4HEEL0A+KB/GAmtC6OlFZjt1ssvhAd538VovrQFnrduCz0JoxyCRTXgsveLhfTSNFQ0JCwxUAIy07R4xwpavWOXOU3nH3ojQgKaAbqzSFSylrgJmAhsAkVzbJBCPGAEOIc52G3AdcIIdYCbwJXStl8cTvbDpd4bghKAdIWep0Yr7cxPvRgCZw3jXG5NCUNCVsMhHAU9KCNFI1UCz28CChVUsoFwAKvbfeYPm8ExgU3aYGz9XAJ3c0bgvGwtYVeD41xuTh/E2yLufNY2L3U11cdakEPug89TOLQ/Q0sqmtbvae06BSNCEEPL20Iz2qmgWw4UExfc14P+mit8HppYYFLzxtQeF2zBgZZmGa9AQU7LaJJDIsxVC4XY0m8E8xCD1anaFi7XPyELYaYMMkhjUdKydJtR0hLMDW3g2ERhdmLCjsaE+Uy9HL1P9DV1gMlPlVNiexNyC10Ywm6IF0/HEeKHs8i0Wa8fejRCeFdBv0NLAoxES/ou46Ucai4ktaJJhEPStyvdrnUjeFDb0AWmnAX/Cm/cQO9GoNrYJG20IOKXwv9ONLnvaJUOA8qgrC10CPe5XKouBKA+BhzJgu2hR5eLy0sMCz0hkY0NFXIouX1Qhy2mJAOPSdDdoAjQesjHAXdck3R45ycq9upYdvp6EL70JuGY2VqcqUYmykTBcUi0hZ6nTQmyqW5CbUA2qLhsneDd75Q34+B8NcpejyTc5kEfdBF6i+cEREathjuHCuvBiDGZnqw2kJvejoM9fwfjhjv0OE9n3KEEjaCXs9cLsc7OVdE0Igor2agBVjoStCjo0wPVvvQm56+0+CWX6BVp/qPDRXmlXVaAuEievXO5XKcUS6RQJi2UCPsKfpSUF5Nclw0UdI89F9b6M1COIs5mCxJZ+HrOVktbhyphKWFHqSwxbCJsQ8UbaE3CYXlNbROinHPaw3Bj0MPs1pYEyA556o1WlOcswgG058dCsJF9OoNW2xExaMt9KAQYU/Rl4KyatITY91LiYEe+q9RjLsZRlztnvM80gkXC91cHqzCFo83Dj0i0BZ6k/D7AzdTGNseEqvdG4Nhoeuh/5GPEC1HzCGMZlusJ2zxROgUNZYpDNaC4kEi4gU9p3Yj1G6EuK7ujUG30DWaMCBcLHR/US6N6RQ9ntj1UNJhKMx8Dvo0+/LJdRJhT7EOak0WerA7RbWFrgkHwkX0/E2f2xhx7jVF/Y80HzrA4Flh1wIMkxzSOKqrTSJu9qEHO2xRW+uacCBsOkX9hS26NgZ+rvPmw83rmm5ZwhOMiBb0ogLTgkjaQte0dMLRQj/e+dCj46B1l+CkSxPZgl56zLQoUm2F+3NQmkHaQteEGeHScei3U9TYrstLqIhoQS8rNFnoDtMi0b3OOP6Ta6tcE26EjYXuJ2zxeEaKaoJCBPZEuKksPuK5YcIfYOCFQZp6U7tcNGFG2Ah6fZ2iuryEijDJIY2jqsRL0DuNgowewTm5HvqvCTfCRdD9DSxqzCLRmqASLjmkURwtLPTckJTVNBfSFocmHAgXQdcWetgSJjmkceQVVXhuCKagawtdE26Ee6eottBDTsQKupSS/GIvQU/MCOIVtA9dE2aEo4VuOZdLmKTzBCRin3xFjZ2aWtOUufFpwZ1lUVvomnAjbAYW1bcEnS4voSJiBb2syk4UpoULooO9qKy20DVhRrhYvh5hixZzuWgDKGSESQ5pOBXVXoIe7LkgtIWuCTfCRtC1hR6uhEkOaTjlNbVEueYkpgk6jLSFrgkzwnH6XA83Z4TOnNiCiNgnX15t9xL0plylSAu6RuPCw+USY7Fdl5dQEbGCXlFtR3gIerAHvWoLXaOpl+OdnEsTVCJW0Mu9fehNuo6ozqAajSVWLhddXkJGBAt6LTZh7hRtSh96kE+t0bQUrFwu2kIPGRE7OdeQ5bcxI/pz94Ym9aFrNBpLPFydWtBDTcRa6F0Ofu65oUmXsNIZVKOxxLw6mO4UDTkRa6H7oDtFNScCM56GdoNCnQo3UVZhi7q8hIoWJOhB9qHrTlFNODL0slCnwBOzIaVXLAo5Eety8UEPLNJomh+bVadoy5GVSKMFPfkgi6620DWa+tGdomFFyxH0oGcibaFrNPUSpTtFw4mWI+jaQtdomh/LuVx0eQkVAQm6EGKqEGKLEGK7EOIuP8dcJITYKITYIIR4I7jJDIBg++2EttA1mnqxstC1Dz1k1BvlIoSwAU8Dk4FcYIUQ4mMp5UbTMb2APwDjpJTHhBBtmirBdSS0KU/ehOfWaCIYKx+6Li8hI5CqdBSwXUq5U0pZDbwFzPA65hrgaSnlMQApZV5wkxlitIWu0VhjGeWiy0uoCETQOwL7TN9zndvM9AZ6CyG+F0IsE0JMDVYCraissftubNJmns6gGo0lllNu6PISKoI1sCga6AVMALKBb4UQA6WUheaDhBBzgDkAnTt3bvTFjpVX095naxNmIm1xaDTWWI3/0OUlZARi1u4HOpm+Zzu3mckFPpZS1kgpdwFbUQLvgZRyvpRyhJRyRFZWVmPTzLGyGt+N2oeu0TQ/5nInpe82TbMSiKCvAHoJIboJIWKBWcDHXsd8iLLOEUJkolwwO4OYTg9KKi0EXaPRhAla0ENFvYIupawFbgIWApuAd6SUG4QQDwghznEethA4KoTYCCwG7pBSHm2qRFfbHb4bm9Iq0BaHRhM4uryEjIB86FLKBcACr233mD5L4FbnX5NTYyXoTWoV6Ayq0QSOLi+hIiJHAFTXSt+N2kLXaMIDPbAoZETkk7d0uWgLXaMJD7QBFDIiUtBrapvbh950p9ZoWh66wISKiBR0605RPbBIowkLtIUeMiJS0Ju9U1RnUI2mAejyEioiUtCrm9vlojOoRhM4ulM0ZETkk2/2TlFtoWs0mgggIgW9prnDFrWFrtFoIoCIFPRqu8Vsi9pC12g0JzgRKeg1dm2hazThh0W51DQrESnolp2irbo03QW1ha7RBI4uLyEjWPOhNysenaJJWXD2P6HXlNAlSKM50bj+R6/l5zThQES+EZ+Ron3PauIraotDo/GgbY7FRmc5kdr1Eioi0+VittCbI/PoJqRGEwBayENNRAq650jR5shEWtA1Gk34E5GCbjl9blOiLXSNJnB0eQkZkSnoliNFmxKdQTUaTfgTkYLu0SmqfegajUYDRKigawtdo9FofIlIQW/2TlFtoWs0mgggIgXdcqSoRqPRnOBEpqA3t8tFW+gajSYCiEhBtztMbpZmGZWmBV2jqRc9QjTkRL6gNwfaQtdoGoAuL6EiIgXd4SHo2kLXaMICl+GjLfVQEZmCrvOLRqPR+BCRgm43++qaxUDXFrpGowl/IlLQHXpgkUYTfuhO0ZATkYIupdWaok2IttA1mgagy0uoiFBB19PnajQajTcRKegeTTs9OZdGEx7oKJeQE5GCLh3ah67RhB3ahx5yIlLQ0T50jSaM0eUlVESmoDuaWdB1BtVoNBFARAq6o7k7RbWFrtFoIoCIE3QpJaK5O0W1ha7RaCKAiBN0h4Qo9PS5Go1G403ECbrdIbGhJ+fSaDQabyJO0B1SIrSFrtFoND4EJOhCiKlCiC1CiO1CiLvqOO58IYQUQowIXhI9sTskUXrggkaj0fhQr6ALIWzA08CZQA5wsRAix+K4FOBmYHmwE2nGLr0EXQ9m0GjCBF0WQ00gFvooYLuUcqeUshp4C5hhcdxfgEeByiCmzwfp0J2iGk1Yo8tLyAhE0DsC+0zfc53bXAghhgGdpJSf1XUiIcQcIcRKIcTK/Pz8BicWnBa60JNzaTRhi241h4zj7hQVQkQBTwC31XeslHK+lHKElHJEVlZWo67n40PvOalR52kQ2uLQaAJAl5NQE4ig7wc6mb5nO7cZpAADgG+EELuBMcDHTdUx6jD70Cf+Cc5/sSku44XOqBpN/WjLPNQEIugrgF5CiG5CiFhgFvCxsVNKWSSlzJRSdpVSdgWWzXLCgwAAES5JREFUAedIKVc2RYKVhe50uaR3g5j4priMJ9pC12g0EUC9gi6lrAVuAhYCm4B3pJQbhBAPCCHOaeoEeqPi0J2WgGiuMHot6BpNwGgDKGREB3KQlHIBsMBr2z1+jp1w/Mnyj8MBNsNCby5B1xlUo9FEABE3UtQjDj3K1kxX1YKu0QSMjnIJGZEn6OYoF22hazQajYuIE3SPuVy0D12jCT+0ARQyIk7QtYWu0Wg01kScoDukbP5OUW2hazT102ea+t/lpNCm4wQmoCiXcMLhIARhixqNpl56TIT7ikKdihOaiFNEFeWiwxY1Go3Gm8gTdPOKRdrlotFoNC4iTtAdUhIjatWX6Ljmuai20DUaTQQQeYLukMTgFHRbTDNdVQu6RqMJfyJO0O1SEkuN+mKLbZ6Lagtdo9FEABEn6A4HxLosdC3oGo1GYxBxgm6XZpdLMwm6RqPRRAARJ+gOhyRG2NUXLegajUbjIvIEXVvoGo1GY0nECbrdIYlzdYo2V5SLRqPRhD8RJ+jaQtdoNBprIk7Q7Q7cgt5cA4s0Go0mAog8QXeOFJUiqhlXLNJoNJrwJwJnW5TEYkdGxerxm5oThpqaGnJzc6msrAx1UjTNRHx8PNnZ2cTEBN5XGHmCbowU1R2imhOI3NxcUlJS6Nq1K0IPdGvxSCk5evQoubm5dOvWLeDfRZ7LxTmXi4zSgq45caisrCQjI0OL+QmCEIKMjIwGt8giTtBdUS46wkVzgqHF/MSiMe874gTd7kB1impB12g0Gg8iT9ClVJNzaUHXaDQaDyJO0KWUxGDXnaIaTTNis9kYMmQI/fv3Z/Dgwfz973/H4XA0y7VfeeUVoqKiWLdunWvbgAED2L17d52/e/LJJykvL3d9v/vuu+nUqRPJyckexz3xxBPk5OQwaNAgTj/9dPbs2ePaN3XqVFq1asX06dODczNNTMRFudgdRpSLttA1Jyb3f7KBjQeKg3rOnA6p3Ht2f7/7ExISWLNmDQB5eXlccsklFBcXc//99wc1Hf7Izs7moYce4u233w74N08++SSXXXYZiYmJAJx99tncdNNN9OrVy+O4oUOHsnLlShITE3nmmWe48847Xde54447KC8v57nnngvezTQhEWehG1EuWtA1mtDQpk0b5s+fz7x585BSYrfbueOOOxg5ciSDBg1yid8333zDhAkTuOCCC+jbty+XXnopUqr1gO+66y6XVXz77bcDkJ+fz/nnn8/IkSMZOXIk33//veua06dPZ8OGDWzZssUnPV988QVjx45l2LBhXHjhhZSWljJ37lwOHDjAxIkTmThxIgBjxoyhffv2Pr+fOHGiS/THjBlDbm6ua9/pp59OSkpKQM/lgQceYOTIkQwYMIA5c+a47nX79u1MmjSJwYMHM2zYMHbs2AHAo48+ysCBAxk8eDB33XVXQNeoFyllSP6GDx8uG8NzS7bLZX8eJWtenNao3zeIe1PVn0YTYjZu3BjS6yclJflsS0tLk4cOHZLPPfec/Mtf/iKllLKyslIOHz5c7ty5Uy5evFimpqbKffv2SbvdLseMGSOXLl0qjxw5Inv37i0dDoeUUspjx45JKaW8+OKL5dKlS6WUUu7Zs0f27dtXSinlyy+/LG+88Ub573//W15xxRVSSin79+8vd+3aJfPz8+X48eNlaWmplFLKRx55RN5///1SSim7dOki8/PzA7oXgxtvvNF1LwaLFy+WZ511Vr3P6OjRo67Pl112mfz444+llFKOGjVKvv/++1JKKSsqKmRZWZlcsGCBHDt2rCwrK/P5rRmr9w6slH50NeJcLraoKOKEHaF96BpNWPDFF1+wbt063n33XQCKiorYtm0bsbGxjBo1iuzsbACGDBnC7t27GTNmDPHx8cyePZvp06e7/NOLFi1i48aNrvMWFxdTWlrq+n7JJZfw0EMPsWvXLte2ZcuWsXHjRsaNGwdAdXU1Y8eObdR9vP7666xcuZIlS5Y06veLFy/mb3/7G+Xl5RQUFNC/f38mTJjA/v37mTlzJqBGf4K616uuusrVMkhPT2/UNb2JOEGffXI32JAIMXpiLo0mVOzcuRObzUabNm2QUvLUU09xxhlneBzzzTffEBfnLqc2m43a2lqio6P56aef+Oqrr3j33XeZN28eX3/9NQ6Hg2XLlrlEz5vo6Ghuu+02Hn30Udc2KSWTJ0/mzTffPK77WbRoEQ899BBLlizxSHOgVFZWcsMNN7By5Uo6derEfffdF5JpGiLOhw5AbbWOctFoQkR+fj7XXXcdN910E0IIzjjjDJ555hlqatQ6BVu3bqWsrMzv70tLSykqKmLatGn84x//YO3atQBMmTKFp556ynWc0Qlr5sorr2TRokXk5+cDyuf9/fffs337dgDKysrYunUrACkpKZSUlNR7P6tXr+baa6/l448/pk2bNgE+BU8M8c7MzKS0tNTVWklJSSE7O5sPP/wQgKqqKsrLy5k8eTIvv/yyKwqnoKCgUdf1JjIF3V6lO0U1mmakoqLCFbY4adIkpkyZwr333gvAr3/9a3Jychg2bBgDBgzg2muvpba21u+5SkpKmD59OoMGDeLkk0/miSeeAGDu3LmsXLmSQYMGkZOTw7PPPuvz29jYWH7729+Sl5cHQFZWFq+88goXX3wxgwYNYuzYsWzevBmAOXPmMHXqVFen6J133kl2djbl5eVkZ2dz3333ASqSpbS0lAsvvJAhQ4ZwzjnnuK43fvx4LrzwQr766iuys7NZuHCh5T21atWKa665hgEDBnDGGWcwcuRI177XXnuNuXPnMmjQIE466SQOHTrE1KlTOeeccxgxYgRDhgzh8ccfD/RV1ImQzp7Y5mbEiBFy5cqVjfvx3/tBz9NgxtPBTZQ396U5/xc17XU0mnrYtGkT/fr1C3UyNM2M1XsXQvwspRxhdXxkWui1FRCdEOpUaDQaTVgRcZ2iANRUQIwWdI1G0/zMnDnTI9IGVEy5d6dwKIg8QXc4oLZSC7pGowkJH3zwQaiT4JfIc7nUOkOBtKBrNBqNB5Er6NqHrtFoNB4EJOhCiKlCiC1CiO1CCJ9JB4QQtwohNgoh1gkhvhJCdAl+Up3UOGdP0xa6RqPReFCvoAshbMDTwJlADnCxECLH67DVwAgp5SDgXeBvwU6oixrtctFoNBorArHQRwHbpZQ7pZTVwFvADPMBUsrFUkpj4uFlQHZwk2mitkL9j7YeHqzRaIKPng89+POhT5gwgUaPxfFDIFEuHYF9pu+5wOg6jp8NfG61QwgxB5gD0Llz5wCT6EWNU9BjEhv3e40m0vn8Lji0PrjnbDcQznzE7249H/oJOB+6EOIyYATwmNV+KeV8KeUIKeWIrKysxl3EJejaQtdoQoGeD92X//3vf1x44YWu7998843Lqr/++usZMWIE/fv3d02X0FQEYqHvBzqZvmc7t3kghJgE3A2cKqWsCk7yLHAJuvaha05Q6rCkm4vu3btjt9vJy8vjo48+Ii0tjRUrVlBVVcW4ceOYMmUKoCa+2rBhAx06dGDcuHF8//339OvXjw8++IDNmzcjhKCwsBCAm2++md/97necfPLJ7N27lzPOOINNmzYBEBUVxZ133snDDz/Mv//9b1c6jhw5woMPPsiiRYtISkri0Ucf5YknnuCee+7hiSeeYPHixWRmZgZ8Xy+++CJnnnlmg5/HpEmTmDNnDmVlZSQlJfH2228za9YsAB566CHS09Ox2+2cfvrprFu3jkGDBjX4GoEQiKCvAHoJIbqhhHwWcIn5ACHEUOA5YKqUMi/oqTTj8qE3g6Bf/wMU7qv/OI3mBEbPh66m9p06dSqffPIJF1xwAZ999hl/+5uKDXnnnXeYP38+tbW1HDx4kI0bN4ZO0KWUtUKIm4CFgA14SUq5QQjxAGrljI9RLpZk4L9CCIC9Uspz/J70eGhOC71tf/Wn0Wg80POh+zJr1izmzZtHeno6I0aMICUlhV27dvH444+zYsUKWrduzZVXXtmk86QH5EOXUi6QUvaWUvaQUj7k3HaPU8yRUk6SUraVUg5x/jWNmIN2uWg0IUbPh27NqaeeyqpVq3j++edd7pbi4mKSkpJIS0vj8OHDfP65ZbxI0Ii8kaJa0DWaZkfPh173fOigWiDTp0/n888/d7mRBg8ezNChQ+nbty+XXHKJyzXUVETefOibP4O1b8EFL+lVizQnDHo+9BOThs6HHnmzLfY9S/1pNBqNxoPIE3SNRqMJIXo+dI1Gc9xIKXFGkWlCSHPNh94Yd3jkdYpqNCcg8fHxHD16tFGFXBN5SCk5evSo3xBOf2gLXaOJALKzs8nNzXWF62laPvHx8a5BWYGiBV2jiQBiYmLo1q1bqJOhCXO0y0Wj0WhaCFrQNRqNpoWgBV2j0WhaCCEbKSqEyAf21HugNZnAkSAmJxLQ93xioO/5xOB47rmLlNJyQYmQCfrxIIRY6W/oa0tF3/OJgb7nE4OmumftctFoNJoWghZ0jUajaSFEqqDPD3UCQoC+5xMDfc8nBk1yzxHpQ9doNBqNL5FqoWs0Go3GCy3oGo1G00KIOEEXQkwVQmwRQmwXQtwV6vQECyHES0KIPCHEL6Zt6UKIL4UQ25z/Wzu3CyHEXOczWCeEGBa6lDceIUQnIcRiIcRGIcQGIcTNzu0t9r6FEPFCiJ+EEGud93y/c3s3IcRy5729LYSIdW6Pc37f7tzfNZTpbyxCCJsQYrUQ4lPn9xZ9vwBCiN1CiPVCiDVCiJXObU2atyNK0IUQNuBp4EwgB7hYCJET2lQFjVeAqV7b7gK+klL2Ar5yfgd1/72cf3OAZ5opjcGmFrhNSpkDjAFudL7PlnzfVcBpUsrBwBBgqhBiDPAo8A8pZU/gGDDbefxs4Jhz+z+cx0UiNwObTN9b+v0aTJRSDjHFnDdt3pZSRswfMBZYaPr+B+APoU5XEO+vK/CL6fsWoL3zc3tgi/Pzc8DFVsdF8h/wETD5RLlvIBFYBYxGjRqMdm535XNgITDW+TnaeZwIddobeJ/ZTvE6DfgUEC35fk33vRvI9NrWpHk7oix0oCOwz/Q917mtpdJWSnnQ+fkQ0Nb5ucU9B2fTeiiwnBZ+3073wxogD/gS2AEUSilrnYeY78t1z879RUBG86b4uHkSuBNwOL9n0LLv10ACXwghfhZCzHFua9K8redDjxCklFII0SJjTIUQycB7wC1SymLzMmst8b6llHZgiBCiFfAB0DfESWoyhBDTgbz/b9/uWasIojCO/5/CN4JEBAUhAQnYphIRTJHKIoVVCiFginwKCfgRBD+AZYggWAQ7NfaK+BZJSCKkuYgBQesUx2LOhkWwSdwsOz4/WO7uzBZzlrnnzj1zb0S8kzTb93hO2ExEjCRdBl5I2mp3djG3h7ZCHwGTreuJbKvVd0lXAPJ1P9ureQ6STlGS+UpEPMvm6uMGiIifwGtKyeGCpGaB1Y7rMObsHwd+nPBQj+MWcEfSHvCEUnZ5RL3xHoqIUb7uUz64b9Dx3B5aQn8LXMsd8tPAXWCt5zF1aQ1YzPNFSo25ab+XO+M3gV+tr3GDobIUfwxsRsTDVle1cUu6lCtzJJ2j7BlsUhL7fN72Z8zNs5gH1iOLrEMQEfcjYiIirlLer+sRsUCl8TYkjUk635wDt4ENup7bfW8cHGGjYQ7YptQdl/sezz+MaxX4BhxQ6mdLlNrhK2AHeAlczHtF+bXPV+AzcL3v8R8x5hlKnfET8CGPuZrjBqaB9xnzBvAg26eAN8Au8BQ4k+1n83o3+6f6juEYsc8Cz/+HeDO+j3l8aXJV13Pbf/03M6vE0EouZmb2F07oZmaVcEI3M6uEE7qZWSWc0M3MKuGEbmZWCSd0M7NK/AaxkkYTPCkOWQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcElIu93yIQU"
      },
      "source": [
        "DenseNet121_model = tf.keras.models.load_model('/content/drive/MyDrive/DACON_CVLC/Checkpoint/BS_08_3_DN121.h5', compile=False)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hR4N2pAZyiR-"
      },
      "source": [
        "!mkdir images_test/none\n",
        "!mv images_test/*.png images_test/none"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxH98QOgyu1z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a547b94b-0db5-4113-f625-ecb2e446c0e1"
      },
      "source": [
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = datagen.flow_from_directory('./images_test', target_size=(224,224), color_mode='grayscale', class_mode='categorical', shuffle=False)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20480 images belonging to 1 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFEcoCR-3DNH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98904a80-a3a8-461e-fa25-45683b4b9cf3"
      },
      "source": [
        "DenseNet121_predict = DenseNet121_model.predict_generator(test_generator).argmax(axis=1)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2035: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYhGZuzr1AjD"
      },
      "source": [
        "submission = pd.read_csv('/content/drive/MyDrive/DACON_CVLC/data/submission.csv')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWALVGA1shFz"
      },
      "source": [
        "import numpy as np\n",
        "mylist = []\n",
        "\n",
        "for i in range(len(submission)):\n",
        "    name =  test_generator.filenames\n",
        "    id = name[i].split('/')[1].rstrip('.').split('.')[0]\n",
        "    mylist.append(id)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xjLSWZJvuVK"
      },
      "source": [
        "for i in range(len(submission)):\n",
        "    submission[\"id\"][i] = mylist[i]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNg9gk9z3Noq"
      },
      "source": [
        "submission[\"DenseNet121_predict\"] = DenseNet121_predict"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Smd-xg6deOK"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "for i in range(len(submission)) :\n",
        "    predicts = submission.loc[i, ['DenseNet121_predict']]\n",
        "    submission.at[i, \"digit\"] = Counter(predicts).most_common(n=1)[0][0]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg9m6Zgk4foS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "ffcc4037-0d2d-4e8a-dc2d-70850e609232"
      },
      "source": [
        "submission = submission[['id', 'digit']]\n",
        "submission.head()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>digit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10000</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10001</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10002</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10003</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10004</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id  digit\n",
              "0  10000      4\n",
              "1  10001      4\n",
              "2  10002      6\n",
              "3  10003      9\n",
              "4  10004      5"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flAHWrtH4flu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "b4c668e8-23fc-4ea3-b3bd-49f8eb85cc45"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "submission.to_csv('/content/drive/MyDrive/DACON_CVLC/Submission/BatchSize_08_3_DenseNet121_model.csv', index=False)\n",
        "files.download('/content/drive/MyDrive/DACON_CVLC/Submission/BatchSize_08_3_DenseNet121_model.csv')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_0f8f1bab-0726-4ee1-9280-11a36f7fd56f\", \"BatchSize_08_3_DenseNet121_model.csv\", 155898)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}